{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nt_mosaic_agb_zonal_stats_fire_scar_v6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output dir: \n",
    "\n",
    " - {drive}:\\cdu\\data\\zonal_stats\\output\\{date}\\fire_scar_zonal\n",
    " - {drive}:\\cdu\\data\\zonal_stats\\output\\{date}\\no_fire_scar_zonal\n",
    " - {drive}:\\cdu\\data\\zonal_stats\\output\\{date}\\fire_mask\n",
    "\n",
    "This notebook produces:\n",
    "*  a fire scar csv, \n",
    "* a csv per variable which outputs the nearest zonal stats to the field data collection date\n",
    "* a csv per variable which outputs the nearest zonal stats to the field data collection date that occurred prior to a fire scak (dka)\n",
    "\n",
    "fire ym_before_fire_scar: this csv includes a feature ym_bfr_fs (Year month before fire scar recorded for a site). The feature was created to increase the number of observation per site so that an observation exists for each month per year where no fire scar had been observed up until a fire scar was observed, if no fire scar occurred in a year 12 observations for that year are crested, if a fire scar occurs in may 4 observation for that year were created.\n",
    "\n",
    "| site | st_fs | end_fs | year | ym_bfr_fs |\n",
    "| :---: | :---: | :---: | :---: | :---:|\n",
    "| abg02 | 0 | 0| 1988 | 198801|\n",
    "| abg02 | 0 | 0| 1988 | 198802|\n",
    "| abg02 | 0 | 0| 1988 | 198803 |\n",
    "| abg02 | 198903 |  198903 | 1989 | 198801|\n",
    "| abg02 | 198803 | 198903 | 1989 | 198802|\n",
    "| abg02 |  0  |  0  | 1990 | 199001 |\n",
    "\n",
    "The following conditions apply:\n",
    "\n",
    " - run after Seasonal Biomass Zonal Pipeline.\n",
    " - env = biomass_zonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "# import plotting and stats modules\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define file name to composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary identifies the data structure of the reference image\n",
    "\n",
    "dict_ = {\"dbi_zonal_stats\": \"seasonal\", \n",
    "         \"dim_zonal_stats\": \"seasonal\",\n",
    "\"dis_zonal_stats\": \"seasonal\",\n",
    "\"dja_zonal_stats\": \"seasonal\",\n",
    "\"dka_zonal_stats\": \"annual\",\n",
    "\"dp1_zonal_stats\": \"multi_unknown\",\n",
    "\"fpc_zonal_stats\": \"single\",\n",
    "\"fpca2_zonal_stats\": \"seasonal\",\n",
    "\"h99a2_zonal_stats\": \"seasonal\",\n",
    "\"pg_zonal_stats\": \"seasonal\",\n",
    "\"ref_zonal_stats\": \"single\",\n",
    "\"stc_zonal_stats\": \"seasonal\",\n",
    "\"th_zonal_stats\": \"seasonal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "date_str = now.strftime(\"%Y%m%d\")\n",
    "date_time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(date_str)\n",
    "print(date_time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive\n",
    "drive = \"D\"\n",
    "# # processing date\n",
    "# date = \"20230204\"\n",
    "\n",
    "# date of data exports\n",
    "field_date = \"20230202\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = r\"{0}:\\cdu\\data\\zonal_stats\\nt_mosaic\\rmcgr_nt_mosaic_20230116_0905\".format(drive)\n",
    "output_dir = r\"{0}:\\cdu\\data\\zonal_stats\\output\\{1}\".format(drive, date_str)\n",
    "no_fire_scar_dir = os.path.join(output_dir, \"no_fire_scar_zonal\")\n",
    "fire_scar_dir = os.path.join(output_dir, \"fire_scar_zonal_stats\")\n",
    "fire_mask_dir = os.path.join(output_dir, \"fire_mask\")\n",
    "fms_dir = os.path.join(fire_mask_dir, \"site\")\n",
    "no_fire_scar_basal_dir = os.path.join(output_dir, \"fire_mask_applied_basal\")\n",
    "w_no_fire_scar_basal_dir = os.path.join(output_dir, \"fire_mask_NOT_applied_basal\")\n",
    "revised_fire_scar_dir = os.path.join(output_dir, \"initial_asof_merge_fmna\")\n",
    "szs_dir = os.path.join(output_dir, \"seasonal_zonal_stats\")\n",
    "ftzs_dir = os.path.join(output_dir, \"file_type_zonal_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dir_fn(dir_):\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.mkdir(dir_)\n",
    "        \n",
    "def export_csv_fn(list_, dir_, file_name):\n",
    "    \n",
    "    df_final = pd.concat(list_, axis =0)    \n",
    "    output_path = os.path.join(dir_, file_name)\n",
    "    df_final.to_csv(os.path.join(output_path), index=False)\n",
    "    print(\"File output to: \", output_path)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_dir_fn(output_dir)\n",
    "mk_dir_fn(no_fire_scar_dir)\n",
    "mk_dir_fn(fire_scar_dir)\n",
    "mk_dir_fn(fire_mask_dir)\n",
    "mk_dir_fn(fms_dir)\n",
    "mk_dir_fn(no_fire_scar_basal_dir)\n",
    "mk_dir_fn(w_no_fire_scar_basal_dir)\n",
    "mk_dir_fn(revised_fire_scar_dir)\n",
    "mk_dir_fn(szs_dir)\n",
    "mk_dir_fn(ftzs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal = r\"{0}:\\cdu\\data\\output\\{1}\\slats_tern_biomass.csv\".format(drive, field_date)\n",
    "basal_df = pd.read_csv(basal)\n",
    "print(basal_df.shape)\n",
    "basal_df.drop_duplicates(inplace=True)\n",
    "print(basal_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1 = r\"{0}:\\cdu\\data\\zonal_stats\\output\\{1}\\dp1_zonal_concat.csv\".format(drive, field_date)\n",
    "dp1_df = pd.read_csv(dp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NT Mosaic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_seasonal_date(date_):\n",
    "    \"\"\" extract the end dates of the seasonal image zonal stats.\"\"\"\n",
    "    \n",
    "    year = date_[:4]\n",
    "    month = date_[4:]\n",
    "    \n",
    "    start_date = str(year) + str(month) + \"01\"\n",
    "    \n",
    "    return start_date\n",
    "    \n",
    "\n",
    "def end_seasonal_date(date_):\n",
    "    \n",
    "    \"\"\" extract the start dates of the seasonal image zonal stats.\"\"\"\n",
    "    #print(\"date: \", date_)\n",
    "    year = str(date_[:4])\n",
    "    month = str(date_[4:])\n",
    "    #print(\"month: \", month)\n",
    "    \n",
    "    month_, day_range = monthrange(int(year), int(month))\n",
    "    end_date = str(year) + str(month) + str(day_range)\n",
    "    #print(end_date)\n",
    "    return end_date\n",
    "\n",
    "\n",
    "def im_date_season(df):\n",
    "    \"\"\"Collate start date of image into im_date column\"\"\"\n",
    "    \n",
    "    st_date_list = []\n",
    "    e_date_list = []\n",
    "    for i in df.im_name:\n",
    "        #print(i)\n",
    "        list_name = i.split(\"_\")\n",
    "        date = list_name[-2]\n",
    "        st_date = date[1:7]\n",
    "        start_date = start_seasonal_date(st_date)\n",
    "        st_date_list.append(start_date)\n",
    "        \n",
    "        e_date = date[7:] \n",
    "        end_date = end_seasonal_date(e_date)\n",
    "        e_date_list.append(end_date)\n",
    "        \n",
    "    df[\"im_s_date\"] = st_date_list\n",
    "    df[\"im_e_date\"] = e_date_list\n",
    "    \n",
    "    return df\n",
    "        \n",
    "    \n",
    "def im_date_annual(df):\n",
    "    \"\"\"Collate start date of image into im_date column\"\"\"\n",
    "    \n",
    "    st_date_list = []\n",
    "    e_date_list = []\n",
    "    for i in df.im_date:\n",
    "        print(i)\n",
    "#         list_name = i.split(\"_\")\n",
    "#         date = str(i) + \"0101\" #list_name[-2]\n",
    "        st_date = str(i) + \"01\"\n",
    "        start_date = start_seasonal_date(st_date)\n",
    "        st_date_list.append(start_date)\n",
    "        \n",
    "        e_date = str(i) + \"12\"\n",
    "        print(e_date)\n",
    "        end_date = end_seasonal_date(e_date)\n",
    "        e_date_list.append(end_date)\n",
    "        \n",
    "    df[\"s_date\"] = st_date_list\n",
    "    df[\"e_date\"] = e_date_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def convert_to_datetime(df, col_nm_s, col_nm_d):\n",
    "    \n",
    "    date_list = []\n",
    "    for i in df[col_nm_s]:\n",
    "        #print(i)\n",
    "        datetime_object = datetime.strptime(str(i), '%Y%m%d')\n",
    "        date_list.append(datetime_object)\n",
    "        print(datetime_object)\n",
    "        #df[col_nm_d] =  pd.to_datetime(df[col_nm_s], format='%Y%m%d.%f')\n",
    "        #date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    df[col_nm_d] = date_list\n",
    "    return df        \n",
    "\n",
    "\n",
    "def convert_to_dt_year(df, col_nm_s, col_nm_d):\n",
    "    \n",
    "    date_list = []\n",
    "    for i in df[col_nm_s]:\n",
    "        #print(i)\n",
    "        datetime_object = datetime.strptime(str(i), '%Y')\n",
    "        date_list.append(datetime_object)\n",
    "        print(datetime_object)\n",
    "        #df[col_nm_d] =  pd.to_datetime(df[col_nm_s], format='%Y%m%d.%f')\n",
    "        #date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    df[col_nm_d] = date_list\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_df = convert_to_datetime(basal_df, \"date\", \"basal_dt\")\n",
    "basal_df.sort_values(by='basal_dt', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fire_percent_fn(df):\n",
    "    \"\"\" Calculate the percent cover burnt by fire. \"\"\"\n",
    "    df.fillna(0, inplace =True)\n",
    "    df[\"area_ha\"] = (df.dka_count * (30 *30)*0.0001)\n",
    "    df[\"jan_per\"] = (df.jan / df.dka_count *100) #(30 *30)/ 1000)\n",
    "    df[\"feb_per\"] = (df.feb / df.dka_count *100)\n",
    "    df[\"mar_per\"] = (df.mar / df.dka_count *100)\n",
    "    df[\"april_per\"] = (df.april / df.dka_count *100)\n",
    "    df[\"may_per\"] = (df.may / df.dka_count *100)\n",
    "    df[\"june_per\"] = (df.june / df.dka_count *100)\n",
    "    df[\"july_per\"] = (df.july / df.dka_count *100)\n",
    "    df[\"aug_per\"] = (df.aug / df.dka_count *100)\n",
    "    df[\"sep_per\"] = (df.sep / df.dka_count *100)\n",
    "    df[\"oct_per\"] = (df.oct / df.dka_count *100)\n",
    "    df[\"nov_per\"] = (df.nov / df.dka_count *100)\n",
    "    df[\"dec_per\"] = (df.dec / df.dka_count *100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fire_yn_fn(df):\n",
    "    \n",
    "    \"\"\" Score if fire occured during the year 0 = No, 1 = yes. \"\"\"\n",
    "    fire_1_0 = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row.dka_major == 0:\n",
    "            fire_1_0.append(0)\n",
    "        else:\n",
    "            fire_1_0.append(1)\n",
    "            \n",
    "    df['burnt'] = fire_1_0\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def fire_intensity_fn(df):\n",
    "    \n",
    "    \"\"\" Score fire intensity by majority burnt 0 = no fire, 1 = Jan - June, 2 July - December \"\"\"\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "                \n",
    "        if row.dka_major == 0:\n",
    "            list_.append(0)\n",
    "            \n",
    "            \n",
    "        elif row.dka_major > 1 and row.dka_major < 7 :\n",
    "            list_.append(1)\n",
    "        else:\n",
    "            list_.append(2)\n",
    "            \n",
    "    df['intens'] = list_\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def ratio_fire_year_fn(x, y, p, n):\n",
    "\n",
    "    # data number of fires per data lenght of time = x/y\n",
    "    # revised time frame i.e. number of fires per time restriction = p/n\n",
    "    \n",
    "    final =  (x*n) / (p*y) \n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "def prop_fire_freq_fn(df):\n",
    "    list_ = []\n",
    "    for i in df.site.unique():\n",
    "        df1 = df[df[\"site\"]==i]\n",
    "\n",
    "        burnt_sum = df1.burnt.sum()\n",
    "          # calculate average time between fires over years of data capture\n",
    "        if burnt_sum > 0:\n",
    "            \n",
    "            freq = ratio_fire_year_fn(1, 10, burnt_sum, (len(df1.index)))\n",
    "            \n",
    "            frequency = round(freq, 5) #round((len(df1.index)) / burnt_sum, 5)\n",
    "        else:\n",
    "            frequency = round(0/10, 5) # round(len(df1.index), 5)\n",
    "                         \n",
    "        df1[\"fire_f\"] = frequency\n",
    "        df1[\"fire_tot\"] = burnt_sum = df1.burnt.sum()\n",
    "\n",
    "        list_.append(df1)\n",
    "        \n",
    "    df2 = pd.concat(list_, axis = 0)\n",
    "    return df2\n",
    "\n",
    "    \n",
    "def fire_previous_year(df):\n",
    "    list_ = []\n",
    "    df.dropna(inplace=True)\n",
    "    for i in df.site.unique():\n",
    "        years_since_list = []\n",
    "        print(i)\n",
    "        df1 = df[df[\"site\"]==i]\n",
    "        \n",
    "        df1.sort_values(by=\"s_date\", inplace=True, ascending=True)\n",
    "#         print(df1)\n",
    "        \n",
    "        no_fire_list = []\n",
    "\n",
    "        loop_x = 1\n",
    "        for index, row in df1.iterrows():\n",
    "            x = row[\"burnt\"]\n",
    "            print(loop_x)\n",
    "            print(\"burnt: \", x)\n",
    "#             print(\"len of list: \", len(no_fire_list))\n",
    "            if x == 0:\n",
    "                if loop_x == 1:\n",
    "                    print(f\"No fire recorded on {str(row['s_date'])}, it is unknown if a fire occured the year before - nan appended\")\n",
    "                    years_since_list.append(np.nan)\n",
    "                else:\n",
    "                    \n",
    "                    no_fire_list.append(1)\n",
    "                    print(f\"No fire recorded on {str(row['s_date'])}, however, fire was recorded {str(len(no_fire_list))} year ago - {str(len(no_fire_list))} appended\")\n",
    "                    years_since_list.append(len(no_fire_list))\n",
    "\n",
    "            else:\n",
    "                if loop_x == 1:\n",
    "                    print(f\"Fire recorded on {str(row['s_date'])}, it is unknown if a fire occured the year before - nan appended\")\n",
    "                    years_since_list.append(np.nan)\n",
    "                else:\n",
    "                    print(f\"Fire recorded on {str(row['s_date'])}, and fire was recorded {str(len(no_fire_list)+1)} year ago - {str(len(no_fire_list)+1)} appended\")\n",
    "                    years_since_list.append(len(no_fire_list)+1)\n",
    "                    no_fire_list = []\n",
    "            loop_x += 1\n",
    "                    \n",
    "        print(\"years_since: \", years_since_list)\n",
    "        df1[\"since_fire\"] = years_since_list\n",
    "        list_.append(df1)\n",
    "                \n",
    "            \n",
    "    df3 = pd.concat(list_, axis=0)    \n",
    "    return(df3)\n",
    "\n",
    "                \n",
    "    return(df3)  \n",
    "\n",
    "def fire_gap_fn(df):\n",
    "    \n",
    "    list_ = []\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    for i in df.site.unique():\n",
    "        years_since_list = []\n",
    "        print(i)\n",
    "        df1 = df[df[\"site\"]==i]\n",
    "        \n",
    "        x = df1.since_fire.mean()\n",
    "        print(\"x: \", x)\n",
    "        \n",
    "        df1[\"fire_gap\"] = x\n",
    "\n",
    "        list_.append(df1)\n",
    "    \n",
    "    df2 = pd.concat(list_, axis=0) \n",
    "    \n",
    "    return df2\n",
    "                       \n",
    "    \n",
    "def ratio_fire_year_fn(x, y, p, n):\n",
    "\n",
    "    # data number of fires per data lenght of time = x/y\n",
    "    # revised time frame i.e. number of fires per time restriction = p/n\n",
    "    \n",
    "    final =  (x*n) / (p*y) \n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "def poisson_fn(df, p, n):\n",
    "    \n",
    "    list_ = []\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    for i in df.site.unique():\n",
    "        years_since_list = []\n",
    "        print(i)\n",
    "        df1 = df[df[\"site\"]==i]\n",
    "        \n",
    "\n",
    "        x = df1[\"fire_tot\"].tolist()[0] # average number of fires per data total years \n",
    "        y = len(df1.index)+1 # total number of years\n",
    "        n = n # number of year time frame \n",
    "        p = p # how many fires per n\n",
    "\n",
    "\n",
    "        k = np.arange(0, n + 1)\n",
    "        # print(k)\n",
    "\n",
    "        m = ratio_fire_year_fn(x, y, p, n)\n",
    "        print(m)\n",
    "\n",
    "        pmf = poisson.pmf(k, mu=m)\n",
    "        pmf = np.round(pmf, 5)\n",
    "\n",
    "        print(pmf)\n",
    "\n",
    "        for val, prob in zip(k, pmf):\n",
    "            if val == p:\n",
    "                print(f\"Within a {n} period, there is a {prob*100} that {val} fires will occur.\")\n",
    "        \n",
    "        df1[f\"fire_pois{p}_{n}\"] = prob*100\n",
    "        \n",
    "        list_.append(df1)\n",
    "\n",
    "    \n",
    "    \n",
    "    df2 = pd.concat(list_, axis=0) \n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "def double_digit_month_fn(d, year_):\n",
    "    print(d)\n",
    "    \n",
    "    if int(d) < 10:\n",
    "        month_ = f\"0{d}\"\n",
    "    else:\n",
    "        month_ = f\"{d}\"  \n",
    "    year_month = f\"{str(year_)}{month_}\"\n",
    "\n",
    "                        \n",
    "    return year_month\n",
    "\n",
    "\n",
    "def fire_scar_year_month_fn(df, month_list):\n",
    "    \"\"\" Calculate the percent cover burnt by fire. \"\"\"\n",
    "    df.fillna(0, inplace =True)\n",
    "    site_list = []\n",
    "    burnt_start_list = []\n",
    "    burnt_end_list = []\n",
    "    burnt_year_list = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        month__ = []\n",
    "        site_list.append(row.site)\n",
    "        burnt_year_list.append(str(row.im_date))\n",
    "        burnt_month_list = []\n",
    "        for month, d in zip(month_list, month_d_list):\n",
    "                \n",
    "            if int(row[f\"{month}\"]) > 0:\n",
    "                \n",
    "                month__.append(d)\n",
    "\n",
    "                \n",
    "                if d < 10:\n",
    "                    month_ = f\"0{d}\"\n",
    "                else:\n",
    "                    month_ = f\"{d}\"  \n",
    "                year_month = f\"{str(row.im_date)}{month_}\"\n",
    "                burnt_month_list.append(year_month)\n",
    "                \n",
    "        acend_month_list = sorted(month__, reverse=False)\n",
    "\n",
    "        if month__:\n",
    "            if len(month__) > 1:          \n",
    "                year_month = double_digit_month_fn(str(month__[0]), str(row.im_date))\n",
    "                print(f\"{row.site} has the following fire scars: {acend_month_list}\")\n",
    "                burnt_start_list.append(year_month)\n",
    "                burnt_end_list.append(year_month)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #calculate first fire scar\n",
    "                year_month = double_digit_month_fn(str(month__[0]), str(row.im_date))\n",
    "                print(f\"{row.site} has the following fire scars: {acend_month_list}\")\n",
    "                burnt_start_list.append(year_month)\n",
    "                \n",
    "                        \n",
    "                # calculate last fire scar        \n",
    "                year_month = double_digit_month_fn(str(month__[-1]), str(row.im_date))\n",
    "                print(f\"{row.site} has the following fire scars: {acend_month_list}\")\n",
    "                burnt_end_list.append(year_month)\n",
    "                \n",
    "        else:\n",
    "            burnt_start_list.append(0)\n",
    "            burnt_end_list.append(0)\n",
    "                           \n",
    "    return site_list, burnt_start_list, burnt_end_list, burnt_year_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_file_fn(df, dir_, file_name):\n",
    "    \n",
    "    output_path = os.path.join(dir_, file_name)\n",
    "    df.to_csv(os.path.join(output_path), index=False)\n",
    "    print(\"File output to: \", output_path)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinate individual site data per type and export df's and merge data asof with basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = next(os.walk(dir_))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zonal_list = []\n",
    "sub_dir_list = []\n",
    "# single_list = []\n",
    "# year_list = []\n",
    "# seasonal_list = []\n",
    "# dja_list = []\n",
    "# dbi_list = []\n",
    "# dim_list = []\n",
    "# dis_list = []\n",
    "# dka_list = []\n",
    "# dp0_list = []\n",
    "# dp1_list = []\n",
    "# h99a2_list = []\n",
    "# fpca2_list = []\n",
    "# stc_list = []\n",
    "\n",
    "\n",
    "\n",
    "cleaned_df_list = []\n",
    "cleaned_str_list = []\n",
    "\n",
    "\n",
    "\n",
    "#create lists for merged ouput data\n",
    "dka_0112_list = []\n",
    "\n",
    "dim_0305_list = []\n",
    "dim_0608_list = []\n",
    "dim_0911_list = []\n",
    "dim_1202_list =[]\n",
    "                    \n",
    "dis_0305_list = []\n",
    "dis_0608_list = []\n",
    "dis_0911_list = []\n",
    "dis_1202_list =[]\n",
    "\n",
    "# dbi_0305_list = []\n",
    "dbi_0608_list = []\n",
    "dbi_0911_list = []\n",
    "dbi_1202_list =[]\n",
    "\n",
    "dja_0305_list = []\n",
    "dja_0608_list = []\n",
    "dja_0911_list = []\n",
    "dja_1202_list =[]\n",
    "\n",
    "h99a2_0112_list = []\n",
    "fpca2_0509_list = []\n",
    "\n",
    "stc_0112_list = []\n",
    "\n",
    "\n",
    "\n",
    "for sub_dir in sub_list:\n",
    "\n",
    "    file_list = []\n",
    "    if \"zonal_stats\" in sub_dir:\n",
    "        sub_dir_list.append(sub_dir)\n",
    "#         print(sub_dir)\n",
    "#         print(\"looking in : \", os.path.join(dir_, sub_dir, \"*.csv\"))\n",
    "        for file_ in glob(os.path.join(dir_, sub_dir, \"*.csv\")):\n",
    "#             print(file_)\n",
    "            df = pd.read_csv(file_)\n",
    "            file_list.append(df)\n",
    "#             print(\"appended: \", file_)\n",
    "            \n",
    "    if len(file_list) > 1:\n",
    "        df1 = pd.concat(file_list)\n",
    "#         print(\"+\"*50)\n",
    "#         print(df1.shape)\n",
    "\n",
    "        if \"date\" in df1.columns and \"im_date\" not in df1.columns:\n",
    "#             print(df1.columns)\n",
    "            \n",
    "            df1.rename(columns = {\"date\": \"im_date\"}, inplace = True)\n",
    "            \n",
    "#             print(df1.columns)\n",
    "            \n",
    "\n",
    "        print(\"+\"*50)\n",
    "        print(sub_dir)\n",
    "        print(\"+\"*50)\n",
    "        \n",
    "        if sub_dir == \"dka_zonal_stats\":\n",
    "#         if sub_dir == \"dka_zonal_stats\":\n",
    "#             print(df1.columns)\n",
    "            dka = df1.copy(deep = True)\n",
    "            if \"date\" in dka.columns and \"im_date\" not in dka.columns:\n",
    "#                 print(dka.columns)\n",
    "\n",
    "                dka.rename(columns = {\"date\": \"im_date\"}, inplace = True)\n",
    "            dka = im_date_annual(dka)\n",
    "            \n",
    "            var_ = \"dka\"\n",
    "            dka_dict = {\"count\":  \"{0}_count\".format(var_), \n",
    "                        \"min\": \"{0}_min\".format(var_), \n",
    "                        \"max\" :\"{0}_max\".format(var_),\n",
    "                        \"mean\": \"{0}_mean\".format(var_), \n",
    "                        \"sum\": \"{0}_sum\".format(var_), \n",
    "                        \"std\": \"{0}_std\".format(var_), \n",
    "                        \"median\": \"{0}_med\".format(var_),\n",
    "                         \"majority\" : \"{0}_major\".format(var_), \n",
    "                        \"minority\": \"{0}_minor\".format(var_), \n",
    "                        \"one\": \"{0}_one\".format(var_), \n",
    "                        \"two\": \"{0}_two\".format(var_), \n",
    "                        \"three\": \"{0}_three\".format(var_), \n",
    "                        \"four\": \"{0}_four\".format(var_), \n",
    "                        \"five\": \"{0}_five\".format(var_), \n",
    "                        \"six\": \"{0}_six\".format(var_), \n",
    "                        \"seven\": \"{0}_seven\".format(var_), \n",
    "                        \"eight\": \"{0}_eight\".format(var_), \n",
    "                         \"nine\" : \"{0}_nine\".format(var_), \n",
    "                        \"ten\": \"{0}_ten\".format(var_)}\n",
    "                        \n",
    "                           \n",
    "            dka.rename(columns = dka_dict, inplace = True)\n",
    "\n",
    "            dka_s_ = convert_to_datetime(dka, \"s_date\", \"image_s_dt\")\n",
    "            dka_s = convert_to_datetime(dka_s_, \"e_date\", \"image_e_dt\")\n",
    "            dka_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            dka_s.dropna(subset = ['dka_min'], inplace=True)\n",
    "            \n",
    "            # call fire frequency etc functions\n",
    "            dka_s = fire_percent_fn(dka_s)\n",
    "            dka_s = fire_yn_fn(dka_s)\n",
    "            dka_s = fire_intensity_fn(dka_s)\n",
    "            dka_s = prop_fire_freq_fn(dka_s)\n",
    "            dka_s = fire_previous_year(dka_s)\n",
    "            dka_s = fire_gap_fn(dka_s)\n",
    "            dka_s = poisson_fn(dka_s, 1, 2)\n",
    "            dka_s = poisson_fn(dka_s, 1, 5)\n",
    "            dka_s = poisson_fn(dka_s, 1, 10)\n",
    "            \n",
    "#             dka_s.to_csv(os.path.join(temp_dir, \"s_dkk.csv\"), index=False)\n",
    "            \n",
    "#             dka_s = fire_fn(dka_s)\n",
    "#             df = dka_s[dka_s['dka_min'].isnull()]\n",
    "#             print(\"+\"*100)\n",
    "#             print(df.shape)\n",
    "            dka_s.sort_values(by=\"image_s_dt\", inplace=True)\n",
    "            dka_s.dropna(subset = ['dka_min'], inplace=True)\n",
    "            print(dka_s.shape)\n",
    "            dka_s.drop_duplicates(inplace=True)\n",
    "            print(dka_s.shape)\n",
    "            \n",
    "            export_csv_file_fn(dka_s, ftzs_dir, \"dka_all_seasons_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dka_s, szs_dir, \"dka_0112_zonal_stats.csv\")\n",
    "            cleaned_df_list.append(dka_s)\n",
    "            cleaned_str_list.append(\"dka_0112\")\n",
    "            \n",
    "\n",
    "#             basal_df.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\seasonal_zonal_stats\\test_basal_dd.csv\")\n",
    "            #merge data with basal datset based on the nearest date to the field data colection\n",
    "            dka_s_single = pd.merge_asof(basal_df, dka_s, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "            dka_s_single.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": f\"uid_{x_str}\"}, inplace = True)\n",
    "            #export_csv_file_fn(dka_s_single, ftzs_dir, \"dka_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "            dka_0112_list.append(dka_s_single)\n",
    "            \n",
    "            print(\"DKA - Done\")\n",
    "            \n",
    "            \n",
    "        elif sub_dir == \"dim_zonal_stats\":\n",
    "            \n",
    "            # REVISED FOR SEASONAL\n",
    "            \n",
    "            print(df1.columns)\n",
    "            dim = df1.copy(deep = True)\n",
    "            \n",
    "#             output_path = os.path.join(szs_dir, \"dim_all_seasons_zonal_stats.csv\")\n",
    "#             dim.to_csv(os.path.join(output_path), index=False)\n",
    "#             print(\"File output to: \", output_path)\n",
    "            dim_s_ = convert_to_datetime(dim, \"s_date\", \"image_s_dt\")\n",
    "            dim_s = convert_to_datetime(dim_s_, \"e_date\", \"image_e_dt\")\n",
    "            dim_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            dim_s.dropna(subset = ['b1_dim_min'], inplace=True)\n",
    "            print(dim_s.shape)\n",
    "            \n",
    "            export_csv_file_fn(dim_s, ftzs_dir, \"dim_all_seasons_zonal_stats.csv\")\n",
    "\n",
    "            \n",
    "            dim_0305 = dim_s[dim_s[\"s_month\"]==3]\n",
    "            print(\"dim_0305.shape: \", dim_0305.shape)\n",
    "            dim_0608 = dim_s[dim_s[\"s_month\"]==6]\n",
    "            print(\"dim_0608.shape: \", dim_0608.shape)\n",
    "            dim_0911 = dim_s[dim_s[\"s_month\"]==9]\n",
    "            print(\"dim_0911.shape: \", dim_0911.shape)\n",
    "            dim_1202 = dim_s[dim_s[\"s_month\"]==12]\n",
    "            print(\"dim_1202.shape: \", dim_1202.shape)\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            \n",
    "            export_csv_file_fn(dim_0305, szs_dir, \"dim_0305_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dim_0608, szs_dir, \"dim_0608_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dim_0911, szs_dir, \"dim_0911_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dim_1202, szs_dir, \"dim_1202_zonal_stats.csv\")\n",
    "            \n",
    "            # append data to list \n",
    "            cleaned_df_list.append(dim_0305)\n",
    "            cleaned_str_list.append(\"dim_0305\")\n",
    "            \n",
    "            cleaned_df_list.append(dim_0608)\n",
    "            cleaned_str_list.append(\"dim_0608\")\n",
    "            \n",
    "            cleaned_df_list.append(dim_0911)\n",
    "            cleaned_str_list.append(\"dim_0911\")\n",
    "            \n",
    "            cleaned_df_list.append(dim_1202)\n",
    "            cleaned_str_list.append(\"dim_1202\")\n",
    "            \n",
    "                 \n",
    "            for df_x, x_str in zip([dim_0305, dim_0608, dim_0911, dim_1202],[\"dim_0305\", \"dim_0608\", \"dim_0911\", \"dim_1202\"]):\n",
    "            \n",
    "#                 dim_s = convert_to_datetime(df_x, \"s_date\", \"image_s_dt\")\n",
    "#                 dim_s.sort_values(by=\"s_date\", inplace=True)\n",
    "#                 dim_s.dropna(subset = ['b1_dim_min'], inplace=True)\n",
    "#                 print(dim_s.shape)\n",
    "\n",
    "                #merge data with basal datset based on the nearest date to the field data colection\n",
    "                dim_s_single = pd.merge_asof(basal_df, df_x, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "                dim_s_single.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": f\"uid_{x_str}\"}, inplace = True)\n",
    "            \n",
    "                export_csv_file_fn(dim_s_single, revised_fire_scar_dir, f\"agb_nt_mosaic_{x_str}_fmna.csv\")\n",
    "                \n",
    "                if x_str == \"dim_0305\":\n",
    "                    dim_0305_list.append(dim_s_single)\n",
    "                elif x_str == \"dim_0608\":\n",
    "                    dim_0608_list.append(dim_s_single)\n",
    "                elif x_str == \"dim_0911\":\n",
    "                    dim_0911_list.append(dim_s_single)\n",
    "                elif x_str == \"dim_1202\":\n",
    "                    dim_1202_list.append(dim_s_single)\n",
    "                else:\n",
    "                    print(\"DIM error\")\n",
    "                    import sys\n",
    "                    sys.exit()\n",
    "                    \n",
    "            print(\"DIM Done\")\n",
    "            \n",
    "            \n",
    "\n",
    "        elif sub_dir == \"dis_zonal_stats\":\n",
    "            \n",
    "            # WORKING - SEASONAL SPLIT\n",
    "            print(df1.columns)\n",
    "            dis_ = df1.copy(deep = True)\n",
    "            \n",
    "            var_ = \"dis\"\n",
    "            dis_dict = {\"count\":  \"{0}_count\".format(var_), \n",
    "                        \"min\": \"{0}_min\".format(var_), \n",
    "                        \"max\" :\"{0}_max\".format(var_),\n",
    "                        \"mean\": \"{0}_mean\".format(var_), \n",
    "                        \"sum\": \"{0}_sum\".format(var_), \n",
    "                        \"std\": \"{0}_std\".format(var_), \n",
    "                        \"median\": \"{0}_med\".format(var_),\n",
    "                         \"majority\" : \"{0}_major\".format(var_), \n",
    "                        \"minority\": \"{0}_minor\".format(var_), \n",
    "                        \"one\": \"{0}_one\".format(var_), \n",
    "                        \"two\": \"{0}_two\".format(var_), \n",
    "                        \"three\": \"{0}_three\".format(var_), \n",
    "                        \"four\": \"{0}_four\".format(var_), \n",
    "                        \"five\": \"{0}_five\".format(var_), \n",
    "                        \"six\": \"{0}_six\".format(var_), \n",
    "                        \"seven\": \"{0}_seven\".format(var_), \n",
    "                        \"eight\": \"{0}_eight\".format(var_), \n",
    "                         \"nine\" : \"{0}_nine\".format(var_), \n",
    "                        \"ten\": \"{0}_ten\".format(var_)}\n",
    "\n",
    "\n",
    "            dis_.rename(columns = dis_dict, inplace = True)\n",
    "\n",
    "            dis_s_ = convert_to_datetime(dis_, \"s_date\", \"image_s_dt\")\n",
    "            dis_s = convert_to_datetime(dis_s_, \"e_date\", \"image_e_dt\")\n",
    "            dis_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            dis_s.dropna(subset = ['dis_min'], inplace=True)\n",
    "            \n",
    "            export_csv_file_fn(dis_s, ftzs_dir, \"dis_all_seasons_zonal_stats.csv\")       \n",
    "                \n",
    "            dis_0305 = dis_s[dis_s[\"s_month\"]==3]\n",
    "            dis_0608 = dis_s[dis_s[\"s_month\"]==6]\n",
    "            dis_0911 = dis_s[dis_s[\"s_month\"]==9]\n",
    "            dis_1202 = dis_s[dis_s[\"s_month\"]==12]\n",
    "            \n",
    "            export_csv_file_fn(dis_0305, szs_dir, \"dis_0305_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dis_0608, szs_dir, \"dis_0608_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dis_0911, szs_dir, \"dis_0911_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dis_1202, szs_dir, \"dis_1202_zonal_stats.csv\")\n",
    "            \n",
    "                        \n",
    "            # append data to list \n",
    "            cleaned_df_list.append(dis_0305)\n",
    "            cleaned_str_list.append(\"dis_0305\")\n",
    "            \n",
    "            cleaned_df_list.append(dis_0608)\n",
    "            cleaned_str_list.append(\"dis_0608\")\n",
    "            \n",
    "            cleaned_df_list.append(dis_0911)\n",
    "            cleaned_str_list.append(\"dis_0911\")\n",
    "            \n",
    "            cleaned_df_list.append(dis_1202)\n",
    "            cleaned_str_list.append(\"dis_1202\")\n",
    "            \n",
    "        \n",
    "            for df_x, x_str in zip([dis_0305, dis_0608, dis_0911, dis_1202],[\"dis_0305\", \"dis_0608\", \"dis_0911\", \"dis_1202\"]):\n",
    "                                \n",
    "\n",
    "                #merge data with basal datset based on the nearest date to the field data colection\n",
    "                dis_s_single = pd.merge_asof(basal_df, dis_s, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "                dis_s_single.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": f\"uid_{x_str}\"}, inplace = True)\n",
    "                export_csv_file_fn(dis_s_single, revised_fire_scar_dir, f\"agb_nt_mosaic_{x_str}_fmna.csv\")\n",
    "                 \n",
    "                if x_str == \"dis_0305\":\n",
    "                    dis_0305_list.append(dis_s_single)\n",
    "                elif x_str == \"dis_0608\":\n",
    "                    dis_0608_list.append(dis_s_single)\n",
    "                elif x_str == \"dis_0911\":\n",
    "                    dis_0911_list.append(dis_s_single)\n",
    "                elif x_str == \"dis_1202\":\n",
    "                    dis_1202_list.append(dis_s_single)\n",
    "                else:\n",
    "                    print(\"dis error\")\n",
    "                    import sys\n",
    "                    sys.exit()\n",
    "                    \n",
    "            print(\"dis Done\")\n",
    "                \n",
    "                \n",
    "        elif sub_dir == \"dja_zonal_stats\":\n",
    "\n",
    "            print(df1.columns)\n",
    "            dja = df1.copy(deep = True)\n",
    "            \n",
    "            dja_s_ = convert_to_datetime(dja, \"s_date\", \"image_s_dt\")\n",
    "            dja_s = convert_to_datetime(dja_s_, \"e_date\", \"image_e_dt\")\n",
    "            dja_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            dja_s.dropna(subset = ['b1_dja_min'], inplace=True)\n",
    "\n",
    "\n",
    "            export_csv_file_fn(dja_s, ftzs_dir, \"dja_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "            dja_0305 = dja_s[dja_s[\"s_month\"]==3]\n",
    "            dja_0608 = dja_s[dja_s[\"s_month\"]==6]\n",
    "            dja_0911 = dja_s[dja_s[\"s_month\"]==9]\n",
    "            dja_1202 = dja_s[dja_s[\"s_month\"]==12]\n",
    "            \n",
    "            export_csv_file_fn(dja_0305, szs_dir, \"dja_0305_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dja_0608, szs_dir, \"dja_0608_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dja_0911, szs_dir, \"dja_0911_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dja_1202, szs_dir, \"dja_1202_zonal_stats.csv\")\n",
    "            \n",
    "            \n",
    "            # append data to list \n",
    "            cleaned_df_list.append(dja_0305)\n",
    "            cleaned_str_list.append(\"dja_0305\")\n",
    "            \n",
    "            cleaned_df_list.append(dja_0608)\n",
    "            cleaned_str_list.append(\"dja_0608\")\n",
    "            \n",
    "            cleaned_df_list.append(dja_0911)\n",
    "            cleaned_str_list.append(\"dja_0911\")\n",
    "            \n",
    "            cleaned_df_list.append(dja_1202)\n",
    "            cleaned_str_list.append(\"dja_1202\")\n",
    "            \n",
    "            for df_x, x_str in zip([dja_0305, dja_0608, dja_0911, dja_1202],[\"dja_0305\", \"dja_0608\", \"dja_0911\", \"dja_1202\"]):\n",
    "                                \n",
    "\n",
    "                #merge data with basal datset based on the nearest date to the field data colection\n",
    "                dja_s_single = pd.merge_asof(basal_df, dja_s, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "                dja_s_single.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": f\"uid_{x_str}\"}, inplace = True)\n",
    "                export_csv_file_fn(dja_s_single, revised_fire_scar_dir, f\"agb_nt_mosaic_{x_str}_fmna.csv\") \n",
    "\n",
    "                if x_str == \"dja_0305\":\n",
    "                    dja_0305_list.append(dja_s_single)\n",
    "                elif x_str == \"dja_0608\":\n",
    "                    dja_0608_list.append(dja_s_single)\n",
    "                elif x_str == \"dja_0911\":\n",
    "                    dja_0911_list.append(dja_s_single)\n",
    "                elif x_str == \"dja_1202\":\n",
    "                    dja_1202_list.append(dja_s_single)\n",
    "                else:\n",
    "                    print(\"dja error\")\n",
    "                    import sys\n",
    "                    sys.exit()\n",
    "                    \n",
    "            print(\"DJA Done\")\n",
    "            \n",
    "        elif sub_dir == \"dbi_zonal_stats\":\n",
    "\n",
    "            print(df1.columns)\n",
    "            dbi = df1.copy(deep = True)\n",
    "            \n",
    "            dbi_s_ = convert_to_datetime(dbi, \"s_date\", \"image_s_dt\")\n",
    "            dbi_s = convert_to_datetime(dbi_s_, \"e_date\", \"image_e_dt\")\n",
    "            dbi_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            dbi_s.dropna(subset = ['b1_dbi_min'], inplace=True)\n",
    "            export_csv_file_fn(dbi_s, ftzs_dir, \"dbi_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "#             dbi_0305 = dbi_s[dbi_s[\"s_month\"]==3]\n",
    "            dbi_0608 = dbi_s[dbi_s[\"s_month\"]==6]\n",
    "            dbi_0911 = dbi_s[dbi_s[\"s_month\"]==9]\n",
    "            dbi_1202 = dbi_s[dbi_s[\"s_month\"]==12]\n",
    "            \n",
    "#             export_csv_file_fn(dbi_0305, szs_dir, \"dbi_0305_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dbi_0608, szs_dir, \"dbi_0608_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dbi_0911, szs_dir, \"dbi_0911_zonal_stats.csv\")\n",
    "            export_csv_file_fn(dbi_1202, szs_dir, \"dbi_1202_zonal_stats.csv\")\n",
    "            \n",
    "            # append data to list \n",
    "#             cleaned_df_list.append(dis_0305)\n",
    "#             cleaned_str_list.append(\"dis_0305\")\n",
    "            \n",
    "            cleaned_df_list.append(dbi_0608)\n",
    "            cleaned_str_list.append(\"dbi_0608\")\n",
    "            \n",
    "            cleaned_df_list.append(dbi_0911)\n",
    "            cleaned_str_list.append(\"dbi_0911\")\n",
    "            \n",
    "            cleaned_df_list.append(dbi_1202)\n",
    "            cleaned_str_list.append(\"dbi_1202\")\n",
    "            \n",
    "            for df_x, x_str in zip([dbi_0608, dbi_0911, dbi_1202],[\"dbi_0608\", \"dbi_0911\", \"dbi_1202\"]):\n",
    "                                \n",
    "\n",
    "                #merge data with basal datset based on the nearest date to the field data colection\n",
    "                dbi_s_single = pd.merge_asof(basal_df, dbi_s, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "                dbi_s_single.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": f\"uid_{x_str}\"}, inplace = True)\n",
    "                export_csv_file_fn(dbi_s_single, revised_fire_scar_dir, f\"agb_nt_mosaic_{x_str}_fmna.csv\") \n",
    "                \n",
    "#                 if x_str == \"dbi_0305\":\n",
    "#                     dbi_0305_list.append(dbi_s_single)\n",
    "                if x_str == \"dbi_0608\":\n",
    "                    dbi_0608_list.append(dbi_s_single)\n",
    "                elif x_str == \"dbi_0911\":\n",
    "                    dbi_0911_list.append(dbi_s_single)\n",
    "                elif x_str == \"dbi_1202\":\n",
    "                    dbi_1202_list.append(dbi_s_single)\n",
    "                else:\n",
    "                    print(\"DBI Error\")\n",
    "                    import sys\n",
    "                    sys.exit()\n",
    "                    \n",
    "            print(\"DBI Done\")\n",
    "            \n",
    "        elif sub_dir == \"h99a2_zonal_stats\":\n",
    "            \n",
    "            print(df1.columns)\n",
    "            h99a2_s = df1.copy(deep = True)\n",
    "            \n",
    "            h99a2_s_ = convert_to_datetime(h99a2_s, \"s_date\", \"image_s_dt\")\n",
    "            h99a2_0112 = convert_to_datetime(h99a2_s_, \"e_date\", \"image_e_dt\")\n",
    "\n",
    "            h99a2_0112_df = h99a2_0112[h99a2_0112[\"s_month\"] == 1]\n",
    "            h99a2_0112_df.sort_values(by=\"image_s_dt\", inplace=True)\n",
    "            h99a2_0112_df.dropna(subset = ['b1_h99a2_min'], inplace=True)\n",
    "            \n",
    "            export_csv_file_fn(h99a2_0112_df, ftzs_dir, \"h99a2_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "            # append data to list \n",
    "            cleaned_df_list.append( h99a2_0112_df)\n",
    "            cleaned_str_list.append(\"h99a2_0112\")\n",
    "\n",
    "            h99a2_0112_s = pd.merge_asof(basal_df, h99a2_0112_df, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"nearest\")\n",
    "            h99a2_0112_s.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": \"uid_h99a2_0112\"}, inplace = True)\n",
    "            export_csv_file_fn(h99a2_0112_s, revised_fire_scar_dir, f\"agb_nt_mosaic_h99a2_0112_fmna.csv\") \n",
    "            \n",
    "            h99a2_0112_list.append(h99a2_0112_s)          \n",
    "            print(\"H99A2 - Done\")\n",
    "\n",
    "\n",
    "        elif sub_dir == \"fpca2_zonal_stats\":\n",
    "            \n",
    "            print(df1.columns)\n",
    "            fpca2_dry_ = df1.copy(deep = True)\n",
    "            \n",
    "            fpca2_dry_s_ = convert_to_datetime(fpca2_dry_, \"s_date\", \"image_s_dt\")\n",
    "            fpca2_dry = convert_to_datetime(fpca2_dry_s_, \"e_date\", \"image_e_dt\")\n",
    "\n",
    "            fpca2_dry_df = fpca2_dry[fpca2_dry[\"s_month\"] == 5]\n",
    "            fpca2_dry_df.sort_values(by=\"s_date\", inplace=True)\n",
    "            fpca2_dry_df.dropna(subset = ['b1_fpca2_min'], inplace=True)\n",
    "            \n",
    "            export_csv_file_fn(fpca2_dry_df, ftzs_dir, \"fpca2_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "            # append data to list \n",
    "            cleaned_df_list.append(fpca2_dry_df)\n",
    "            cleaned_str_list.append(\"fpca2_0509\")\n",
    "            \n",
    "            fpca2_0509 = pd.merge_asof(basal_df, fpca2_dry_df, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"nearest\")\n",
    "            fpca2_0509.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": \"uid_fpca2_0509\"}, inplace = True)\n",
    "            export_csv_file_fn(fpca2_0509, revised_fire_scar_dir, f\"agb_nt_mosaic_fpca2_0509_fmna.csv\") \n",
    "\n",
    "            fpca2_0509_list.append(fpca2_0509)            \n",
    "            print(\"FPCA2 - Done\")\n",
    "\n",
    "            \n",
    "        elif sub_dir == \"stc_zonal_stats\":\n",
    "            print(df1.columns)\n",
    "            stc = df1.copy(deep = True)\n",
    "\n",
    "            \n",
    "            var_ = \"stc\"\n",
    "            stc_dict = {\"count\":  \"{0}_count\".format(var_), \n",
    "                        \"min\": \"{0}_min\".format(var_), \n",
    "                        \"max\" :\"{0}_max\".format(var_),\n",
    "                        \"mean\": \"{0}_mean\".format(var_), \n",
    "                        \"sum\": \"{0}_sum\".format(var_), \n",
    "                        \"std\": \"{0}_std\".format(var_), \n",
    "                        \"median\": \"{0}_med\".format(var_),\n",
    "                         \"majority\" : \"{0}_major\".format(var_), \n",
    "                        \"minority\": \"{0}_minor\".format(var_), \n",
    "                        \"one\": \"{0}_one\".format(var_), \n",
    "                        \"two\": \"{0}_two\".format(var_), \n",
    "                        \"three\": \"{0}_three\".format(var_), \n",
    "                        \"four\": \"{0}_four\".format(var_), \n",
    "                        \"five\": \"{0}_five\".format(var_), \n",
    "                        \"six\": \"{0}_six\".format(var_), \n",
    "                        \"seven\": \"{0}_seven\".format(var_), \n",
    "                        \"eight\": \"{0}_eight\".format(var_), \n",
    "                         \"nine\" : \"{0}_nine\".format(var_), \n",
    "                        \"ten\": \"{0}_ten\".format(var_),\n",
    "                        \"eleven\": \"{0}_elev\".format(var_), \n",
    "                        \"twelve\": \"{0}_twelv\".format(var_), \n",
    "                        \"thirteen\": \"{0}_thirt\".format(var_), \n",
    "                        \"fourteen\": \"{0}_fourt\".format(var_), \n",
    "                        \"fifteen\": \"{0}_fift\".format(var_), \n",
    "                        \"sixteen\": \"{0}_sixt\".format(var_), \n",
    "                        \"seventeen\": \"{0}_sevent\".format(var_)}\n",
    "            \n",
    "       \n",
    "                           \n",
    "            stc.rename(columns = stc_dict, inplace = True)\n",
    "\n",
    "            stc_s_ = convert_to_datetime(stc, \"s_date\", \"image_s_dt\")\n",
    "            stc_s = convert_to_datetime(stc_s_, \"e_date\", \"image_e_dt\")\n",
    "            stc_s.sort_values(by=\"s_date\", inplace=True)\n",
    "            stc_s.dropna(subset = ['stc_min'], inplace=True)\n",
    "            export_csv_file_fn(stc_s, ftzs_dir, \"stc_all_seasons_zonal_stats.csv\")\n",
    "\n",
    "             # append data to list \n",
    "            cleaned_df_list.append(stc_s)\n",
    "            cleaned_str_list.append(\"stc_0112\")\n",
    "            \n",
    "\n",
    "            #merge data with basal datset based on the nearest date to the field data colection\n",
    "            stc_0112 = pd.merge_asof(basal_df, stc_s, left_on=\"basal_dt\", right_on= \"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "            stc_0112.rename(columns = {\"uid_x\": \"uid\", \"uid_y\": \"uid_stc_0112\"}, inplace = True)\n",
    "            export_csv_file_fn(stc_0112, revised_fire_scar_dir, f\"agb_nt_mosaic_stc_0112_fmna.csv\") \n",
    "\n",
    "            stc_0112_list.append(stc_0112)            \n",
    "            print(\"STC - Done\")\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(\"FAILED\")\n",
    "            print(\"+\"*50)\n",
    "            print(sub_dir)\n",
    "            print(\"+\"*50)\n",
    "            import sys\n",
    "            sys.exit()\n",
    "            \n",
    "# # ------------------------------- Basal ----------------------------------------\n",
    "\n",
    "stc_0112_basal_nfs = export_csv_fn(stc_0112_list, w_no_fire_scar_basal_dir, \"stc_0112_basal_with_y_fire_mask_not_applied.csv\")\n",
    "fpca_0509_basal_nfs = export_csv_fn(fpca2_0509_list, w_no_fire_scar_basal_dir, \"fpca2_0509_basal_with_y_fire_mask_not_applied.csv\")\n",
    "h99a_0112_basal_nfs = export_csv_fn(h99a2_0112_list, w_no_fire_scar_basal_dir, \"h99a_0112_basal_with_y_fire_mask_not_applied.csv\")\n",
    "\n",
    "dbi_0608_basal_nfs = export_csv_fn(dbi_0608_list, w_no_fire_scar_basal_dir, \"dbi_0608_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dbi_0911_basal_nfs = export_csv_fn(dbi_0911_list, w_no_fire_scar_basal_dir, \"dbi_0911_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dbi_1202_basal_nfs = export_csv_fn(dbi_1202_list, w_no_fire_scar_basal_dir, \"dbi_1202_basal_with_y_fire_mask_not_applied.csv\")\n",
    "\n",
    "dja_0305_basal_nfs = export_csv_fn(dja_0305_list, w_no_fire_scar_basal_dir, \"dja_0305_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dja_0608_basal_nfs = export_csv_fn(dja_0608_list, w_no_fire_scar_basal_dir, \"dja_0608_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dja_0911_basal_nfs = export_csv_fn(dja_0911_list, w_no_fire_scar_basal_dir, \"dja_0911_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dja_1202_basal_nfs = export_csv_fn(dja_1202_list, w_no_fire_scar_basal_dir, \"dja_1202_basal_with_y_fire_mask_not_applied.csv\")\n",
    "\n",
    "dis_0305_basal_nfs = export_csv_fn(dis_0305_list, w_no_fire_scar_basal_dir, \"dis_0305_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dis_0608_basal_nfs = export_csv_fn(dis_0608_list, w_no_fire_scar_basal_dir, \"dis_0608_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dis_0911_basal_nfs = export_csv_fn(dis_0911_list, w_no_fire_scar_basal_dir, \"dis_0911_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dis_1202_basal_nfs = export_csv_fn(dis_1202_list, w_no_fire_scar_basal_dir, \"dis_1202_basal_with_y_fire_mask_not_applied.csv\")\n",
    "\n",
    "dim_0305_basal_nfs = export_csv_fn(dim_0305_list, w_no_fire_scar_basal_dir, \"dim_0305_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dim_0608_basal_nfs = export_csv_fn(dim_0608_list, w_no_fire_scar_basal_dir, \"dim_0608_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dim_0911_basal_nfs = export_csv_fn(dim_0911_list, w_no_fire_scar_basal_dir, \"dim_0911_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dim_1202_basal_nfs = export_csv_fn(dim_1202_list, w_no_fire_scar_basal_dir, \"dim_1202_basal_with_y_fire_mask_not_applied.csv\")\n",
    "\n",
    "dka_0112_basal_nfs = export_csv_fn(dka_0112_list, w_no_fire_scar_basal_dir, \"dka_0112_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dka_0112_basal_nfs = export_csv_fn(dka_0112_list, no_fire_scar_basal_dir, \"dka_0112_basal_with_y_fire_mask_applied.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP1  and season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dp1_0112_list = []\n",
    "dp1_0509_list = []\n",
    "\n",
    "\n",
    "dp1_annual = dp1_df[dp1_df[\"s_month\"]==1]\n",
    "dp1_dry = dp1_df[dp1_df[\"s_month\"]==5]\n",
    "\n",
    "\n",
    "for df, var_ in zip([dp1_annual, dp1_dry], [\"dp1_0112\", \"dp1_0509\"]):\n",
    "    \n",
    "    \n",
    "\n",
    "    print(df.columns)\n",
    "    dp1 = df.copy(deep = True)\n",
    "\n",
    "    dp1_s = convert_to_datetime(dp1, \"s_date\", \"image_s_dt\")\n",
    "    dp1_s.sort_values(by=\"s_date\", inplace=True)\n",
    "    dp1_s.dropna(subset=['b1_dp1_min'], inplace=True)\n",
    "    \n",
    "    export_csv_file_fn(dp1_s, ftzs_dir, f\"{var_}_all_seasons_zonal_stats.csv\")\n",
    "\n",
    "    # merge data with basal datset based on the nearest date to the field data colection\n",
    "    dp1_s_single = pd.merge_asof(basal_df, dp1_s, left_on=\"basal_dt\", right_on=\"image_s_dt\", by=\"site\", direction=\"forward\")\n",
    "    \n",
    "    dp1_s_single.rename(columns={\"uid_x\":\"uid\", \"uid_y\":f\"uid_{var_}\"}, inplace=True)\n",
    "    export_csv_file_fn(dp1_s_single, revised_fire_scar_dir, f\"agb_nt_mosaic_{var_}_fmna.csv\") \n",
    "\n",
    "    if var_ == \"dp1_0112\":\n",
    "        dp1_0112_list.append(dp1_s_single)\n",
    "        \n",
    "        # append data to list \n",
    "        cleaned_df_list.append(dp1_s)\n",
    "        cleaned_str_list.append(\"dp1_0112\")\n",
    "        \n",
    "    elif var_ == \"dp1_0509\":\n",
    "        dp1_0509_list.append(dp1_s_single)\n",
    "        \n",
    "        # append data to list \n",
    "        cleaned_df_list.append(dp1_s)\n",
    "        cleaned_str_list.append(\"dp1_0509\")\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        import sys\n",
    "        sys.exit()\n",
    "        \n",
    "# concatinate data here\n",
    "dp1_0112_basal_nfs = export_csv_fn(dp1_0112_list, w_no_fire_scar_basal_dir, \"dp1_0112_basal_with_y_fire_mask_not_applied.csv\")\n",
    "dp1_0509_basal_nfs = export_csv_fn(dp1_0509_list, w_no_fire_scar_basal_dir, \"dp1_0509_basal_with_y_fire_mask_not_applied.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List output df's and order before merge asof with basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define month list as per seasonal zonal stats\n",
    "month_list = [\"jan\", \"feb\", \"mar\", \"april\", \"may\", \"june\", \"july\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "month_d_list = np.arange(1, 13).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scar Year Month Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list, burnt_start_list, burnt_end_list, burnt_year_list = fire_scar_year_month_fn(dka_s, month_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dka_s[\"site_check\"] = site_list\n",
    "dka_s.loc[:, \"bnt_st_ym\"] = burnt_start_list\n",
    "dka_s.loc[:, \"bn_end_ym\"] = burnt_end_list\n",
    "dka_s.loc[:,\"bn_end_ym\"] = burnt_year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dka_s.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out readable fire scar info per site\n",
    "for s, d in zip(site_list, burnt_start_list):\n",
    "    print(f\"{s} was burnt on {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dka_s_nfire = dka_s[dka_s[\"burnt\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mask_path = os.path.join(fire_mask_dir, \"dkk_with_fire_scars_years_removed.csv\")\n",
    "dka_s_nfire.to_csv(fire_mask_path, index=False)\n",
    "print(fire_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mask = pd.DataFrame()\n",
    "fire_mask[\"site\"] = site_list\n",
    "fire_mask[\"bnt_st_ym\"] = burnt_start_list\n",
    "fire_mask[\"bn_end_ym\"] = burnt_end_list\n",
    "fire_mask[\"bn_year\"] = burnt_year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_s_merge_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_month_mask(df):\n",
    "    \n",
    "    list_ = []\n",
    "    site_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "    \n",
    "    fire_mask_dir = os.path.join(output_dir, \"fire_mask\")\n",
    "\n",
    "    mk_dir_fn(fire_mask_dir)\n",
    "#     if not os.path.isdir(fire_mask_dir):\n",
    "#         os.mkdir(fire_mask_dir)\n",
    "    \n",
    "    for i in df.site.unique():\n",
    "\n",
    "        \n",
    "        df_site = df[df[\"site\"]==i]\n",
    "    \n",
    "        for index, row in df_site.iterrows():\n",
    "            st = (row['bnt_st_ym'])\n",
    "            end =(row['bn_end_ym'])\n",
    "            year = (row['bn_year'])\n",
    "            \n",
    "            if st == 0 and end == 0:\n",
    "#                 print(i)\n",
    "#                 print(st)\n",
    "                \n",
    "                for n in range(1,13):\n",
    "                    \n",
    "                    if n < 10:\n",
    "                        z = \"0\" + str(n)\n",
    "                    else:\n",
    "                        z = str(n)\n",
    "                        \n",
    "                    ym = str(year) + z  \n",
    "                    \n",
    "                    # load values to lists\n",
    "                    site_list.append(str(i))\n",
    "                    st_list.append(int(st))\n",
    "                    end_list.append(int(end))\n",
    "                    y_list.append(int(year))\n",
    "                    ym_list.append(int(ym))\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                # st != 0 and end != 0\n",
    "                \n",
    "                # convert to string \n",
    "                st_ = str(st)\n",
    "                end_ = str(end)\n",
    "                \n",
    "                # seperate month\n",
    "                st_month = st_[4:]\n",
    "                end_month = end_[4:]\n",
    "                \n",
    "                #convert_to_int\n",
    "                st_int_month = int(st_month)\n",
    "                end_int_month = int(end_month)\n",
    "                \n",
    "                \n",
    "                # start date\n",
    "                if st_int_month > 10:\n",
    "                    st_m_str = st_month[1:]\n",
    "                else:\n",
    "                    st_m_str = st_month\n",
    "                    \n",
    "                for n in range(1,int(st_m_str)):\n",
    "#                     print(n)\n",
    "                    \n",
    "                    if n < 10:\n",
    "                        z = \"0\" + str(n)\n",
    "                    else:\n",
    "                        z = str(n)\n",
    "                        \n",
    "                    ym = str(year) + z \n",
    "                    \n",
    "                    # load values to lists\n",
    "                    site_list.append(str(i))\n",
    "                    st_list.append(int(st))\n",
    "                    end_list.append(int(end))\n",
    "                    y_list.append(int(year))\n",
    "                    ym_list.append(int(ym))\n",
    "                    \n",
    "\n",
    "                    \n",
    "    fire_ym_mask = pd.DataFrame()\n",
    "\n",
    "    fire_ym_mask[\"site\"] = site_list\n",
    "    fire_ym_mask[\"st_fs\"] = st_list\n",
    "    fire_ym_mask[\"end_fs\"] = end_list\n",
    "    fire_ym_mask[\"year\"] = y_list\n",
    "    fire_ym_mask[\"ym_bfr_fs\"] = ym_list\n",
    "\n",
    "\n",
    "    output_path = os.path.join(fire_mask_dir, \"fire_ym_before_fire_scar.csv\")\n",
    "    fire_ym_mask.to_csv(os.path.join(output_path), index=False)\n",
    "    print(\"File output to: \", output_path)\n",
    "    \n",
    "    return fire_ym_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ym_mask = fire_month_mask(fire_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_fpca2_ym(df):\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    count_list = []\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    mean_list = []\n",
    "    med_list = []\n",
    "    std_list = []\n",
    "    p25_list = []\n",
    "    p50_list = []\n",
    "    p75_list = []\n",
    "    p95_list = []\n",
    "    p99_list = []\n",
    "    range_list = []\n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_fpca2_\"*20)\n",
    "    for i in df.site.unique():\n",
    "        \n",
    "        df_site = df[df[\"site\"]==i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "            \n",
    "            st = (row['s_month'])\n",
    "            end =(row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            image = (row['image'])\n",
    "            count_ = (row['b1_fpca2_count'])\n",
    "            min_ = (row['b1_fpca2_min'])\n",
    "            max_ = (row['b1_fpca2_max'])\n",
    "            mean_ = (row['b1_fpca2_mean'])\n",
    "            med_ = (row['b1_fpca2_med'])\n",
    "            std_ = (row['b1_fpca2_std'])\n",
    "            p25_ = (row['b1_fpca2_p25'])\n",
    "            p50_ = (row['b1_fpca2_p50'])\n",
    "            p75_ = (row['b1_fpca2_p75'])\n",
    "            p95_ = (row['b1_fpca2_p95'])\n",
    "            p99_ = (row['b1_fpca2_p99'])\n",
    "            range_ = (row['b1_fpca2_range'])\n",
    "            image_s_dt = (row['image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "            \n",
    "                \n",
    "            # convert to string \n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "            \n",
    "            #convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1,int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z \n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                count_list.append(int(count_))\n",
    "                min_list.append(float(min_))\n",
    "                max_list.append(float(max_))\n",
    "                mean_list.append(float(mean_))\n",
    "                med_list.append(float(med_))\n",
    "                std_list.append(float(std_))\n",
    "                p25_list.append(float(p25_))\n",
    "                p50_list.append(float(p50_))\n",
    "                p75_list.append(float(p75_))\n",
    "                p95_list.append(float(p95_))\n",
    "                p99_list.append(float(p99_))\n",
    "                range_list.append(float(range_))\n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "                \n",
    "\n",
    "                    \n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year' : dt_year_list,\n",
    "                'dt_ym' : ym_list,\n",
    "                  'fpca2_count' : count_list,\n",
    "                    'fpca2_min': min_list,\n",
    "                   'fpca2_max' : max_list,\n",
    "                    'fpca2_mean' : mean_list,\n",
    "                    'fpca2_med': med_list,\n",
    "                    'fpca2_std': std_list,\n",
    "                    'fpca2_p25' : p25_list,\n",
    "                    'fpca2_p50' : p50_list,\n",
    "                    'fpca2_p75' : p75_list,\n",
    "                    'fpca2_p95' : p95_list,\n",
    "                    'fpca2_p99' : p99_list,\n",
    "                  'fpca2_range' : range_list,\n",
    "\n",
    "\n",
    "\n",
    "               }\n",
    "        \n",
    "    if len(mean_list)> 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "        \n",
    "    return out_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_var_ym(df, var_):\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    count_list = []\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    mean_list = []\n",
    "    med_list = []\n",
    "    std_list = []\n",
    "    p25_list = []\n",
    "    p50_list = []\n",
    "    p75_list = []\n",
    "    p95_list = []\n",
    "    p99_list = []\n",
    "    range_list = []\n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_var_\"*20)\n",
    "    print(\"var_: \", var_)\n",
    "    print(df)\n",
    "    for i in df.site.unique():\n",
    "        \n",
    "        df_site = df[df[\"site\"]==i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "\n",
    "            \n",
    "            st = (row['s_month'])\n",
    "            end =(row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            \n",
    "            if var_ == \"dis\":\n",
    "                image = (row[f'{var_}_image'])\n",
    "            else:\n",
    "                \n",
    "                image = (row['image'])\n",
    "            count_ = (row[f'b1_{var_}_count'])\n",
    "            min_ = (row[f'b1_{var_}_min'])\n",
    "            max_ = (row[f'b1_{var_}_max'])\n",
    "            mean_ = (row[f'b1_{var_}_mean'])\n",
    "            med_ = (row[f'b1_{var_}_med'])\n",
    "            std_ = (row[f'b1_{var_}_std'])\n",
    "            p25_ = (row[f'b1_{var_}_p25'])\n",
    "            p50_ = (row[f'b1_{var_}_p50'])\n",
    "            p75_ = (row[f'b1_{var_}_p75'])\n",
    "            p95_ = (row[f'b1_{var_}_p95'])\n",
    "            p99_ = (row[f'b1_{var_}_p99'])\n",
    "            range_ = (row[f'b1_{var_}_range'])\n",
    "            image_s_dt = (row[f'image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "            \n",
    "                \n",
    "            # convert to string \n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "            \n",
    "            #convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1,int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z \n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                count_list.append(int(count_))\n",
    "                min_list.append(float(min_))\n",
    "                max_list.append(float(max_))\n",
    "                mean_list.append(float(mean_))\n",
    "                med_list.append(float(med_))\n",
    "                std_list.append(float(std_))\n",
    "                p25_list.append(float(p25_))\n",
    "                p50_list.append(float(p50_))\n",
    "                p75_list.append(float(p75_))\n",
    "                p95_list.append(float(p95_))\n",
    "                p99_list.append(float(p99_))\n",
    "                range_list.append(float(range_))\n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "                \n",
    "\n",
    "                    \n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year' : dt_year_list,\n",
    "                'dt_ym' : ym_list,\n",
    "                  f'{var_}_count' : count_list,\n",
    "                    f'{var_}_min': min_list,\n",
    "                   f'{var_}_max' : max_list,\n",
    "                    f'{var_}_mean' : mean_list,\n",
    "                    f'{var_}_med': med_list,\n",
    "                    f'{var_}_std': std_list,\n",
    "                    f'{var_}_p25' : p25_list,\n",
    "                    f'{var_}_p50' : p50_list,\n",
    "                    f'{var_}_p75' : p75_list,\n",
    "                    f'{var_}_p95' : p95_list,\n",
    "                    f'{var_}_p99' : p99_list,\n",
    "                  f'{var_}_range' : range_list,\n",
    "\n",
    "\n",
    "\n",
    "               }\n",
    "        \n",
    "    if len(mean_list)> 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "        \n",
    "    return out_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_dis_ym(df, var_):\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    count_list = []\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    mean_list = []\n",
    "    med_list = []\n",
    "    std_list = []\n",
    "    major_list = []\n",
    "    minor_list = []\n",
    "#     range_list = []\n",
    "    one_list = []\n",
    "    two_list = []\n",
    "    three_list = []\n",
    "    four_list = []\n",
    "    five_list = []\n",
    "    six_list = []\n",
    "    seven_list = []\n",
    "    eight_list = []\n",
    "    nine_list = []\n",
    "    ten_list = []\n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "    \n",
    "\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_var_\"*20)\n",
    "    print(\"var_: \", var_)\n",
    "    print(df)\n",
    "    for i in df.site.unique():\n",
    "        \n",
    "        df_site = df[df[\"site\"]==i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            st = (row['s_month'])\n",
    "            end =(row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            image = (row[f'{var_}_image'])\n",
    "            count_ = (row[f'{var_}_count'])\n",
    "            min_ = (row[f'{var_}_min'])\n",
    "            max_ = (row[f'{var_}_max'])\n",
    "            mean_ = (row[f'{var_}_mean'])\n",
    "            med_ = (row[f'{var_}_med'])\n",
    "            std_ = (row[f'{var_}_std']) \n",
    "            major_ = (row[f'{var_}_major'])\n",
    "            minor_ = (row[f'{var_}_minor'])\n",
    "#             range_ = (row[f'{var_}_range'])\n",
    "            one_ = (row[f'{var_}_one'])\n",
    "            two_ = (row[f'{var_}_two'])\n",
    "            three_ = (row[f'{var_}_three'])\n",
    "            four_ = (row[f'{var_}_four'])\n",
    "            five_ = (row[f'{var_}_five'])\n",
    "            six_ = (row[f'{var_}_six'])\n",
    "            seven_ = (row[f'{var_}_seven'])\n",
    "            eight_ = (row[f'{var_}_eight'])\n",
    "            nine_ = (row[f'{var_}_nine'])\n",
    "            ten_ = (row[f'{var_}_ten'])\n",
    "            image_s_dt = (row[f'image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "            \n",
    "                \n",
    "            # convert to string \n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "            \n",
    "            #convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1,int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z \n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                count_list.append(int(count_))\n",
    "                min_list.append(float(min_))\n",
    "                max_list.append(float(max_))\n",
    "                mean_list.append(float(mean_))\n",
    "                med_list.append(float(med_))\n",
    "                std_list.append(float(std_))\n",
    "                major_list.append(float(major_))\n",
    "                minor_list.append(float(minor_))\n",
    "                one_list.append(float(one_))\n",
    "                two_list.append(float(two_))\n",
    "                three_list.append(float(three_))\n",
    "                four_list.append(float(four_))\n",
    "                five_list.append(float(five_))\n",
    "                six_list.append(float(six_))\n",
    "                seven_list.append(float(seven_))\n",
    "                eight_list.append(float(eight_))\n",
    "                nine_list.append(float(nine_))\n",
    "                ten_list.append(float(ten_))\n",
    "#                 range_list.append(float(range_))\n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "                \n",
    "\n",
    "                    \n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year' : dt_year_list,\n",
    "                'dt_ym' : ym_list,\n",
    "                  f'{var_}_count' : count_list,\n",
    "                    f'{var_}_min': min_list,\n",
    "                   f'{var_}_max' : max_list,\n",
    "                    f'{var_}_mean' : mean_list,\n",
    "                    f'{var_}_med': med_list,\n",
    "                    f'{var_}_std': std_list,\n",
    "                    f'{var_}_major' : major_list,\n",
    "                    f'{var_}_minor' : minor_list,\n",
    "#                     f'{var_}_range' : range_list,\n",
    "                    f'{var_}_one' : one_list,\n",
    "                    f'{var_}_two' : two_list,\n",
    "                    f'{var_}_three' : three_list,\n",
    "                    f'{var_}_four': four_list,\n",
    "                    f'{var_}_five': five_list,\n",
    "                    f'{var_}_six' : six_list,\n",
    "                    f'{var_}_seven' : seven_list,\n",
    "                    f'{var_}_eight' : eight_list,\n",
    "                    f'{var_}_nine' : nine_list,\n",
    "                    f'{var_}_ten' : ten_list,\n",
    "\n",
    "               }\n",
    "        \n",
    "    if len(mean_list)> 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "        \n",
    "    print('-'*50)    \n",
    "    print(out_df.columns)\n",
    "    print('-'*50)  \n",
    "        \n",
    "    return out_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_dbi_ym(df):\n",
    "    \n",
    "    print(df.columns)\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    b1_count_list = []\n",
    "    b1_min_list = []\n",
    "    b1_max_list = []\n",
    "    b1_mean_list = []\n",
    "    b1_med_list = []\n",
    "    b1_std_list = []\n",
    "    b1_p25_list = []\n",
    "    b1_p50_list = []\n",
    "    b1_p75_list = []\n",
    "    b1_p95_list = []\n",
    "    b1_p99_list = []\n",
    "    b1_range_list = []\n",
    "    \n",
    " \n",
    "    b2_count_list = []\n",
    "    b2_min_list = []\n",
    "    b2_max_list = []\n",
    "    b2_mean_list = []\n",
    "    b2_med_list = []\n",
    "    b2_std_list = []\n",
    "    b2_p25_list = []\n",
    "    b2_p50_list = []\n",
    "    b2_p75_list = []\n",
    "    b2_p95_list = []\n",
    "    b2_p99_list = []\n",
    "    b2_range_list = []\n",
    "    \n",
    "    b3_count_list = []\n",
    "    b3_min_list = []\n",
    "    b3_max_list = []\n",
    "    b3_mean_list = []\n",
    "    b3_med_list = []\n",
    "    b3_std_list = []\n",
    "    b3_p25_list = []\n",
    "    b3_p50_list = []\n",
    "    b3_p75_list = []\n",
    "    b3_p95_list = []\n",
    "    b3_p99_list = []\n",
    "    b3_range_list = []\n",
    "    \n",
    "    b4_count_list = []\n",
    "    b4_min_list = []\n",
    "    b4_max_list = []\n",
    "    b4_mean_list = []\n",
    "    b4_med_list = []\n",
    "    b4_std_list = []\n",
    "    b4_p25_list = []\n",
    "    b4_p50_list = []\n",
    "    b4_p75_list = []\n",
    "    b4_p95_list = []\n",
    "    b4_p99_list = []\n",
    "    b4_range_list = []\n",
    "    \n",
    "    b5_count_list = []\n",
    "    b5_min_list = []\n",
    "    b5_max_list = []\n",
    "    b5_mean_list = []\n",
    "    b5_med_list = []\n",
    "    b5_std_list = []\n",
    "    b5_p25_list = []\n",
    "    b5_p50_list = []\n",
    "    b5_p75_list = []\n",
    "    b5_p95_list = []\n",
    "    b5_p99_list = []\n",
    "    b5_range_list = []\n",
    "    \n",
    "    b6_count_list = []\n",
    "    b6_min_list = []\n",
    "    b6_max_list = []\n",
    "    b6_mean_list = []\n",
    "    b6_med_list = []\n",
    "    b6_std_list = []\n",
    "    b6_p25_list = []\n",
    "    b6_p50_list = []\n",
    "    b6_p75_list = []\n",
    "    b6_p95_list = []\n",
    "    b6_p99_list = []\n",
    "    b6_range_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "    \n",
    "#     ['uid', 'site', 'image', 's_day', 's_month', 's_year', 's_date', 'e_day', 'e_month', 'e_year', \n",
    "#      'e_date', 'b1_dbi_count', 'b1_dbi_min', 'b1_dbi_max', 'b1_dbi_mean', 'b1_dbi_med', 'b1_dbi_std',\n",
    "#      'b1_dbi_p25', 'b1_dbi_p50', 'b1_dbi_p75', 'b1_dbi_p95', 'b1_dbi_p99', 'b1_dbi_range', 'b2_dbi_count',\n",
    "#      'b2_dbi_min', 'b2_dbi_max', 'b2_dbi_mean', 'b2_dbi_med', 'b2_dbi_std', 'b2_dbi_p25', 'b2_dbi_p50', 'b2_dbi_p75', \n",
    "#      'b2_dbi_p95', 'b2_dbi_p99', 'b2_dbi_range', 'b3_dbi_count', 'b3_dbi_min', 'b3_dbi_max', 'b3_dbi_mean', 'b3_dbi_med', \n",
    "#      'b3_dbi_std', 'b3_dbi_p25', 'b3_dbi_p50', 'b3_dbi_p75', 'b3_dbi_p95', 'b3_dbi_p99', 'b3_dbi_range', 'b4_dbi_count', \n",
    "#      'b4_dbi_min', 'b4_dbi_max', 'b4_dbi_mean', 'b4_dbi_med', 'b4_dbi_std', 'b4_dbi_p25', 'b4_dbi_p50', 'b4_dbi_p75', \n",
    "#      'b4_dbi_p95', 'b4_dbi_p99', 'b4_dbi_range', 'b5_dbi_count', 'b5_dbi_min', 'b5_dbi_max', 'b5_dbi_mean', 'b5_dbi_med', \n",
    "#      'b5_dbi_std', 'b5_dbi_p25', 'b5_dbi_p50', 'b5_dbi_p75', 'b5_dbi_p95', 'b5_dbi_p99', 'b5_dbi_range', 'b6_dbi_count', \n",
    "#      'b6_dbi_min', 'b6_dbi_max', 'b6_dbi_mean', 'b6_dbi_med', 'b6_dbi_std', 'b6_dbi_p25', 'b6_dbi_p50', 'b6_dbi_p75', \n",
    "#      'b6_dbi_p95', 'b6_dbi_p99', \n",
    "#      'b6_dbi_range', 'image_s_dt', 'image_e_dt', 'dt_year', 'dt_ym']\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_fpca2_\"*20)\n",
    "    for i in df.site.unique():\n",
    "        \n",
    "        df_site = df[df[\"site\"]==i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "            \n",
    "            st = (row['s_month'])\n",
    "            end =(row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            image = (row['image'])\n",
    "            count_1 = (row['b1_dbi_count'])\n",
    "            min_1 = (row['b1_dbi_min'])\n",
    "            max_1 = (row['b1_dbi_max'])\n",
    "            mean_1 = (row['b1_dbi_mean'])\n",
    "            med_1 = (row['b1_dbi_med'])\n",
    "            std_1 = (row['b1_dbi_std'])\n",
    "            p25_1 = (row['b1_dbi_p25'])\n",
    "            p50_1 = (row['b1_dbi_p50'])\n",
    "            p75_1 = (row['b1_dbi_p75'])\n",
    "            p95_1 = (row['b1_dbi_p95'])\n",
    "            p99_1 = (row['b1_dbi_p99'])\n",
    "            range_1 = (row['b1_dbi_range'])\n",
    "            \n",
    "            count_2 = (row['b2_dbi_count'])\n",
    "            min_2 = (row['b2_dbi_min'])\n",
    "            max_2 = (row['b2_dbi_max'])\n",
    "            mean_2 = (row['b2_dbi_mean'])\n",
    "            med_2 = (row['b2_dbi_med'])\n",
    "            std_2 = (row['b2_dbi_std'])\n",
    "            p25_2 = (row['b2_dbi_p25'])\n",
    "            p50_2 = (row['b2_dbi_p50'])\n",
    "            p75_2 = (row['b2_dbi_p75'])\n",
    "            p95_2 = (row['b2_dbi_p95'])\n",
    "            p99_2 = (row['b2_dbi_p99'])\n",
    "            range_2 = (row['b2_dbi_range'])\n",
    "            \n",
    "            \n",
    "            count_3 = (row['b3_dbi_count'])\n",
    "            min_3 = (row['b3_dbi_min'])\n",
    "            max_3 = (row['b3_dbi_max'])\n",
    "            mean_3 = (row['b3_dbi_mean'])\n",
    "            med_3 = (row['b3_dbi_med'])\n",
    "            std_3 = (row['b3_dbi_std'])\n",
    "            p25_3 = (row['b3_dbi_p25'])\n",
    "            p50_3 = (row['b3_dbi_p50'])\n",
    "            p75_3 = (row['b3_dbi_p75'])\n",
    "            p95_3 = (row['b3_dbi_p95'])\n",
    "            p99_3 = (row['b3_dbi_p99'])\n",
    "            range_3 = (row['b3_dbi_range'])\n",
    "            \n",
    "            count_4 = (row['b4_dbi_count'])\n",
    "            min_4 = (row['b4_dbi_min'])\n",
    "            max_4 = (row['b4_dbi_max'])\n",
    "            mean_4 = (row['b4_dbi_mean'])\n",
    "            med_4 = (row['b4_dbi_med'])\n",
    "            std_4 = (row['b4_dbi_std'])\n",
    "            p25_4 = (row['b4_dbi_p25'])\n",
    "            p50_4 = (row['b4_dbi_p50'])\n",
    "            p75_4 = (row['b4_dbi_p75'])\n",
    "            p95_4 = (row['b4_dbi_p95'])\n",
    "            p99_4 = (row['b4_dbi_p99'])\n",
    "            range_4 = (row['b4_dbi_range'])\n",
    "            \n",
    "            count_5 = (row['b5_dbi_count'])\n",
    "            min_5 = (row['b5_dbi_min'])\n",
    "            max_5 = (row['b5_dbi_max'])\n",
    "            mean_5 = (row['b5_dbi_mean'])\n",
    "            med_5 = (row['b5_dbi_med'])\n",
    "            std_5 = (row['b5_dbi_std'])\n",
    "            p25_5 = (row['b5_dbi_p25'])\n",
    "            p50_5 = (row['b5_dbi_p50'])\n",
    "            p75_5 = (row['b5_dbi_p75'])\n",
    "            p95_5 = (row['b5_dbi_p95'])\n",
    "            p99_5 = (row['b5_dbi_p99'])\n",
    "            range_5 = (row['b5_dbi_range'])\n",
    "            \n",
    "            \n",
    "            count_6 = (row['b6_dbi_count'])\n",
    "            min_6 = (row['b6_dbi_min'])\n",
    "            max_6 = (row['b6_dbi_max'])\n",
    "            mean_6 = (row['b6_dbi_mean'])\n",
    "            med_6 = (row['b6_dbi_med'])\n",
    "            std_6 = (row['b6_dbi_std'])\n",
    "            p25_6 = (row['b6_dbi_p25'])\n",
    "            p50_6 = (row['b6_dbi_p50'])\n",
    "            p75_6 = (row['b6_dbi_p75'])\n",
    "            p95_6 = (row['b6_dbi_p95'])\n",
    "            p99_6 = (row['b6_dbi_p99'])\n",
    "            range_6 = (row['b6_dbi_range'])\n",
    "            \n",
    "            image_s_dt = (row['image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "            \n",
    "                \n",
    "            # convert to string \n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "            \n",
    "            #convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1,int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z \n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                b1_count_list.append(int(count_1))\n",
    "                b1_min_list.append(float(min_1))\n",
    "                b1_max_list.append(float(max_1))\n",
    "                b1_mean_list.append(float(mean_1))\n",
    "                b1_med_list.append(float(med_1))\n",
    "                b1_std_list.append(float(std_1))\n",
    "                b1_p25_list.append(float(p25_1))\n",
    "                b1_p50_list.append(float(p50_1))\n",
    "                b1_p75_list.append(float(p75_1))\n",
    "                b1_p95_list.append(float(p95_1))\n",
    "                b1_p99_list.append(float(p99_1))\n",
    "                b1_range_list.append(float(range_1))\n",
    "                \n",
    "\n",
    "                b2_count_list.append(int(count_2))\n",
    "                b2_min_list.append(float(min_2))\n",
    "                b2_max_list.append(float(max_2))\n",
    "                b2_mean_list.append(float(mean_2))\n",
    "                b2_med_list.append(float(med_2))\n",
    "                b2_std_list.append(float(std_2))\n",
    "                b2_p25_list.append(float(p25_2))\n",
    "                b2_p50_list.append(float(p50_2))\n",
    "                b2_p75_list.append(float(p75_2))\n",
    "                b2_p95_list.append(float(p95_2))\n",
    "                b2_p99_list.append(float(p99_2))\n",
    "                b2_range_list.append(float(range_2))\n",
    "                \n",
    "                b3_count_list.append(int(count_3))\n",
    "                b3_min_list.append(float(min_3))\n",
    "                b3_max_list.append(float(max_3))\n",
    "                b3_mean_list.append(float(mean_3))\n",
    "                b3_med_list.append(float(med_3))\n",
    "                b3_std_list.append(float(std_3))\n",
    "                b3_p25_list.append(float(p25_3))\n",
    "                b3_p50_list.append(float(p50_3))\n",
    "                b3_p75_list.append(float(p75_3))\n",
    "                b3_p95_list.append(float(p95_3))\n",
    "                b3_p99_list.append(float(p99_3))\n",
    "                b3_range_list.append(float(range_3))\n",
    "                \n",
    "                b4_count_list.append(int(count_4))\n",
    "                b4_min_list.append(float(min_4))\n",
    "                b4_max_list.append(float(max_4))\n",
    "                b4_mean_list.append(float(mean_4))\n",
    "                b4_med_list.append(float(med_4))\n",
    "                b4_std_list.append(float(std_4))\n",
    "                b4_p25_list.append(float(p25_4))\n",
    "                b4_p50_list.append(float(p50_4))\n",
    "                b4_p75_list.append(float(p75_4))\n",
    "                b4_p95_list.append(float(p95_4))\n",
    "                b4_p99_list.append(float(p99_4))\n",
    "                b4_range_list.append(float(range_4))\n",
    "                \n",
    "                b5_count_list.append(int(count_5))\n",
    "                b5_min_list.append(float(min_5))\n",
    "                b5_max_list.append(float(max_5))\n",
    "                b5_mean_list.append(float(mean_5))\n",
    "                b5_med_list.append(float(med_5))\n",
    "                b5_std_list.append(float(std_5))\n",
    "                b5_p25_list.append(float(p25_5))\n",
    "                b5_p50_list.append(float(p50_5))\n",
    "                b5_p75_list.append(float(p75_5))\n",
    "                b5_p95_list.append(float(p95_5))\n",
    "                b5_p99_list.append(float(p99_5))\n",
    "                b5_range_list.append(float(range_5))\n",
    "                \n",
    "                b6_count_list.append(int(count_6))\n",
    "                b6_min_list.append(float(min_6))\n",
    "                b6_max_list.append(float(max_6))\n",
    "                b6_mean_list.append(float(mean_6))\n",
    "                b6_med_list.append(float(med_6))\n",
    "                b6_std_list.append(float(std_6))\n",
    "                b6_p25_list.append(float(p25_6))\n",
    "                b6_p50_list.append(float(p50_6))\n",
    "                b6_p75_list.append(float(p75_6))\n",
    "                b6_p95_list.append(float(p95_6))\n",
    "                b6_p99_list.append(float(p99_6))\n",
    "                b6_range_list.append(float(range_6))\n",
    "                \n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "                \n",
    "\n",
    "                    \n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year' : dt_year_list,\n",
    "                'dt_ym' : ym_list,\n",
    "                  'b1_dbi_count' : b1_count_list,\n",
    "                    'b1_dbi_min': b1_min_list,\n",
    "                   'b1_dbi_max' : b1_max_list,\n",
    "                    'b1_dbi_mean' : b1_mean_list,\n",
    "                    'b1_dbi_med': b1_med_list,\n",
    "                    'b1_dbi_std': b1_std_list,\n",
    "                    'b1_dbi_p25' : b1_p25_list,\n",
    "                    'b1_dbi_p50' : b1_p50_list,\n",
    "                    'b1_dbi_p75' : b1_p75_list,\n",
    "                    'b1_dbi_p95' : b1_p95_list,\n",
    "                    'b1_dbi_p99' : b1_p99_list,\n",
    "                  'b1_dbi_range' : b1_range_list,\n",
    "\n",
    "                  'b2_dbi_count' : b2_count_list,\n",
    "                    'b2_dbi_min': b2_min_list,\n",
    "                   'b2_dbi_max' : b2_max_list,\n",
    "                    'b2_dbi_mean' : b2_mean_list,\n",
    "                    'b2_dbi_med': b2_med_list,\n",
    "                    'b2_dbi_std': b2_std_list,\n",
    "                    'b2_dbi_p25' : b2_p25_list,\n",
    "                    'b2_dbi_p50' : b2_p50_list,\n",
    "                    'b2_dbi_p75' : b2_p75_list,\n",
    "                    'b2_dbi_p95' : b2_p95_list,\n",
    "                    'b2_dbi_p99' : b2_p99_list,\n",
    "                  'b2_dbi_range' : b2_range_list,\n",
    "                \n",
    "                    'b3_dbi_count' : b3_count_list,\n",
    "                    'b3_dbi_min': b3_min_list,\n",
    "                   'b3_dbi_max' : b3_max_list,\n",
    "                    'b3_dbi_mean' : b3_mean_list,\n",
    "                    'b3_dbi_med': b3_med_list,\n",
    "                    'b3_dbi_std': b3_std_list,\n",
    "                    'b3_dbi_p25' : b3_p25_list,\n",
    "                    'b3_dbi_p50' : b3_p50_list,\n",
    "                    'b3_dbi_p75' : b3_p75_list,\n",
    "                    'b3_dbi_p95' : b3_p95_list,\n",
    "                    'b3_dbi_p99' : b3_p99_list,\n",
    "                  'b3_dbi_range' : b3_range_list,\n",
    "                \n",
    "                    'b4_dbi_count' : b4_count_list,\n",
    "                    'b4_dbi_min': b4_min_list,\n",
    "                   'b4_dbi_max' : b4_max_list,\n",
    "                    'b4_dbi_mean' : b4_mean_list,\n",
    "                    'b4_dbi_med': b4_med_list,\n",
    "                    'b4_dbi_std': b4_std_list,\n",
    "                    'b4_dbi_p25' : b4_p25_list,\n",
    "                    'b4_dbi_p50' : b4_p50_list,\n",
    "                    'b4_dbi_p75' : b4_p75_list,\n",
    "                    'b4_dbi_p95' : b4_p95_list,\n",
    "                    'b4_dbi_p99' : b4_p99_list,\n",
    "                  'b4_dbi_range' : b4_range_list,\n",
    "                \n",
    "                    'b5_dbi_count' : b5_count_list,\n",
    "                    'b5_dbi_min': b5_min_list,\n",
    "                   'b5_dbi_max' : b5_max_list,\n",
    "                    'b5_dbi_mean' : b5_mean_list,\n",
    "                    'b5_dbi_med': b5_med_list,\n",
    "                    'b5_dbi_std': b5_std_list,\n",
    "                    'b5_dbi_p25' : b5_p25_list,\n",
    "                    'b5_dbi_p50' : b5_p50_list,\n",
    "                    'b5_dbi_p75' : b5_p75_list,\n",
    "                    'b5_dbi_p95' : b5_p95_list,\n",
    "                    'b5_dbi_p99' : b5_p99_list,\n",
    "                  'b5_dbi_range' : b5_range_list,\n",
    "                \n",
    "                    'b6_dbi_count' : b6_count_list,\n",
    "                    'b6_dbi_min': b6_min_list,\n",
    "                   'b6_dbi_max' : b6_max_list,\n",
    "                    'b6_dbi_mean' : b6_mean_list,\n",
    "                    'b6_dbi_med': b6_med_list,\n",
    "                    'b6_dbi_std': b6_std_list,\n",
    "                    'b6_dbi_p25' : b6_p25_list,\n",
    "                    'b6_dbi_p50' : b6_p50_list,\n",
    "                    'b6_dbi_p75' : b6_p75_list,\n",
    "                    'b6_dbi_p95' : b6_p95_list,\n",
    "                    'b6_dbi_p99' : b6_p99_list,\n",
    "                  'b6_dbi_range' : b6_range_list,\n",
    "\n",
    "               }\n",
    "        \n",
    "    if len(b1_mean_list)> 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "        \n",
    "    return out_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_dim_ym(df):\n",
    "    print(df.columns)\n",
    "\n",
    "    list_ = []\n",
    "\n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    b1_count_list = []\n",
    "    b1_min_list = []\n",
    "    b1_max_list = []\n",
    "    b1_mean_list = []\n",
    "    b1_med_list = []\n",
    "    b1_std_list = []\n",
    "    b1_p25_list = []\n",
    "    b1_p50_list = []\n",
    "    b1_p75_list = []\n",
    "    b1_p95_list = []\n",
    "    b1_p99_list = []\n",
    "    b1_range_list = []\n",
    "\n",
    "    b2_count_list = []\n",
    "    b2_min_list = []\n",
    "    b2_max_list = []\n",
    "    b2_mean_list = []\n",
    "    b2_med_list = []\n",
    "    b2_std_list = []\n",
    "    b2_p25_list = []\n",
    "    b2_p50_list = []\n",
    "    b2_p75_list = []\n",
    "    b2_p95_list = []\n",
    "    b2_p99_list = []\n",
    "    b2_range_list = []\n",
    "\n",
    "    b3_count_list = []\n",
    "    b3_min_list = []\n",
    "    b3_max_list = []\n",
    "    b3_mean_list = []\n",
    "    b3_med_list = []\n",
    "    b3_std_list = []\n",
    "    b3_p25_list = []\n",
    "    b3_p50_list = []\n",
    "    b3_p75_list = []\n",
    "    b3_p95_list = []\n",
    "    b3_p99_list = []\n",
    "    b3_range_list = []\n",
    "\n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_fpca2_\" * 20)\n",
    "    for i in df.site.unique():\n",
    "\n",
    "        df_site = df[df[\"site\"] == i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "\n",
    "            st = (row['s_month'])\n",
    "            end = (row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            image = (row['image'])\n",
    "            count_1 = (row['b1_dim_count'])\n",
    "            min_1 = (row['b1_dim_min'])\n",
    "            max_1 = (row['b1_dim_max'])\n",
    "            mean_1 = (row['b1_dim_mean'])\n",
    "            med_1 = (row['b1_dim_med'])\n",
    "            std_1 = (row['b1_dim_std'])\n",
    "            p25_1 = (row['b1_dim_p25'])\n",
    "            p50_1 = (row['b1_dim_p50'])\n",
    "            p75_1 = (row['b1_dim_p75'])\n",
    "            p95_1 = (row['b1_dim_p95'])\n",
    "            p99_1 = (row['b1_dim_p99'])\n",
    "            range_1 = (row['b1_dim_range'])\n",
    "\n",
    "            count_2 = (row['b2_dim_count'])\n",
    "            min_2 = (row['b2_dim_min'])\n",
    "            max_2 = (row['b2_dim_max'])\n",
    "            mean_2 = (row['b2_dim_mean'])\n",
    "            med_2 = (row['b2_dim_med'])\n",
    "            std_2 = (row['b2_dim_std'])\n",
    "            p25_2 = (row['b2_dim_p25'])\n",
    "            p50_2 = (row['b2_dim_p50'])\n",
    "            p75_2 = (row['b2_dim_p75'])\n",
    "            p95_2 = (row['b2_dim_p95'])\n",
    "            p99_2 = (row['b2_dim_p99'])\n",
    "            range_2 = (row['b2_dim_range'])\n",
    "\n",
    "            count_3 = (row['b3_dim_count'])\n",
    "            min_3 = (row['b3_dim_min'])\n",
    "            max_3 = (row['b3_dim_max'])\n",
    "            mean_3 = (row['b3_dim_mean'])\n",
    "            med_3 = (row['b3_dim_med'])\n",
    "            std_3 = (row['b3_dim_std'])\n",
    "            p25_3 = (row['b3_dim_p25'])\n",
    "            p50_3 = (row['b3_dim_p50'])\n",
    "            p75_3 = (row['b3_dim_p75'])\n",
    "            p95_3 = (row['b3_dim_p95'])\n",
    "            p99_3 = (row['b3_dim_p99'])\n",
    "            range_3 = (row['b3_dim_range'])\n",
    "\n",
    "\n",
    "            image_s_dt = (row['image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "\n",
    "            # convert to string\n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "\n",
    "            # convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1, int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z\n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                b1_count_list.append(int(count_1))\n",
    "                b1_min_list.append(float(min_1))\n",
    "                b1_max_list.append(float(max_1))\n",
    "                b1_mean_list.append(float(mean_1))\n",
    "                b1_med_list.append(float(med_1))\n",
    "                b1_std_list.append(float(std_1))\n",
    "                b1_p25_list.append(float(p25_1))\n",
    "                b1_p50_list.append(float(p50_1))\n",
    "                b1_p75_list.append(float(p75_1))\n",
    "                b1_p95_list.append(float(p95_1))\n",
    "                b1_p99_list.append(float(p99_1))\n",
    "                b1_range_list.append(float(range_1))\n",
    "\n",
    "                b2_count_list.append(int(count_2))\n",
    "                b2_min_list.append(float(min_2))\n",
    "                b2_max_list.append(float(max_2))\n",
    "                b2_mean_list.append(float(mean_2))\n",
    "                b2_med_list.append(float(med_2))\n",
    "                b2_std_list.append(float(std_2))\n",
    "                b2_p25_list.append(float(p25_2))\n",
    "                b2_p50_list.append(float(p50_2))\n",
    "                b2_p75_list.append(float(p75_2))\n",
    "                b2_p95_list.append(float(p95_2))\n",
    "                b2_p99_list.append(float(p99_2))\n",
    "                b2_range_list.append(float(range_2))\n",
    "\n",
    "                b3_count_list.append(int(count_3))\n",
    "                b3_min_list.append(float(min_3))\n",
    "                b3_max_list.append(float(max_3))\n",
    "                b3_mean_list.append(float(mean_3))\n",
    "                b3_med_list.append(float(med_3))\n",
    "                b3_std_list.append(float(std_3))\n",
    "                b3_p25_list.append(float(p25_3))\n",
    "                b3_p50_list.append(float(p50_3))\n",
    "                b3_p75_list.append(float(p75_3))\n",
    "                b3_p95_list.append(float(p95_3))\n",
    "                b3_p99_list.append(float(p99_3))\n",
    "                b3_range_list.append(float(range_3))\n",
    "\n",
    "\n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "\n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year': dt_year_list,\n",
    "                'dt_ym': ym_list,\n",
    "                'b1_dim_count': b1_count_list,\n",
    "                'b1_dim_min': b1_min_list,\n",
    "                'b1_dim_max': b1_max_list,\n",
    "                'b1_dim_mean': b1_mean_list,\n",
    "                'b1_dim_med': b1_med_list,\n",
    "                'b1_dim_std': b1_std_list,\n",
    "                'b1_dim_p25': b1_p25_list,\n",
    "                'b1_dim_p50': b1_p50_list,\n",
    "                'b1_dim_p75': b1_p75_list,\n",
    "                'b1_dim_p95': b1_p95_list,\n",
    "                'b1_dim_p99': b1_p99_list,\n",
    "                'b1_dim_range': b1_range_list,\n",
    "\n",
    "                'b2_dim_count': b2_count_list,\n",
    "                'b2_dim_min': b2_min_list,\n",
    "                'b2_dim_max': b2_max_list,\n",
    "                'b2_dim_mean': b2_mean_list,\n",
    "                'b2_dim_med': b2_med_list,\n",
    "                'b2_dim_std': b2_std_list,\n",
    "                'b2_dim_p25': b2_p25_list,\n",
    "                'b2_dim_p50': b2_p50_list,\n",
    "                'b2_dim_p75': b2_p75_list,\n",
    "                'b2_dim_p95': b2_p95_list,\n",
    "                'b2_dim_p99': b2_p99_list,\n",
    "                'b2_dim_range': b2_range_list,\n",
    "\n",
    "                'b3_dim_count': b3_count_list,\n",
    "                'b3_dim_min': b3_min_list,\n",
    "                'b3_dim_max': b3_max_list,\n",
    "                'b3_dim_mean': b3_mean_list,\n",
    "                'b3_dim_med': b3_med_list,\n",
    "                'b3_dim_std': b3_std_list,\n",
    "                'b3_dim_p25': b3_p25_list,\n",
    "                'b3_dim_p50': b3_p50_list,\n",
    "                'b3_dim_p75': b3_p75_list,\n",
    "                'b3_dim_p95': b3_p95_list,\n",
    "                'b3_dim_p99': b3_p99_list,\n",
    "                'b3_dim_range': b3_range_list,\n",
    "\n",
    "\n",
    "                }\n",
    "\n",
    "    if len(b1_mean_list) > 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_dp1_ym(df):\n",
    "    print(df.columns)\n",
    "\n",
    "    list_ = []\n",
    "\n",
    "    uid_list = []\n",
    "    site_list = []\n",
    "    image_list = []\n",
    "    st_list = []\n",
    "    end_list = []\n",
    "    y_list = []\n",
    "    ym_list = []\n",
    "\n",
    "    b1_count_list = []\n",
    "    b1_min_list = []\n",
    "    b1_max_list = []\n",
    "    b1_mean_list = []\n",
    "    b1_med_list = []\n",
    "    b1_std_list = []\n",
    "    b1_p25_list = []\n",
    "    b1_p50_list = []\n",
    "    b1_p75_list = []\n",
    "    b1_p95_list = []\n",
    "    b1_p99_list = []\n",
    "    b1_range_list = []\n",
    "\n",
    "    b2_count_list = []\n",
    "    b2_min_list = []\n",
    "    b2_max_list = []\n",
    "    b2_mean_list = []\n",
    "    b2_med_list = []\n",
    "    b2_std_list = []\n",
    "    b2_p25_list = []\n",
    "    b2_p50_list = []\n",
    "    b2_p75_list = []\n",
    "    b2_p95_list = []\n",
    "    b2_p99_list = []\n",
    "    b2_range_list = []\n",
    "\n",
    "    b3_count_list = []\n",
    "    b3_min_list = []\n",
    "    b3_max_list = []\n",
    "    b3_mean_list = []\n",
    "    b3_med_list = []\n",
    "    b3_std_list = []\n",
    "    b3_p25_list = []\n",
    "    b3_p50_list = []\n",
    "    b3_p75_list = []\n",
    "    b3_p95_list = []\n",
    "    b3_p99_list = []\n",
    "    b3_range_list = []\n",
    "\n",
    "    image_s_dt_list = []\n",
    "    dt_year_list = []\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"init seasonal_fpca2_\" * 20)\n",
    "    for i in df.site.unique():\n",
    "\n",
    "        df_site = df[df[\"site\"] == i]\n",
    "        print('working on site: ', i)\n",
    "        for index, row in df_site.iterrows():\n",
    "\n",
    "            st = (row['s_month'])\n",
    "            end = (row['e_month'])\n",
    "            year = (row['s_year'])\n",
    "            uid = (row['uid'])\n",
    "            image = (row['image'])\n",
    "            count_1 = (row['b1_dp1_count'])\n",
    "            min_1 = (row['b1_dp1_min'])\n",
    "            max_1 = (row['b1_dp1_max'])\n",
    "            mean_1 = (row['b1_dp1_mean'])\n",
    "            med_1 = (row['b1_dp1_med'])\n",
    "            std_1 = (row['b1_dp1_std'])\n",
    "            p25_1 = (row['b1_dp1_p25'])\n",
    "            p50_1 = (row['b1_dp1_p50'])\n",
    "            p75_1 = (row['b1_dp1_p75'])\n",
    "            p95_1 = (row['b1_dp1_p95'])\n",
    "            p99_1 = (row['b1_dp1_p99'])\n",
    "            range_1 = (row['b1_dp1_range'])\n",
    "\n",
    "            count_2 = (row['b2_dp1_count'])\n",
    "            min_2 = (row['b2_dp1_min'])\n",
    "            max_2 = (row['b2_dp1_max'])\n",
    "            mean_2 = (row['b2_dp1_mean'])\n",
    "            med_2 = (row['b2_dp1_med'])\n",
    "            std_2 = (row['b2_dp1_std'])\n",
    "            p25_2 = (row['b2_dp1_p25'])\n",
    "            p50_2 = (row['b2_dp1_p50'])\n",
    "            p75_2 = (row['b2_dp1_p75'])\n",
    "            p95_2 = (row['b2_dp1_p95'])\n",
    "            p99_2 = (row['b2_dp1_p99'])\n",
    "            range_2 = (row['b2_dp1_range'])\n",
    "\n",
    "            count_3 = (row['b3_dp1_count'])\n",
    "            min_3 = (row['b3_dp1_min'])\n",
    "            max_3 = (row['b3_dp1_max'])\n",
    "            mean_3 = (row['b3_dp1_mean'])\n",
    "            med_3 = (row['b3_dp1_med'])\n",
    "            std_3 = (row['b3_dp1_std'])\n",
    "            p25_3 = (row['b3_dp1_p25'])\n",
    "            p50_3 = (row['b3_dp1_p50'])\n",
    "            p75_3 = (row['b3_dp1_p75'])\n",
    "            p95_3 = (row['b3_dp1_p95'])\n",
    "            p99_3 = (row['b3_dp1_p99'])\n",
    "            range_3 = (row['b3_dp1_range'])\n",
    "\n",
    "\n",
    "            image_s_dt = (row['image_s_dt'])\n",
    "            dt_year = (row['dt_year'])\n",
    "\n",
    "            # convert to string\n",
    "            st_month = str(st)\n",
    "            end_month = str(end)\n",
    "\n",
    "            print(\"st_month: \", st_month)\n",
    "\n",
    "            # convert_to_int\n",
    "            st_int_month = int(st_month)\n",
    "            end_int_month = int(end_month)\n",
    "\n",
    "            print(\"st_int_month: \", str(st_int_month))\n",
    "            for n in range(1, int(st_int_month)):\n",
    "                print(n)\n",
    "\n",
    "                if n < 10:\n",
    "                    z = \"0\" + str(n)\n",
    "                else:\n",
    "                    z = str(n)\n",
    "\n",
    "                ym = str(year) + z\n",
    "\n",
    "                # load values to lists\n",
    "                site_list.append(str(i))\n",
    "                st_list.append(int(st))\n",
    "                end_list.append(int(end))\n",
    "                y_list.append(int(year))\n",
    "                ym_list.append(int(ym))\n",
    "                uid_list.append(int(uid))\n",
    "                image_list = str(image)\n",
    "                b1_count_list.append(int(count_1))\n",
    "                b1_min_list.append(float(min_1))\n",
    "                b1_max_list.append(float(max_1))\n",
    "                b1_mean_list.append(float(mean_1))\n",
    "                b1_med_list.append(float(med_1))\n",
    "                b1_std_list.append(float(std_1))\n",
    "                b1_p25_list.append(float(p25_1))\n",
    "                b1_p50_list.append(float(p50_1))\n",
    "                b1_p75_list.append(float(p75_1))\n",
    "                b1_p95_list.append(float(p95_1))\n",
    "                b1_p99_list.append(float(p99_1))\n",
    "                b1_range_list.append(float(range_1))\n",
    "\n",
    "                b2_count_list.append(int(count_2))\n",
    "                b2_min_list.append(float(min_2))\n",
    "                b2_max_list.append(float(max_2))\n",
    "                b2_mean_list.append(float(mean_2))\n",
    "                b2_med_list.append(float(med_2))\n",
    "                b2_std_list.append(float(std_2))\n",
    "                b2_p25_list.append(float(p25_2))\n",
    "                b2_p50_list.append(float(p50_2))\n",
    "                b2_p75_list.append(float(p75_2))\n",
    "                b2_p95_list.append(float(p95_2))\n",
    "                b2_p99_list.append(float(p99_2))\n",
    "                b2_range_list.append(float(range_2))\n",
    "\n",
    "                b3_count_list.append(int(count_3))\n",
    "                b3_min_list.append(float(min_3))\n",
    "                b3_max_list.append(float(max_3))\n",
    "                b3_mean_list.append(float(mean_3))\n",
    "                b3_med_list.append(float(med_3))\n",
    "                b3_std_list.append(float(std_3))\n",
    "                b3_p25_list.append(float(p25_3))\n",
    "                b3_p50_list.append(float(p50_3))\n",
    "                b3_p75_list.append(float(p75_3))\n",
    "                b3_p95_list.append(float(p95_3))\n",
    "                b3_p99_list.append(float(p99_3))\n",
    "                b3_range_list.append(float(range_3))\n",
    "\n",
    "\n",
    "                image_s_dt_list.append(str(image_s_dt))\n",
    "                dt_year_list.append(int(dt_year))\n",
    "\n",
    "        data = {'uid': uid_list,\n",
    "                'site': site_list,\n",
    "                'image': image_list,\n",
    "                'image_s_dt': image_s_dt_list,\n",
    "                'dt_year': dt_year_list,\n",
    "                'dt_ym': ym_list,\n",
    "                'b1_dp1_count': b1_count_list,\n",
    "                'b1_dp1_min': b1_min_list,\n",
    "                'b1_dp1_max': b1_max_list,\n",
    "                'b1_dp1_mean': b1_mean_list,\n",
    "                'b1_dp1_med': b1_med_list,\n",
    "                'b1_dp1_std': b1_std_list,\n",
    "                'b1_dp1_p25': b1_p25_list,\n",
    "                'b1_dp1_p50': b1_p50_list,\n",
    "                'b1_dp1_p75': b1_p75_list,\n",
    "                'b1_dp1_p95': b1_p95_list,\n",
    "                'b1_dp1_p99': b1_p99_list,\n",
    "                'b1_dp1_range': b1_range_list,\n",
    "\n",
    "                'b2_dp1_count': b2_count_list,\n",
    "                'b2_dp1_min': b2_min_list,\n",
    "                'b2_dp1_max': b2_max_list,\n",
    "                'b2_dp1_mean': b2_mean_list,\n",
    "                'b2_dp1_med': b2_med_list,\n",
    "                'b2_dp1_std': b2_std_list,\n",
    "                'b2_dp1_p25': b2_p25_list,\n",
    "                'b2_dp1_p50': b2_p50_list,\n",
    "                'b2_dp1_p75': b2_p75_list,\n",
    "                'b2_dp1_p95': b2_p95_list,\n",
    "                'b2_dp1_p99': b2_p99_list,\n",
    "                'b2_dp1_range': b2_range_list,\n",
    "\n",
    "                'b3_dp1_count': b3_count_list,\n",
    "                'b3_dp1_min': b3_min_list,\n",
    "                'b3_dp1_max': b3_max_list,\n",
    "                'b3_dp1_mean': b3_mean_list,\n",
    "                'b3_dp1_med': b3_med_list,\n",
    "                'b3_dp1_std': b3_std_list,\n",
    "                'b3_dp1_p25': b3_p25_list,\n",
    "                'b3_dp1_p50': b3_p50_list,\n",
    "                'b3_dp1_p75': b3_p75_list,\n",
    "                'b3_dp1_p95': b3_p95_list,\n",
    "                'b3_dp1_p99': b3_p99_list,\n",
    "                'b3_dp1_range': b3_range_list,\n",
    "\n",
    "\n",
    "                }\n",
    "\n",
    "    if len(b1_mean_list) > 0:\n",
    "        out_df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "        print(\"out_df len : \", out_df.shape)\n",
    "        list_.append(out_df)\n",
    "\n",
    "        print(\"out_df: \", out_df)\n",
    "    else:\n",
    "        out_df = out_df = pd.DataFrame()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_month_fn(var_filt, var_col):\n",
    "    ym_list = []\n",
    "    y_list = []\n",
    "    for i in var_filt[var_col].tolist():\n",
    "        n = int(i.month)\n",
    "        if n < 10:\n",
    "            z = \"0\" + str(n)\n",
    "        else:\n",
    "            z = str(n)\n",
    "        x = str(i.year) + z\n",
    "        ym_list.append(int(x))\n",
    "        y_list.append(int(i.year))\n",
    "\n",
    "    return ym_list, y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List output df feature structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_s_merge_list = ['uid', 'site',  'dt_year',  'stc_image', 'image_s_dt', 'band', 'stc_count', 'stc_min', 'stc_max', 'stc_mean', \n",
    "                  'stc_sum', 'stc_std', 'stc_med', 'stc_major', 'stc_minor', 'stc_one', 'stc_two', 'stc_three', \n",
    "                  'stc_four', 'stc_five', 'stc_six', 'stc_seven', 'stc_eight', 'stc_nine', 'stc_ten', 'stc_elev', \n",
    "                  'stc_twelv', 'stc_thirt', 'stc_fourt', 'stc_fift', 'stc_sixt', 'stc_sevent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpca2_y_merge_list = ['uid', 'site', 'dt_year', 'image', 'image_s_dt', 'b1_fpca2_count', 'b1_fpca2_min', \n",
    "                      'b1_fpca2_max', 'b1_fpca2_mean', 'b1_fpca2_med', 'b1_fpca2_std', 'b1_fpca2_p25', \n",
    "                      'b1_fpca2_p50', 'b1_fpca2_p75', 'b1_fpca2_p95', 'b1_fpca2_p99', 'b1_fpca2_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpca2_ym_merge_list = ['uid', 'site', 'dt_year', 'dt_ym', 'image', 'image_s_dt',  'fpca2_count', \n",
    "'fpca2_min', 'fpca2_max', 'fpca2_mean', 'fpca2_med', 'fpca2_std', 'fpca2_p25', 'fpca2_p50', 'fpca2_p75', 'fpca2_p95', \n",
    "'fpca2_p99', 'fpca2_range']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h99a_merge_list = ['uid', 'site', 'dt_year',  'image', 'image_s_dt', 'b1_h99a2_count', 'b1_h99a2_min', \n",
    "                   'b1_h99a2_max', 'b1_h99a2_mean', 'b1_h99a2_med', 'b1_h99a2_std', 'b1_h99a2_p25',\n",
    "                   'b1_h99a2_p50', 'b1_h99a2_p75', 'b1_h99a2_p95', 'b1_h99a2_p99', 'b1_h99a2_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_y_merge_list = ['uid', 'site', 'dt_year',  'image', 'image_s_dt', 'b1_dbi_count', 'b1_dbi_min', \n",
    "                    'b1_dbi_max', 'b1_dbi_mean', 'b1_dbi_med', 'b1_dbi_std', 'b1_dbi_p25', 'b1_dbi_p50', 'b1_dbi_p75', \n",
    "                    'b1_dbi_p95', 'b1_dbi_p99', 'b1_dbi_range', 'b2_dbi_count', 'b2_dbi_min', 'b2_dbi_max', 'b2_dbi_mean',\n",
    "                    'b2_dbi_med', 'b2_dbi_std', 'b2_dbi_p25', 'b2_dbi_p50', 'b2_dbi_p75', 'b2_dbi_p95', 'b2_dbi_p99', \n",
    "                    'b2_dbi_range', 'b3_dbi_count', 'b3_dbi_min', 'b3_dbi_max', 'b3_dbi_mean', 'b3_dbi_med', 'b3_dbi_std',\n",
    "                    'b3_dbi_p25', 'b3_dbi_p50', 'b3_dbi_p75', 'b3_dbi_p95', 'b3_dbi_p99', 'b3_dbi_range', 'b4_dbi_count', \n",
    "                    'b4_dbi_min', 'b4_dbi_max', 'b4_dbi_mean', 'b4_dbi_med', 'b4_dbi_std', 'b4_dbi_p25', 'b4_dbi_p50', \n",
    "                    'b4_dbi_p75', 'b4_dbi_p95', 'b4_dbi_p99', 'b4_dbi_range', 'b5_dbi_count', 'b5_dbi_min', 'b5_dbi_max',\n",
    "                    'b5_dbi_mean', 'b5_dbi_med', 'b5_dbi_std', 'b5_dbi_p25', 'b5_dbi_p50', 'b5_dbi_p75', 'b5_dbi_p95', \n",
    "                    'b5_dbi_p99', 'b5_dbi_range', 'b6_dbi_count', 'b6_dbi_min', 'b6_dbi_max', 'b6_dbi_mean', 'b6_dbi_med',\n",
    "                    'b6_dbi_std', 'b6_dbi_p25', 'b6_dbi_p50', 'b6_dbi_p75', 'b6_dbi_p95', 'b6_dbi_p99', 'b6_dbi_range', \n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_ym_merge_list = ['uid', 'site', 'dt_year', 'dt_ym', 'image', 'image_s_dt', 'b1_dbi_count', 'b1_dbi_min', 'b1_dbi_max', 'b1_dbi_mean', 'b1_dbi_med', 'b1_dbi_std', 'b1_dbi_p25', 'b1_dbi_p50', 'b1_dbi_p75', \n",
    " 'b1_dbi_p95', 'b1_dbi_p99', 'b1_dbi_range', 'b2_dbi_count', 'b2_dbi_min', 'b2_dbi_max', 'b2_dbi_mean', \n",
    " 'b2_dbi_med', 'b2_dbi_std', 'b2_dbi_p25', 'b2_dbi_p50', 'b2_dbi_p75', 'b2_dbi_p95', 'b2_dbi_p99', 'b2_dbi_range',\n",
    " 'b3_dbi_count', 'b3_dbi_min', 'b3_dbi_max', 'b3_dbi_mean', 'b3_dbi_med', 'b3_dbi_std', 'b3_dbi_p25', 'b3_dbi_p50',\n",
    " 'b3_dbi_p75', 'b3_dbi_p95', 'b3_dbi_p99', 'b3_dbi_range', 'b4_dbi_count', 'b4_dbi_min', 'b4_dbi_max', 'b4_dbi_mean',\n",
    " 'b4_dbi_med', 'b4_dbi_std', 'b4_dbi_p25', 'b4_dbi_p50', 'b4_dbi_p75', 'b4_dbi_p95', 'b4_dbi_p99', 'b4_dbi_range',\n",
    " 'b5_dbi_count', 'b5_dbi_min', 'b5_dbi_max', 'b5_dbi_mean', 'b5_dbi_med', 'b5_dbi_std', 'b5_dbi_p25', 'b5_dbi_p50',\n",
    " 'b5_dbi_p75', 'b5_dbi_p95', 'b5_dbi_p99', 'b5_dbi_range', 'b6_dbi_count', 'b6_dbi_min', 'b6_dbi_max', 'b6_dbi_mean',\n",
    " 'b6_dbi_med', 'b6_dbi_std', 'b6_dbi_p25', 'b6_dbi_p50', 'b6_dbi_p75', 'b6_dbi_p95', 'b6_dbi_p99', 'b6_dbi_range' \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dja_y_merge_list =  ['uid', 'site', 'dt_year', 'image', 'image_s_dt', 'b1_dja_count', 'b1_dja_min', 'b1_dja_max', 'b1_dja_mean', 'b1_dja_med', \n",
    " 'b1_dja_std', 'b1_dja_p25', 'b1_dja_p50', 'b1_dja_p75', 'b1_dja_p95', 'b1_dja_p99', 'b1_dja_range', 'image_s_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dja_y_merge_list = ['uid', 'site', 'dt_year',  'image', 'b1_dja_count', 'b1_dja_min', 'b1_dja_max', 'b1_dja_mean', 'b1_dja_med', 'b1_dja_std', 'b1_dja_p25', 'b1_dja_p50', 'b1_dja_p75', 'b1_dja_p95', 'b1_dja_p99', 'b1_dja_range', 'image_s_dt', 'image_e_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dja_ym_merge_list =['uid', 'site',  'dt_ym',  'image', 'image_s_dt',\n",
    "                    'dja_count', 'dja_min', 'dja_max', 'dja_mean', 'dja_med', 'dja_std', 'dja_p25', 'dja_p50', 'dja_p75', \n",
    "                    'dja_p95', 'dja_p99', 'dja_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_y_merge_list = ['uid', 'site', 'dt_year',  'dis_image', 'image_s_dt', 'dis_count', 'dis_min', 'dis_max', 'dis_mean', 'dis_sum', 'dis_std', 'dis_med', 'dis_major',\n",
    "                    'dis_minor', 'dis_one', 'dis_two', 'dis_three', 'dis_four', 'dis_five', 'dis_six', 'dis_seven', \n",
    "                    'dis_eight', 'dis_nine', 'dis_ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_ym_merge_list = ['uid', 'site', 'dt_year', 'dt_ym', 'image', 'image_s_dt', \n",
    " 'dis_count', 'dis_min', 'dis_max', 'dis_mean', 'dis_med', 'dis_std', 'dis_major', 'dis_minor', \n",
    " 'dis_one', 'dis_two', 'dis_three', 'dis_four', 'dis_five', 'dis_six', 'dis_seven', 'dis_eight', \n",
    " 'dis_nine', 'dis_ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dja_ym_merge_list = ['uid','site',  'dt_ym',  'image', 'image_s_dt', 'dja_count', 'dja_min', 'dja_max', 'dja_mean', 'dja_med', \n",
    "                     'dja_std', 'dja_p25', 'dja_p50', 'dja_p75', 'dja_p95', 'dja_p99', 'dja_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_y_merge_list = ['uid', 'site',  'dt_year',  'image', 'image_s_dt', \n",
    "                    'b1_dim_count', 'b1_dim_min', 'b1_dim_max', 'b1_dim_mean',\n",
    "       'b1_dim_med', 'b1_dim_std', 'b1_dim_p25', 'b1_dim_p50', 'b1_dim_p75',\n",
    "       'b1_dim_p95', 'b1_dim_p99', 'b1_dim_range', 'b2_dim_count',\n",
    "       'b2_dim_min', 'b2_dim_max', 'b2_dim_mean', 'b2_dim_med', 'b2_dim_std',\n",
    "       'b2_dim_p25', 'b2_dim_p50', 'b2_dim_p75', 'b2_dim_p95', 'b2_dim_p99',\n",
    "       'b2_dim_range', 'b3_dim_count', 'b3_dim_min', 'b3_dim_max',\n",
    "       'b3_dim_mean', 'b3_dim_med', 'b3_dim_std', 'b3_dim_p25', 'b3_dim_p50',\n",
    "       'b3_dim_p75', 'b3_dim_p95', 'b3_dim_p99', 'b3_dim_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_ym_merge_list = ['uid', 'site',  'dt_year', 'dt_ym', 'image', 'image_s_dt', 'b1_dim_count', 'b1_dim_min', 'b1_dim_max', 'b1_dim_mean', 'b1_dim_med', 'b1_dim_std', 'b1_dim_p25', 'b1_dim_p50', 'b1_dim_p75', \n",
    " 'b1_dim_p95', 'b1_dim_p99', 'b1_dim_range', 'b2_dim_count', 'b2_dim_min', 'b2_dim_max', 'b2_dim_mean', \n",
    " 'b2_dim_med', 'b2_dim_std', 'b2_dim_p25', 'b2_dim_p50', 'b2_dim_p75', 'b2_dim_p95', 'b2_dim_p99', 'b2_dim_range',\n",
    " 'b3_dim_count', 'b3_dim_min', 'b3_dim_max', 'b3_dim_mean', 'b3_dim_med', 'b3_dim_std', 'b3_dim_p25', 'b3_dim_p50',\n",
    " 'b3_dim_p75', 'b3_dim_p95', 'b3_dim_p99', 'b3_dim_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1_y_merge_list = ['uid', 'site', 'dt_year',  'image', 'image_s_dt',\n",
    "                    'b1_dp1_min', 'b1_dp1_max', 'b1_dp1_mean', 'b1_dp1_count', 'b1_dp1_std', 'b1_dp1_med', \n",
    "                    'b1_dp1_p25', 'b1_dp1_p50', 'b1_dp1_p75', 'b1_dp1_p95', 'b1_dp1_p99', 'b1_dp1_range', \n",
    "                    'b2_dp1_min', 'b2_dp1_max', 'b2_dp1_mean', 'b2_dp1_count', 'b2_dp1_std', 'b2_dp1_med', \n",
    "                    'b2_dp1_p25', 'b2_dp1_p50', 'b2_dp1_p75', 'b2_dp1_p95', 'b2_dp1_p99', 'b2_dp1_range', \n",
    "                    'b3_dp1_min', 'b3_dp1_max', 'b3_dp1_mean', 'b3_dp1_count', 'b3_dp1_med', 'b3_dp1_p25', \n",
    "                    'b3_dp1_p50', 'b3_dp1_p75', 'b3_dp1_p95', 'b3_dp1_p99', 'b3_dp1_range', 'b3_dp1_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1_ym_merge_list = ['uid', 'site', 'dt_year', 'dt_ym', 'image', 'image_s_dt', 'b1_dp1_count', 'b1_dp1_min', 'b1_dp1_max', 'b1_dp1_mean', 'b1_dp1_med', 'b1_dp1_std', 'b1_dp1_p25', 'b1_dp1_p50', 'b1_dp1_p75', \n",
    " 'b1_dp1_p95', 'b1_dp1_p99', 'b1_dp1_range', 'b2_dp1_count', 'b2_dp1_min', 'b2_dp1_max', 'b2_dp1_mean', \n",
    " 'b2_dp1_med', 'b2_dp1_std', 'b2_dp1_p25', 'b2_dp1_p50', 'b2_dp1_p75', 'b2_dp1_p95', 'b2_dp1_p99', 'b2_dp1_range',\n",
    " 'b3_dp1_count', 'b3_dp1_min', 'b3_dp1_max', 'b3_dp1_mean', 'b3_dp1_med', 'b3_dp1_std', 'b3_dp1_p25', 'b3_dp1_p50',\n",
    " 'b3_dp1_p75', 'b3_dp1_p95', 'b3_dp1_p99', 'b3_dp1_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_y_fn(var_df, fire_occ):\n",
    "    \n",
    "    # filter variable df by same site\n",
    "    var_filt = var_df[var_df[\"site\"]== i]\n",
    "    #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "    var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "    var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "\n",
    "    #merge fire scar and variable data on the nearest unburnt date\n",
    "    y_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "    \n",
    "    return y_merge, var_filt\n",
    "\n",
    "\n",
    "\n",
    "def append_y_merge(merge_df, merge_list, column_dict):\n",
    "    \n",
    "    merge_fire_mask = merge_df[merge_list]\n",
    "    merge_fire_mask.rename(columns=column_dict, inplace=True)\n",
    "\n",
    "    return merge_fire_mask\n",
    "\n",
    "\n",
    "def clean_ym_fn(df):\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    df.dropna(subset = [\"image_s_dt\"], inplace=True)\n",
    "    df.sort_values(by=\"image_s_dt\", inplace=True)\n",
    "\n",
    "    ym_list, y_list = year_month_fn(df, \"image_s_dt\")\n",
    "\n",
    "    df.loc[:, \"dt_ym\"] = ym_list\n",
    "    df.loc[:, \"dt_ym\"] = var_filt[\"dt_ym\"].astype(int)\n",
    "    df.loc[:, \"dt_year\"] = y_list\n",
    "    df.loc[:, \"dt_year\"] = var_filt[\"dt_year\"].astype(int)\n",
    "\n",
    "    df_ym = df.copy(deep = True)\n",
    "    print(\"var_filt_ym: \", df_ym.shape)\n",
    "    \n",
    "    return(df_ym)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_var_fire_scar_zonal_fn(df, dir_, var_, site):\n",
    "    exp_dir = os.path.join(dir_, var_)\n",
    "    mk_dir_fn(exp_dir)\n",
    "    output_path = os.path.join(exp_dir, f\"{site}_agb_nt_mosaic_{var_}_fire_scar_zonal.csv\")\n",
    "    df.to_csv((output_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list orders for export dataframe's\n",
    "df_list_ = [dka_0112_list,\n",
    "dim_0305_list,\n",
    "dim_0608_list,\n",
    "dim_0911_list,\n",
    "dim_1202_list,\n",
    "dis_0305_list,\n",
    "dis_0608_list,\n",
    "dis_0911_list,\n",
    "dis_1202_list,\n",
    "dbi_0608_list,\n",
    "dbi_0911_list,\n",
    "dbi_1202_list,\n",
    "dja_0305_list,\n",
    "dja_0608_list,\n",
    "dja_0911_list,\n",
    "dja_1202_list,       \n",
    "h99a2_0112_list,\n",
    "fpca2_0509_list,\n",
    "stc_0112_list,\n",
    "dp1_0112_list,\n",
    "dp1_0509_list]\n",
    "\n",
    "# df_list_ = [dka_0112_list[0],\n",
    "# dim_0305_list[0],\n",
    "# dim_0608_list[0],\n",
    "# dim_0911_list[0],\n",
    "# dim_1202_list[0],\n",
    "# dis_0305_list[0],\n",
    "# dis_0608_list[0],\n",
    "# dis_0911_list[0],\n",
    "# dis_1202_list[0],\n",
    "# dja_0305_list[0],\n",
    "# dja_0608_list[0],\n",
    "# dja_0911_list[0],\n",
    "# dja_1202_list[0],         \n",
    "# dbi_0608_list[0],\n",
    "# dbi_0911_list[0],\n",
    "# dbi_1202_list[0],\n",
    "# h99a2_0112_list[0],\n",
    "# fpca2_0509_list[0],\n",
    "# stc_0112_list[0],\n",
    "# dp1_0112_list[0],\n",
    "# dp1_0509_list[0]]\n",
    "\n",
    "df_str_list_ = [\"dka_0112_list\",\n",
    "\"dim_0305_list\",\n",
    "\"dim_0608_list\",\n",
    "\"dim_0911_list\",\n",
    "\"dim_1202_list\",\n",
    "\"dis_0305_list\",\n",
    "\"dis_0608_list\",\n",
    "\"dis_0911_list\",\n",
    "\"dis_1202_list\",\n",
    "\"dbi_0608_list\",\n",
    "\"dbi_0911_list\",\n",
    "\"dbi_1202_list\",\n",
    "\"dja_0305_list\",\n",
    "\"dja_0608_list\",\n",
    "\"dja_0911_list\",\n",
    "\"dja_1202_list\", \n",
    "\"h99a2_0112_list\",\n",
    "\"fpca2_0509_list\",\n",
    "\"stc_0112_list\",\n",
    "\"dp1_0112_list\",\n",
    "\"dp1_0509_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv_fn(list_, dir_, file_name):\n",
    "    \n",
    "    df_final = pd.concat(list_, axis =0)    \n",
    "    output_path = os.path.join(dir_, file_name)\n",
    "    df_final.to_csv(os.path.join(output_path), index=False)\n",
    "    print(\"File output to: \", output_path)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basal_merge_fire_year(basal_df, var_df, var_col, var_, i):\n",
    "\n",
    "    basal_filt = basal_df[basal_df[\"site\"]== i]\n",
    "    # merge previous output with basal\n",
    "    basal_nfy = pd.merge_asof(basal_filt, var_df, left_on=\"basal_dt\", right_on= var_col, by=\"site\", direction=\"nearest\")\n",
    "    column_dict ={'uid_x': 'uid', 'uid_y': f'uid_{var_}'}\n",
    "    basal_nfy.rename(columns=column_dict, inplace=True)\n",
    "    print(basal_nfy.columns)\n",
    "    \n",
    "    return basal_nfy\n",
    "\n",
    "def basal_dbi_merge_fire_year(basal_df, var_df, var_col, var_, i):\n",
    "\n",
    "    basal_filt = basal_df[basal_df[\"site\"]== i]\n",
    "    # merge previous output with basal\n",
    "    basal_nfy = pd.merge_asof(basal_filt, var_df, left_on=\"basal_dt\", right_on= var_col, by=\"site\", direction=\"nearest\")\n",
    "    column_dict ={'uid_x': 'uid', 'uid_y': f'uid_{var_}'}\n",
    "    basal_nfy.rename(columns=column_dict, inplace=True)\n",
    "    print(basal_nfy.columns)\n",
    "    \n",
    "    return basal_nfy\n",
    "\n",
    "\n",
    "# create lists for output df's\n",
    "\n",
    "fire_list = []\n",
    "\n",
    "dka_0112_list2 = []\n",
    "\n",
    "dim_0305_list2 = []\n",
    "dim_0608_list2 = []\n",
    "dim_0911_list2 = []\n",
    "dim_1202_list2 =[]\n",
    "                    \n",
    "dis_0305_list2 = []\n",
    "dis_0608_list2 = []\n",
    "dis_0911_list2 = []\n",
    "dis_1202_list2 =[]\n",
    "\n",
    "# dbi_0305_list = []\n",
    "dbi_0608_list2 = []\n",
    "dbi_0911_list2 = []\n",
    "dbi_1202_list2 =[]\n",
    "\n",
    "dja_0305_list2 = []\n",
    "dja_0608_list2 = []\n",
    "dja_0911_list2 = []\n",
    "dja_1202_list2 =[]\n",
    "\n",
    "\n",
    "h99a2_0112_list2 = []\n",
    "fpca2_0509_list2 = []\n",
    "\n",
    "stc_0112_list2 = []\n",
    "dp1_0112_list2 = []\n",
    "dp1_0509_list2 = []\n",
    "\n",
    "# basal output lists\n",
    "dka_0112_basal_list = []\n",
    "\n",
    "dim_0305_basal_list = []\n",
    "dim_0608_basal_list = []\n",
    "dim_0911_basal_list = []\n",
    "dim_1202_basal_list =[]\n",
    "                    \n",
    "dis_0305_basal_list = []\n",
    "dis_0608_basal_list = []\n",
    "dis_0911_basal_list = []\n",
    "dis_1202_basal_list =[]\n",
    "\n",
    "# dbi_0305_list = []\n",
    "dbi_0608_basal_list = []\n",
    "dbi_0911_basal_list = []\n",
    "dbi_1202_basal_list =[]\n",
    "\n",
    "dja_0305_basal_list = []\n",
    "dja_0608_basal_list = []\n",
    "dja_0911_basal_list = []\n",
    "dja_1202_basal_list =[]\n",
    "\n",
    "\n",
    "h99a2_0112_basal_list = []\n",
    "fpca2_0509_basal_list = []\n",
    "\n",
    "stc_0112_basal_list = []\n",
    "dp1_0112_basal_list = []\n",
    "dp1_0509_basal_list = []\n",
    "\n",
    "\n",
    "def concat_export_csv_fn(list_, dir_, file_name):\n",
    "    if len(list_) > 0:\n",
    "    \n",
    "        df_final = pd.concat(list_, axis =0)\n",
    "        print(\"df_final: \", df_final)\n",
    "        output_path = os.path.join(dir_, file_name)\n",
    "        df_final.to_csv(os.path.join(output_path), index=False)\n",
    "        print(\"File output to: \", output_path)\n",
    "    \n",
    "    else:\n",
    "        df_final = None\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "# ensure that bnt_st_ym is an int\n",
    "fire_mask.loc[:, \"bnt_st_ym\"] = fire_mask.loc[:, \"bnt_st_ym\"].astype(int)\n",
    "\n",
    "for var_, var_df in zip(cleaned_str_list, cleaned_df_list):\n",
    "    print(\"var: \", var_)\n",
    "    for i in fire_mask.site.unique():\n",
    "\n",
    "        # filter fire mask by site\n",
    "        fire_occ = fire_mask[(fire_mask[\"site\"]==i) & (fire_mask[\"bnt_st_ym\"]==0)]\n",
    "\n",
    "        #fire_occ = convert_to_dt_year(fire_occ, \"bn_year\", \"bn_y\")\n",
    "        fire_occ.loc[:, 'dt_year'] = fire_occ.loc[:,'bn_year'].astype(int)\n",
    "        fire_occ.sort_values(by=\"dt_year\", inplace=True)\n",
    "        #print(fire_occ)\n",
    "        fire_list.append(fire_occ)\n",
    "        \n",
    "        fire_occ_ym = fire_ym_mask[fire_ym_mask[\"site\"]==i]\n",
    "        fire_occ_ym.loc[:, 'dt_ym'] = fire_occ_ym.loc[:, \"ym_bfr_fs\"].astype(int)\n",
    "        fire_occ_ym.sort_values(by='dt_ym', inplace=True)\n",
    "        \n",
    "        # export csv per site to dir\n",
    "        export_csv_file_fn(fire_occ_ym, fms_dir, f\"fire_mask_ym_per_{i}.csv\") \n",
    "        \n",
    "#         if var_ == \"dka_0112\":\n",
    "            \n",
    "#             print(\"LOCATED_\"*100)\n",
    "            \n",
    "\n",
    "            \n",
    "#             var_filt = var_df[var_df[\"site\"]== i]\n",
    "#             print(\"dka len: \", var_filt.shape)\n",
    "#             #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "#             var_filt.dropna(subset=[\"s_date\"], inplace=True)\n",
    "#             print(\"dka len dropna: \", var_filt.shape)\n",
    "#             var_filt['dt_date'] = var_filt['s_date'].astype(int)\n",
    "#             var_filt.drop_duplicates(inplace=True)\n",
    "#             var_filt.sort_values(by=\"s_date\", inplace=True)\n",
    "#             print(\"-\"*50)\n",
    "#             print(i)\n",
    "#             print(var_filt.shape)\n",
    "#             print(var_filt)\n",
    "#             print(basal_df)\n",
    "            \n",
    "#                 # merge with basal not fire as is fire\n",
    "#             dka_merge = pd.merge_asof(basal_df, var_filt, left_on=\"basal_dt\", right_on= \"dt_date\", by=\"site\", direction=\"forward\")\n",
    "#             dka_single = dka_merge[dka_s_merge_list]\n",
    "#             dka_single.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dka_s_dt'}, inplace=True)\n",
    "            \n",
    "#             dka_0112_basal_list.append(dka_single)\n",
    "            \n",
    "#             import sys\n",
    "#             sys.exit()\n",
    "            #export_csv_file_fn(dka_s_single, ftzs_dir, \"dka_all_seasons_zonal_stats.csv\")\n",
    "            \n",
    "            #dka_0112_list.append(dka_s)\n",
    "\n",
    "            \n",
    "        if var_ == \"stc_0112\":\n",
    "            \n",
    "            # WORKING\n",
    "\n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            print(\"stc len: \", var_filt.shape)\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt.dropna(subset=[\"s_year\"], inplace=True)\n",
    "            print(\"stc len dropna: \", var_filt.shape)\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            print(\"-\"*50)\n",
    "            print(i)\n",
    "            print(var_filt.shape)\n",
    "\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            stc_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "\n",
    "            stc_s_fire_mask = stc_merge[stc_s_merge_list]\n",
    "            stc_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'stc_s_dt'}, inplace=True)\n",
    "            # append df to list\n",
    "            \n",
    "            stc_0112_list2.append(stc_s_fire_mask)\n",
    "            \n",
    "            # export site df to csv\n",
    "            export_var_fire_scar_zonal_fn(stc_s_fire_mask, fire_scar_dir, var_, i)\n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            stc_basal_nfy = basal_merge_fire_year(basal_df, stc_s_fire_mask, \"stc_s_dt\", var_, i) \n",
    "            stc_0112_basal_list.append(stc_basal_nfy)\n",
    "            #stc_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "        \n",
    "            \n",
    "        elif var_ == \"fpca2_0509\":\n",
    "            \n",
    "            # WORKING \n",
    "\n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "\n",
    "            var_filt.dropna(subset = [\"image_s_dt\"], inplace=True)\n",
    "            var_filt.sort_values(by=\"image_s_dt\", inplace=True)\n",
    "        \n",
    "            ym_list, y_list = year_month_fn(var_filt, \"image_s_dt\")\n",
    "     \n",
    "            var_filt.loc[:, \"dt_ym\"] = ym_list\n",
    "            var_filt.loc[:, \"dt_ym\"] = var_filt[\"dt_ym\"].astype(int)\n",
    "            var_filt.loc[:, \"dt_year\"] = y_list\n",
    "            var_filt.loc[:, \"dt_year\"] = var_filt[\"dt_year\"].astype(int)\n",
    "            \n",
    "            var_filt_ym = var_filt.copy(deep = True)\n",
    "            print(\"var_filt_ym: \", var_filt_ym.shape)       \n",
    "\n",
    "            \n",
    "            # merge fire scar and variable data on the nearest unburnt date\n",
    "            fpca2_0509_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            fpca2_0509_fire_mask = fpca2_0509_merge[fpca2_y_merge_list]\n",
    "            fpca2_0509_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'dt_ym': 'dt_be_ym_fs', \n",
    "                                                 'image_s_dt': 'fpca2_s_dt'}, inplace=True)\n",
    "\n",
    "            fpca2_0509_list2.append(fpca2_0509_fire_mask)\n",
    "            \n",
    "            # export file\n",
    "            export_var_fire_scar_zonal_fn(fpca2_0509_fire_mask, fire_scar_dir, var_, i)\n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            fpca2_0509_basal_nfy = basal_merge_fire_year(basal_df, fpca2_0509_fire_mask, \"fpca2_s_dt\", var_, i) \n",
    "            fpca2_0509_basal_list.append(fpca2_0509_basal_nfy)\n",
    "            #fpca2_0509_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "             \n",
    "   \n",
    "        elif var_ == \"h99a2_0112\":\n",
    "            \n",
    "            # WORKING\n",
    "            \n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            h99a_0112_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            \n",
    "            h99a_0112_fire_mask = h99a_0112_merge[h99a_merge_list]\n",
    "            h99a_0112_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'h99a_s_dt',\n",
    "                                                'image_e_dt': 'h99a_e_dt'}, inplace=True)\n",
    "            \n",
    "            #h99a_0112_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_merge_fire_mask.csv\".format(i, var_))\n",
    "\n",
    "            h99a2_0112_list2.append(h99a_0112_fire_mask)  \n",
    "            # export file\n",
    "            export_var_fire_scar_zonal_fn(h99a_0112_fire_mask, fire_scar_dir, var_, i)\n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            h99a2_basal_nfy = basal_merge_fire_year(basal_df, h99a_0112_fire_mask, \"h99a_s_dt\", var_, i) \n",
    "            h99a2_0112_basal_list.append(h99a2_basal_nfy)\n",
    "            #h99a2_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        elif var_ == \"dbi_0608\" or var_ == \"dbi_0911\" or var_ == \"dbi_1202\":\n",
    "            \n",
    "            # NOT WORKING -SEASONAL\n",
    "            \n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            var_filt.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}.csv\".format(i, var_))\n",
    "            fire_occ.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}_fire_occ.csv\".format(i, var_))\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            dbi_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            \n",
    "            #todo - remove this once I have additonal processed dbi landsat mosaics\n",
    "            if dbi_merge.empty:\n",
    "                print(dbi_merge)\n",
    "\n",
    "                dbi_merge = var_filt\n",
    "                \n",
    "            dbi_s_fire_mask = dbi_merge[dbi_y_merge_list]\n",
    "            dbi_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dbi_s_dt'}, inplace=True)\n",
    "            dbi_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}_mask.csv\".format(i, var_))\n",
    "            \n",
    "    \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            dbi_basal_nfy = basal_merge_fire_year(basal_df, dbi_s_fire_mask, \"dbi_s_dt\", var_, i)\n",
    "            \n",
    "            \n",
    "            if var_ == \"dbi_0608\":\n",
    "                dbi_0608_list2.append(dbi_s_fire_mask)\n",
    "                dbi_0608_basal_list.append(dbi_basal_nfy)\n",
    "                dbi_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dbi_0911\":\n",
    "                dbi_0911_list2.append(dbi_s_fire_mask)\n",
    "                dbi_0911_basal_list.append(dbi_basal_nfy)\n",
    "                dbi_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            \n",
    "            elif var_ == \"dbi_1202\":\n",
    "                dbi_1202_list2.append(dbi_s_fire_mask)\n",
    "                dbi_1202_basal_list.append(dbi_basal_nfy)\n",
    "                dbi_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230204\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "                \n",
    "            var_split_list = var_.split(\"_\")\n",
    "            var_str = var_split_list[0] + \"_\" + var_split_list[1]\n",
    "            # export file\n",
    "#             export_var_fire_scar_zonal_fn(dbi_s_fire_mask, fire_scar_dir, var_, i)\n",
    "            export_var_fire_scar_zonal_fn(dbi_s_fire_mask, fire_scar_dir, var_str, i)\n",
    "    \n",
    "#             # NOTE insufficent data to mask\n",
    "            \n",
    "        elif var_ == \"dja_0305\" or var_ == \"dja_0608\" or var_ == \"dja_0911\" or var_ == \"dja_1202\":\n",
    "        \n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            #var_filt.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}.csv\".format(i, var_))\n",
    "            #fire_occ.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_fire_occ.csv\".format(i, var_))\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            dja_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            dja_s_fire_mask = dja_merge[dja_y_merge_list]\n",
    "            dja_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dja_s_dt'}, inplace=True)\n",
    "            #dja_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_mask.csv\".format(i, var_))           \n",
    "            \n",
    "            \n",
    "            # WORKING\n",
    "            dja_y_merge, var_filt = clean_y_fn(var_df, fire_occ)\n",
    "            \n",
    "            column_dict ={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dja_s_dt'}\n",
    "            dja_y_merge_fire_mask = append_y_merge(dja_y_merge, dja_y_merge_list, column_dict)\n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            dja_basal_nfy = basal_merge_fire_year(basal_df, dja_s_fire_mask, \"dja_s_dt\", var_, i)\n",
    "\n",
    "\n",
    "            if var_ == \"dja_0305\":\n",
    "                dja_0305_list2.append(dja_y_merge_fire_mask)\n",
    "                \n",
    "                dja_0305_basal_list.append(dja_basal_nfy)\n",
    "               # dja_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dja_0608\":\n",
    "                dja_0608_list2.append(dja_y_merge_fire_mask)\n",
    "                \n",
    "                dja_0608_basal_list.append(dja_basal_nfy)\n",
    "                #dja_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dja_0911\":\n",
    "                dja_0911_list2.append(dja_y_merge_fire_mask)\n",
    "                \n",
    "                dja_0911_basal_list.append(dja_basal_nfy)\n",
    "                #dja_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dja_1202\":\n",
    "                dja_1202_list2.append(dja_y_merge_fire_mask)\n",
    "                \n",
    "                dja_1202_basal_list.append(dja_basal_nfy)\n",
    "                #dja_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "                \n",
    "            \n",
    "        elif var_ == \"dis_0305\" or var_ == \"dis_0608\" or var_ == \"dis_0911\" or var_ == \"dis_1202\":\n",
    "        \n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            var_filt.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}.csv\".format(i, var_))\n",
    "            fire_occ.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_fire_occ.csv\".format(i, var_))\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            dis_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            dis_s_fire_mask = dis_merge[dis_y_merge_list]\n",
    "            dis_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dis_s_dt'}, inplace=True)\n",
    "            #dis_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_mask.csv\".format(i, var_))           \n",
    "            \n",
    "\n",
    "                        \n",
    "            # WORKING\n",
    "            dis_y_merge, var_filt = clean_y_fn(var_df, fire_occ)\n",
    "            \n",
    "            column_dict ={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dis_s_dt'}\n",
    "            dis_y_merge_fire_mask = append_y_merge(dis_y_merge, dis_y_merge_list, column_dict)\n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            dis_basal_nfy = basal_merge_fire_year(basal_df, dis_y_merge_fire_mask, \"dis_s_dt\", var_, i)    \n",
    "            \n",
    "            \n",
    "            if var_ == \"dis_0305\":\n",
    "                dis_0305_list2.append(dis_y_merge_fire_mask)\n",
    "                dis_0305_basal_list.append(dis_basal_nfy)\n",
    "                #dis_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dis_0608\":\n",
    "                dis_0608_list2.append(dis_y_merge_fire_mask)\n",
    "                dis_0608_basal_list.append(dis_basal_nfy)\n",
    "                #dis_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dis_0911\":\n",
    "                dis_0911_list2.append(dis_y_merge_fire_mask)\n",
    "                dis_0911_basal_list.append(dis_basal_nfy)\n",
    "                #dis_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dis_1202\":\n",
    "                dis_1202_list2.append(dis_y_merge_fire_mask)\n",
    "                dis_1202_basal_list.append(dis_basal_nfy)\n",
    "                #dis_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            else:\n",
    "                print(\"ERROR\")        \n",
    "\n",
    "                \n",
    "        elif var_ == \"dim_0305\" or var_ == \"dim_0608\" or var_ == \"dim_0911\" or var_ == \"dim_1202\":\n",
    "        \n",
    "            # filter variable df by same site\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            #var_filt.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}.csv\".format(i, var_))\n",
    "            #fire_occ.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_fire_occ.csv\".format(i, var_))\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            dim_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            \n",
    "            dim_s_fire_mask = dim_merge[dim_y_merge_list]\n",
    "            dim_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dim_s_dt'}, inplace=True)\n",
    "            #dim_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_mask.csv\".format(i, var_))           \n",
    "            \n",
    "            \n",
    "            #WORKING\n",
    "            dim_y_merge, var_filt = clean_y_fn(var_df, fire_occ)\n",
    "            \n",
    "            column_dict ={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dim_s_dt'}\n",
    "            dim_y_merge_fire_mask = append_y_merge(dim_y_merge, dim_y_merge_list, column_dict)\n",
    "            \n",
    "            \n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            dim_basal_nfy = basal_merge_fire_year(basal_df, dim_y_merge_fire_mask, \"dim_s_dt\", var_, i)\n",
    "                \n",
    "            \n",
    "            if var_ == \"dim_0305\":\n",
    "                dim_0305_list2.append(dim_y_merge_fire_mask)\n",
    "                dim_0305_basal_list.append(dim_basal_nfy)\n",
    "                #dim_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            \n",
    "            \n",
    "            elif var_ == \"dim_0608\":\n",
    "                dim_0608_list2.append(dim_y_merge_fire_mask)\n",
    "                dim_0608_basal_list.append(dim_basal_nfy)\n",
    "                #dim_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dim_0911\":\n",
    "                dim_0911_list2.append(dim_y_merge_fire_mask)\n",
    "                \n",
    "                dim_0911_basal_list.append(dim_basal_nfy)\n",
    "                #dim_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            \n",
    "            elif var_ == \"dim_1202\":\n",
    "                dim_1202_list2.append(dim_y_merge_fire_mask)\n",
    "                dim_1202_basal_list.append(dim_basal_nfy)\n",
    "                #dim_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "                \n",
    "                \n",
    "        elif var_ == \"dp1_0112\" or var_ == \"dp1_0509\": # or var_ == \"dp1_0911\" or var_ == \"dp1_1202\":\n",
    "        \n",
    "            # filter variable df by same site\n",
    "            var_df.dropna(subset=[\"b1_dp1_mean\"], inplace = True)\n",
    "            var_filt = var_df[var_df[\"site\"]== i]\n",
    "            #test = convert_to_dt_year(test, \"s_year\", \"bn_y\")\n",
    "            var_filt['dt_year'] = var_filt['s_year'].astype(int)\n",
    "            var_filt.sort_values(by=\"dt_year\", inplace=True)\n",
    "            #var_filt.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}.csv\".format(i, var_))\n",
    "            #fire_occ.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_fire_occ.csv\".format(i, var_))\n",
    "            #merge fire scar and variable data on the nearest unburnt date\n",
    "            dp1_merge = pd.merge(fire_occ, var_filt, on = [\"site\", \"dt_year\"], how= \"inner\")\n",
    "            dp1_s_fire_mask = dp1_merge[dp1_y_merge_list]\n",
    "            dp1_s_fire_mask.rename(columns={'dt_year': 'dt_no_a_fs', 'image_s_dt': 'dp1_s_dt',\n",
    "                                            'uid_x': 'uid'}, inplace=True)\n",
    "\n",
    "            # call the basal merge function to merge no fire year zonal with site basal\n",
    "            dp1_basal_nfy = basal_merge_fire_year(basal_df, dp1_s_fire_mask, \"dp1_s_dt\", var_, i)\n",
    "                \n",
    "                \n",
    "            # append and export data    \n",
    "            if var_ == \"dp1_0112\":\n",
    "                dp1_0112_list2.append(dp1_s_fire_mask)\n",
    "                #dp1_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_merge_fire_mask.csv\".format(i, var_))\n",
    "                dp1_0112_basal_list.append(dp1_basal_nfy)\n",
    "                #dp1_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "            \n",
    "            elif var_ == \"dp1_0509\":\n",
    "                dp1_0509_list2.append(dp1_s_fire_mask)\n",
    "                #dp1_s_fire_mask.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_fire_mask.csv\".format(i, var_))\n",
    "                dp1_0509_basal_list.append(dp1_basal_nfy)\n",
    "                #dp1_basal_nfy.to_csv(r\"D:\\cdu\\data\\zonal_stats\\output\\20230128\\scratch\\{0}_{1}_y_basal_merge_fire_mask.csv\".format(i, var_))\n",
    "           \n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "                \n",
    "# complete outputs\n",
    "stc_0112_y = export_csv_fn(stc_0112_list2, fire_scar_dir, \"stc_0112_fire_mask.csv\")\n",
    "fpca_0509_y = export_csv_fn(fpca2_0509_list2, fire_scar_dir, \"fpca2_0509_fire_mask.csv\")\n",
    "h99a_0112_y = export_csv_fn(h99a2_0112_list2, fire_scar_dir, \"h99a_0112_fire_mask.csv\")\n",
    "\n",
    "dbi_0608_y = export_csv_fn(dbi_0608_list2, fire_scar_dir, \"dbi_0608_fire_mask.csv\")\n",
    "dbi_0911_y = export_csv_fn(dbi_0911_list2, fire_scar_dir, \"dbi_0911_fire_mask.csv\")\n",
    "dbi_1202_y = export_csv_fn(dbi_1202_list2, fire_scar_dir, \"dbi_1202_fire_mask.csv\")\n",
    "\n",
    "\n",
    "dja_0305_y = export_csv_fn(dja_0305_list2, fire_scar_dir, \"dja_0305_fire_mask.csv\")\n",
    "dja_0608_y = export_csv_fn(dja_0608_list2, fire_scar_dir, \"dja_0608_fire_mask.csv\")\n",
    "dja_0911_y = export_csv_fn(dja_0911_list2, fire_scar_dir, \"dja_0911_fire_mask.csv\")\n",
    "dja_1202_y = export_csv_fn(dja_1202_list2, fire_scar_dir, \"dja_1202_fire_mask.csv\")\n",
    "\n",
    "dis_0305_y = export_csv_fn(dis_0305_list2, fire_scar_dir, \"dis_0305_fire_mask.csv\")\n",
    "dis_0608_y = export_csv_fn(dis_0608_list2, fire_scar_dir, \"dis_0608_fire_mask.csv\")\n",
    "dis_0911_y = export_csv_fn(dis_0911_list2, fire_scar_dir, \"dis_0911_fire_mask.csv\")\n",
    "dis_1202_y = export_csv_fn(dis_1202_list2, fire_scar_dir, \"dis_1202_fire_mask.csv\")\n",
    "\n",
    "dim_0305_y = export_csv_fn(dim_0305_list2, fire_scar_dir, \"dim_0305_fire_mask.csv\")\n",
    "dim_0608_y = export_csv_fn(dim_0608_list2, fire_scar_dir, \"dim_0608_fire_mask.csv\")\n",
    "dim_0911_y = export_csv_fn(dim_0911_list2, fire_scar_dir, \"dim_0911_fire_mask.csv\")\n",
    "dim_1202_y = export_csv_fn(dim_1202_list2, fire_scar_dir, \"dim_1202_fire_mask.csv\")\n",
    "\n",
    "dp1_0112_y = export_csv_fn(dp1_0112_list2, fire_scar_dir, \"dp1_0112_fire_mask.csv\")\n",
    "dp1_0509_y = export_csv_fn(dp1_0509_list2, fire_scar_dir, \"dp1_0509_fire_mask.csv\")\n",
    "\n",
    "# # ------------------------------- Basal ----------------------------------------\n",
    "\n",
    "stc_0112_basal_y = export_csv_fn(stc_0112_basal_list, no_fire_scar_basal_dir, \"stc_0112_basal_with_y_fire_mask_applied.csv\")\n",
    "fpca_0509_basal_y = export_csv_fn(fpca2_0509_basal_list, no_fire_scar_basal_dir, \"fpca2_0509_basal_with_y_fire_mask_applied.csv\")\n",
    "h99a_0112_basal_y = export_csv_fn(h99a2_0112_basal_list, no_fire_scar_basal_dir, \"h99a_0112_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "dbi_0608_basal_y = export_csv_fn(dbi_0608_basal_list, no_fire_scar_basal_dir, \"dbi_0608_basal_with_y_fire_mask_applied.csv\")\n",
    "dbi_0911_basal_y = export_csv_fn(dbi_0911_basal_list, no_fire_scar_basal_dir, \"dbi_0911_basal_with_y_fire_mask_applied.csv\")\n",
    "dbi_1202_basal_y = export_csv_fn(dbi_1202_basal_list, no_fire_scar_basal_dir, \"dbi_1202_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "\n",
    "dja_0305_basal_y = export_csv_fn(dja_0305_basal_list, no_fire_scar_basal_dir, \"dja_0305_basal_with_y_fire_mask_applied.csv\")\n",
    "dja_0608_basal_y = export_csv_fn(dja_0608_basal_list, no_fire_scar_basal_dir, \"dja_0608_basal_with_y_fire_mask_applied.csv\")\n",
    "dja_0911_basal_y = export_csv_fn(dja_0911_basal_list, no_fire_scar_basal_dir, \"dja_0911_basal_with_y_fire_mask_applied.csv\")\n",
    "dja_1202_basal_y = export_csv_fn(dja_1202_basal_list, no_fire_scar_basal_dir, \"dja_1202_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "dis_0305_basal_y = export_csv_fn(dis_0305_basal_list, no_fire_scar_basal_dir, \"dis_0305_basal_with_y_fire_mask_applied.csv\")\n",
    "dis_0608_basal_y = export_csv_fn(dis_0608_basal_list, no_fire_scar_basal_dir, \"dis_0608_basal_with_y_fire_mask_applied.csv\")\n",
    "dis_0911_basal_y = export_csv_fn(dis_0911_basal_list, no_fire_scar_basal_dir, \"dis_0911_basal_with_y_fire_mask_applied.csv\")\n",
    "dis_1202_basal_y = export_csv_fn(dis_1202_basal_list, no_fire_scar_basal_dir, \"dis_1202_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "dim_0305_basal_y = export_csv_fn(dim_0305_basal_list, no_fire_scar_basal_dir, \"dim_0305_basal_with_y_fire_mask_applied.csv\")\n",
    "dim_0608_basal_y = export_csv_fn(dim_0608_basal_list, no_fire_scar_basal_dir, \"dim_0608_basal_with_y_fire_mask_applied.csv\")\n",
    "dim_0911_basal_y = export_csv_fn(dim_0911_basal_list, no_fire_scar_basal_dir, \"dim_0911_basal_with_y_fire_mask_applied.csv\")\n",
    "dim_1202_basal_y = export_csv_fn(dim_1202_basal_list, no_fire_scar_basal_dir, \"dim_1202_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "dp1_0112_basal_y = export_csv_fn(dp1_0112_basal_list, no_fire_scar_basal_dir, \"dp1_0112_basal_with_y_fire_mask_applied.csv\")\n",
    "dp1_0509_basal_y = export_csv_fn(dp1_0509_basal_list, no_fire_scar_basal_dir, \"dp1_0509_basal_with_y_fire_mask_applied.csv\")\n",
    "\n",
    "\n",
    "# Fire mask output csv\n",
    "export_csv_fn(fire_list, fire_mask_dir, \"fire_mask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dim_0305_basal_y\n",
    "col_names = df.columns.tolist()\n",
    "band = 1\n",
    "res = [i for i in col_names if f\"b{band}\" in i]\n",
    "res.append('bio_agb_kg1ha')\n",
    "# print(res)\n",
    "df1 = df[res]\n",
    "# print(df1)\n",
    "corr_matrix = df1.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire scars taken into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df1, title in zip([h99a_0112_basal_y],\n",
    "             [\"h99a_0112_basal_y\"]):\n",
    "    df1.dropna(inplace=True)\n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_h99a2_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df1, title in zip([fpca_0509_basal_y],\n",
    "             [\"fpca_0509_basal_y\"]):\n",
    "    df1.dropna(inplace=True)\n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_fpca2_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for band_ in range(0,3):\n",
    "    print(\"=\"*100)\n",
    "    band = band_+1\n",
    "    print(f\"BAND: {band}\")\n",
    "\n",
    "    for df1, title in zip([dim_0305_basal_y, dim_0608_basal_y, dim_0911_basal_y, dim_1202_basal_y],\n",
    "                 [\"dim_0305_basal_y\", \"dim_0608_basal_y\", \"dim_0911_basal_y\", \"dim_1202_basal_y\"]):\n",
    "        df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "        value_y = 'bio_agb_kg1ha'\n",
    "        value_x = f'b{band}_dim_mean'\n",
    "        sns.regplot(x= value_x, y=value_y, data=df)\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "\n",
    "        print(f\"Comparison relationship: {title} title, band: {band}\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"slope: \", slope)\n",
    "        print(\"intersept: \", intercept)\n",
    "        print(\"r2: \", r_value)\n",
    "        print(\"P_value: \", p_value)\n",
    "        print(\"std error: \", std_err)\n",
    "        plt.show()\n",
    "        print('-'*50)\n",
    "\n",
    "        # correlation matrix\n",
    "        col_names = df.columns.tolist()\n",
    "        res = [i for i in col_names if f\"b{band}\" in i]\n",
    "        res_ = res[:6]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        \n",
    "        print('-'*50)\n",
    "        res_ = res[6:]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim_0911 highest r2 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df1, title in zip([dja_0305_basal_y, dja_0608_basal_y, dja_0911_basal_y, dja_1202_basal_y],\n",
    "             [\"dja_0305_basal_y\", \"dja_0608_basal_y\", \"dja_0911_basal_y\", \"dja_1202_basal_y\"]):\n",
    "    \n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    \n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_dja_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for band_ in range(0,3):\n",
    "    print(\"=\"*100)\n",
    "    band = band_+1\n",
    "    print(f\"BAND: {band}\")\n",
    "\n",
    "    for df1, title in zip([dp1_0112_basal_y, dp1_0509_basal_y],\n",
    "                 [\"dp1_0112_basal_y\", \"dp1_0509_basal_y\"]):\n",
    "        \n",
    "        df1.dropna(inplace=True)\n",
    "        df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "        value_y = 'bio_agb_kg1ha'\n",
    "        value_x = f'b{band}_dp1_mean'\n",
    "        sns.regplot(x= value_x, y=value_y, data=df)\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "\n",
    "        print(f\"Comparison relationship: {title} title, band: {band}\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"slope: \", slope)\n",
    "        print(\"intersept: \", intercept)\n",
    "        print(\"r2: \", r_value)\n",
    "        print(\"P_value: \", p_value)\n",
    "        print(\"std error: \", std_err)\n",
    "        plt.show()\n",
    "        print('-'*50)\n",
    "        \n",
    "        # correlation matrix\n",
    "        col_names = df.columns.tolist()\n",
    "        res = [i for i in col_names if f\"b{band}\" in i]\n",
    "        res_ = res[:6]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        \n",
    "        print('-'*50)\n",
    "        res_ = res[6:]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dp1_0112 highest R2 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire scars not considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df1, title in zip([h99a_0112_basal_nfs],\n",
    "             [\"h99a_0112_basal_nfs\"]):\n",
    "    df1.dropna(inplace=True)\n",
    "    \n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_h99a2_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df1, title in zip([fpca_0509_basal_nfs],\n",
    "             [\"fpca_0509_basal_nfs\"]):\n",
    "    df1.dropna(inplace=True)\n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_fpca2_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_0608_basal_nfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band_ in range(0,3):\n",
    "    print(\"=\"*100)\n",
    "    band = band_+1\n",
    "    print(f\"BAND: {band}\")\n",
    "\n",
    "    for df1, title in zip([dim_0305_basal_nfs, dim_0608_basal_nfs, dim_0911_basal_nfs, dim_1202_basal_nfs],\n",
    "                 [\"dim_0305_basal_nfs\", \"dim_0608_basal_nfs\", \"dim_0911_basal_nfs\", \"dim_1202_basal_nfs\"]):\n",
    "\n",
    "        df1.dropna(inplace=True)\n",
    "        \n",
    "        df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "\n",
    "        value_y = 'bio_agb_kg1ha'\n",
    "        value_x = f'b{band}_dim_mean'\n",
    "        print(value_x)\n",
    "        print(value_y)\n",
    "        sns.regplot(x= value_x, y=value_y, data=df)\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "\n",
    "        print(f\"Comparison relationship: {title} title, band: {band_}\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"slope: \", slope)\n",
    "        print(\"intersept: \", intercept)\n",
    "        print(\"r2: \", r_value)\n",
    "        print(\"P_value: \", p_value)\n",
    "        print(\"std error: \", std_err)\n",
    "#         plt.text(28000, 65, f\"R2: {r_value.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "#         plt.text(28000, 60, f\"std: {std_err.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "        plt.show()\n",
    "        print('-'*50)\n",
    "\n",
    "        # correlation matrix\n",
    "        col_names = df.columns.tolist()\n",
    "        res = [i for i in col_names if f\"b{band}\" in i]\n",
    "        res_ = res[:6]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        \n",
    "        print('-'*50)\n",
    "        res_ = res[6:]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim_1911 highest R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df1, title in zip([dja_0305_basal_nfs, dja_0608_basal_nfs, dja_0911_basal_nfs, dja_1202_basal_nfs],\n",
    "             [\"dja_0305_basal_nfs\", \"dja_0608_basal_nfs\", \"dja_0911_basal_nfs\", \"dja_1202_basal_nfs\"]):\n",
    "    df1.dropna(inplace=True)\n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    \n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_dja_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "#     plt.text(28000, 65, f\"R2: {r_value.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "#     plt.text(28000, 60, f\"std: {std_err.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band_ in range(0,1):\n",
    "    print(\"=\"*100)\n",
    "    band = band_+1\n",
    "    print(f\"BAND: {band}\")\n",
    "\n",
    "    for df1, title in zip([dp1_0112_basal_nfs, dp1_0509_basal_nfs],\n",
    "                 [\"dp1_0112_basal_nfs\", \"dp1_0509_basal_nfs\"]):\n",
    "\n",
    "        df1.dropna(inplace=True)\n",
    "        df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "            \n",
    "        value_y = 'bio_agb_kg1ha'\n",
    "        value_x = f'b{band}_dp1_mean'\n",
    "        sns.regplot(x= value_x, y=value_y, data=df)\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "\n",
    "        print(f\"Comparison relationship: {title} title, band: {band}\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"slope: \", slope)\n",
    "        print(\"intersept: \", intercept)\n",
    "        print(\"r2: \", r_value)\n",
    "        print(\"P_value: \", p_value)\n",
    "        print(\"std error: \", std_err)\n",
    "#         plt.text(28000, 65, f\"R2: {r_value.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "#         plt.text(28000, 60, f\"std: {std_err.round(3)}\", {'color': 'black', 'fontsize': 12})\n",
    "        plt.show()\n",
    "        print('-'*50)\n",
    "        \n",
    "        # correlation matrix\n",
    "        col_names = df.columns.tolist()\n",
    "        res = [i for i in col_names if f\"b{band}\" in i]\n",
    "        res_ = res[:6]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        \n",
    "        print('-'*50)\n",
    "        res_ = res[6:]\n",
    "        res_.append('bio_agb_kg1ha')\n",
    "        df1 = df[res_]\n",
    "        # plotting correlation heatmap\n",
    "        dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "        plt.show()\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df1, title in zip([dja_0305_basal_y, dja_0608_basal_y, dja_0911_basal_y, dja_1202_basal_y],\n",
    "             [\"dja_0305_basal_y\", \"dja_0608_basal_y\", \"dja_0911_basal_y\", \"dja_1202_basal_y\"]):\n",
    "    \n",
    "    df = df1[df1['bio_agb_kg1ha']!=0]\n",
    "    value_y = 'bio_agb_kg1ha'\n",
    "    value_x = 'b1_dja_mean'\n",
    "    sns.regplot(x= value_x, y=value_y, data=df)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "    \n",
    "    print(\"Comparison relationship: \", title)\n",
    "    print(\"*\"*50)\n",
    "    print(\"slope: \", slope)\n",
    "    print(\"intersept: \", intercept)\n",
    "    print(\"r2: \", r_value)\n",
    "    print(\"P_value: \", p_value)\n",
    "    print(\"std error: \", std_err)\n",
    "    plt.show()\n",
    "    print('-'*50)\n",
    "    \n",
    "    # correlation matrix\n",
    "    band = 1\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res.append('bio_agb_kg1ha')\n",
    "    # print(res)\n",
    "    df1 = df[res]\n",
    "    # print(df1)\n",
    "\n",
    "    \n",
    "    # correlation matrix\n",
    "    col_names = df.columns.tolist()\n",
    "    res = [i for i in col_names if f\"b{band}\" in i]\n",
    "    res_ = res[:6]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('-'*50)\n",
    "    res_ = res[6:]\n",
    "    res_.append('bio_agb_kg1ha')\n",
    "    df1 = df[res_]\n",
    "    # plotting correlation heatmap\n",
    "    dataplot = sns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "    plt.show()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://builtin.com/data-science/poisson-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pyshark.com/poisson-distribution-and-poisson-process-in-python/#:~:text=A%20Poisson%20point%20process%20(or,stochastic)%20process%20in%20one%20dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://timeseriesreasoning.com/contents/poisson-process/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

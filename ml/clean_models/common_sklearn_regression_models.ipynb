{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn_classification_models_for_seasonal_selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.projectpro.io/article/multi-class-classification-python-example/547#mcetoc_1fpjsn4g8g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educba.com/keras-sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load modules "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following conditions apply:\n",
    "\n",
    " - env = biomass_zonal\n",
    " - data merged_slats_field_agb_dp1_start.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#from sklearn import metrics\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy\n",
    "import scipy.stats as sc\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(x, y):\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    try:\n",
    "        ev = metrics.explained_variance_score(x, y)\n",
    "    except:\n",
    "        ev = np.nan\n",
    "    print(\"Ev score: \", ev)\n",
    "    \n",
    "    try:\n",
    "        me = metrics.max_error(x, y)\n",
    "    except:\n",
    "        me = np.nan\n",
    "    print(\"Maximum Error: \", me)\n",
    "    \n",
    "    try:\n",
    "        mae = metrics.mean_absolute_error(x, y)\n",
    "    except:\n",
    "        mae = np.nan\n",
    "    print(\"Mean Absolute Error: \", mae)\n",
    "    \n",
    "    try:\n",
    "        mse = metrics.mean_squared_error(x, y)\n",
    "    except:\n",
    "        mse = np.nan\n",
    "    print(\"Mean Squared Error: \", mse)\n",
    "    \n",
    "    try:\n",
    "        rmse = metrics.mean_squared_error(x, y, squared=False)\n",
    "    except:\n",
    "        rmse =  np.nan\n",
    "    print(\"Root Mean Squared Error: \", rmse)\n",
    "    \n",
    "    try:\n",
    "        msle = metrics.mean_squared_log_error(x, y)\n",
    "    except:\n",
    "        msle = np.nan\n",
    "    print(\"Mean Squared Log Error: \", msle)\n",
    "   \n",
    "    try:\n",
    "        rmsle = metrics.mean_squared_log_error(x, y, squared = False)\n",
    "    except:\n",
    "        rmsle =  np.nan\n",
    "    print(\"Root Mean Squared Log Error : \", rmsle)\n",
    "    \n",
    "    try:\n",
    "        mape = metrics.mean_absolute_percentage_error(x, y)\n",
    "    except:\n",
    "        mape = np.nan\n",
    "    print(\"Mean Absolute Percentage Error: \", mape)\n",
    "    \n",
    "    try:\n",
    "        medae = metrics.median_absolute_error(x, y)\n",
    "    except:\n",
    "        medae = np.nan\n",
    "    print(\"Median Absolute Error: \", medae)\n",
    "    \n",
    "    try:\n",
    "        r2 = metrics.r2_score(x, y)\n",
    "    except:\n",
    "        r2 = np.nan\n",
    "    print(\"Coefficient of determination: \", r2)\n",
    "    \n",
    "    try:\n",
    "        bias = bias = np.mean(x) - np.mean(y)\n",
    "    except:\n",
    "        bias = np.nan\n",
    "    print(\"Bias: \", bias)\n",
    "    \n",
    "    n = len(x)\n",
    "    print(\"n: \", n)\n",
    "    \n",
    "    return ev, me, mae, mse, rmse, msle, rmsle, mape, medae, r2, bias, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230225\n",
      "20230225_081811\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "date_str = now.strftime(\"%Y%m%d\")\n",
    "date_time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(date_str)\n",
    "print(date_time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_str = \"20230201\"\n",
    "drive = \"D\"\n",
    "data_date = \"20230205\"\n",
    "# define output directory\n",
    "output_dir = r\"{0}:\\cdu\\data\\zonal_stats\\output\\{1}\".format(drive, date_str)\n",
    "export_dir = os.path.join(output_dir, date_time_str)\n",
    "\n",
    "# data dir\n",
    "dir_ = r\"{0}:\\cdu\\data\\zonal_stats\\output\\{1}\\ml_data_si_dir\".format(drive, data_date)\n",
    "\n",
    "# model csv outputs\n",
    "output_ml_csv = r\"{0}:\\cdu\\data\\ml_outputs\".format(drive)\n",
    "\n",
    "index_ = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dir_fn(dir_):\n",
    "    \"\"\" Create a new directory if one does not already exist. \"\"\"\n",
    "    print(\"pathway \", dir_)\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.mkdir(dir_)\n",
    "\n",
    "        \n",
    "def export_csv_fn(list_, dir_, file_name):\n",
    "    \n",
    "    \"\"\" Create and export path from directory and file name and exports csv with no dropping the index column. \"\"\"\n",
    "    \n",
    "    df_final = pd.concat(list_, axis =0)    \n",
    "    output_path = os.path.join(dir_, file_name)\n",
    "    df_final.to_csv(os.path.join(output_path), index=False)\n",
    "    print(\"File output to: \", output_path)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set output file locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\cdu\\data\\zonal_stats\\output\\20230205\\ml_data_si_dir\\s_mean_max_min_med_std__fnm_fms_si_reg.csv\n",
      "D:\\cdu\\data\\zonal_stats\\output\\20230205\\ml_data_si_dir\\r2_bs_mean_max_min_med_std__fnm_fms_si_reg.csv\n",
      "D:\\cdu\\data\\zonal_stats\\output\\20230205\\ml_data_si_dir\\s_mean_max_min_med_std_met_fnm_fms_si_reg.csv\n",
      "D:\\cdu\\data\\zonal_stats\\output\\20230205\\ml_data_si_dir\\r2_bs_mean_max_min_med_std_met_fnm_fms_si_reg.csv\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for f in glob(os.path.join(dir_, \"*reg.csv\")):\n",
    "    print(f)\n",
    "    file_list.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, file_name = os.path.split(file_list[index_])\n",
    "split_list = file_name.split(\".\")\n",
    "data_set = split_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r2_bs_mean_max_min_med_std__fnm_fms_si_reg'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_reg_dir= os.path.join(output_dir, \"ml_reg_dir\")\n",
    "# data_set_dir= os.path.join(ml_reg_dir, \"data_set\")\n",
    "# export_ml_reg_dir = os.path.join(data_set_dir, date_time_str)\n",
    "# # plots_dir = os.path.join(export_ml_rf_reg_dir, data_set)\n",
    "# all_plots_dir = os.path.join(export_ml_reg_dir, \"all_plots\")\n",
    "# # no_tern_plots_dir = os.path.join(plots_dir, \"no_tern\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\n",
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\n",
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\ml_reg_plots_dir\n",
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\ml_reg_plots_dir\\all\n",
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\ml_reg_plots_dir\\no_tern\n"
     ]
    }
   ],
   "source": [
    "ml_reg_dir= os.path.join(output_dir, \"ml_reg_dir\")\n",
    "plots_dir = os.path.join(ml_reg_dir, \"ml_reg_plots_dir\")\n",
    "all_plots_dir = os.path.join(plots_dir, \"all\")\n",
    "no_tern_plots_dir = os.path.join(plots_dir, \"no_tern\")\n",
    "mk_dir_fn(output_dir)\n",
    "mk_dir_fn(ml_reg_dir)\n",
    "mk_dir_fn(plots_dir)\n",
    "mk_dir_fn(all_plots_dir)\n",
    "mk_dir_fn(no_tern_plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mk_dir_fn(output_dir)\n",
    "# mk_dir_fn(ml_rf_reg_dir)\n",
    "# mk_dir_fn(data_set_dir)\n",
    "# mk_dir_fn(export_ml_rf_reg_dir)\n",
    "# # mk_dir_fn(plots_dir)\n",
    "# mk_dir_fn(all_plots_dir)\n",
    "# # mk_dir_fn(no_tern_plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_list[index_], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>site</th>\n",
       "      <th>uid</th>\n",
       "      <th>date</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>nt001</td>\n",
       "      <td>71</td>\n",
       "      <td>20110523</td>\n",
       "      <td>13.78</td>\n",
       "      <td>47.00</td>\n",
       "      <td>25.85</td>\n",
       "      <td>23.87</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-1797183</td>\n",
       "      <td>1797183</td>\n",
       "      <td>803085</td>\n",
       "      <td>-1648544</td>\n",
       "      <td>1653000</td>\n",
       "      <td>2108509</td>\n",
       "      <td>1334000</td>\n",
       "      <td>2374229</td>\n",
       "      <td>3891482</td>\n",
       "      <td>5080174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely01</td>\n",
       "      <td>24</td>\n",
       "      <td>20111025</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-1237840</td>\n",
       "      <td>1237840</td>\n",
       "      <td>145478</td>\n",
       "      <td>-1258856</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1063379</td>\n",
       "      <td>731000</td>\n",
       "      <td>1207466</td>\n",
       "      <td>1791228</td>\n",
       "      <td>1985065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely04</td>\n",
       "      <td>27</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-1493838</td>\n",
       "      <td>1493838</td>\n",
       "      <td>965693</td>\n",
       "      <td>-982236</td>\n",
       "      <td>1174000</td>\n",
       "      <td>951763</td>\n",
       "      <td>677000</td>\n",
       "      <td>1068835</td>\n",
       "      <td>1504110</td>\n",
       "      <td>1636492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely03</td>\n",
       "      <td>26</td>\n",
       "      <td>20111026</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-1481481</td>\n",
       "      <td>1481481</td>\n",
       "      <td>-2046</td>\n",
       "      <td>-1713026</td>\n",
       "      <td>1178000</td>\n",
       "      <td>1068186</td>\n",
       "      <td>738000</td>\n",
       "      <td>1210101</td>\n",
       "      <td>1779171</td>\n",
       "      <td>1970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely02</td>\n",
       "      <td>25</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>13.78</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1563211</td>\n",
       "      <td>1563211</td>\n",
       "      <td>758167</td>\n",
       "      <td>-969300</td>\n",
       "      <td>1320000</td>\n",
       "      <td>1195078</td>\n",
       "      <td>839000</td>\n",
       "      <td>1339257</td>\n",
       "      <td>1908119</td>\n",
       "      <td>2131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20805.22</td>\n",
       "      <td>ntadac0002</td>\n",
       "      <td>79</td>\n",
       "      <td>20160506</td>\n",
       "      <td>15.54</td>\n",
       "      <td>42.39</td>\n",
       "      <td>29.12</td>\n",
       "      <td>29.27</td>\n",
       "      <td>6.84</td>\n",
       "      <td>16.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-652174</td>\n",
       "      <td>652174</td>\n",
       "      <td>4386747</td>\n",
       "      <td>1261845</td>\n",
       "      <td>1828000</td>\n",
       "      <td>3096442</td>\n",
       "      <td>1768000</td>\n",
       "      <td>3422819</td>\n",
       "      <td>6433770</td>\n",
       "      <td>11466633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>30472.45</td>\n",
       "      <td>ntaarp0001</td>\n",
       "      <td>75</td>\n",
       "      <td>20160602</td>\n",
       "      <td>27.61</td>\n",
       "      <td>38.92</td>\n",
       "      <td>34.75</td>\n",
       "      <td>34.33</td>\n",
       "      <td>3.24</td>\n",
       "      <td>17.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1485714</td>\n",
       "      <td>1485714</td>\n",
       "      <td>4017258</td>\n",
       "      <td>1123510</td>\n",
       "      <td>1746000</td>\n",
       "      <td>2727661</td>\n",
       "      <td>1590000</td>\n",
       "      <td>3059261</td>\n",
       "      <td>5686695</td>\n",
       "      <td>9070438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>24414.13</td>\n",
       "      <td>ntaarp0002</td>\n",
       "      <td>76</td>\n",
       "      <td>20160602</td>\n",
       "      <td>9.06</td>\n",
       "      <td>22.31</td>\n",
       "      <td>14.40</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.60</td>\n",
       "      <td>9.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-2065698</td>\n",
       "      <td>2065698</td>\n",
       "      <td>2615783</td>\n",
       "      <td>222222</td>\n",
       "      <td>1810000</td>\n",
       "      <td>2372051</td>\n",
       "      <td>1483000</td>\n",
       "      <td>2650423</td>\n",
       "      <td>4370763</td>\n",
       "      <td>5977733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>17598.35</td>\n",
       "      <td>ntaarp0003</td>\n",
       "      <td>77</td>\n",
       "      <td>20160603</td>\n",
       "      <td>5.26</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.08</td>\n",
       "      <td>12.11</td>\n",
       "      <td>5.79</td>\n",
       "      <td>9.61</td>\n",
       "      <td>...</td>\n",
       "      <td>-1885790</td>\n",
       "      <td>1885790</td>\n",
       "      <td>2958064</td>\n",
       "      <td>395123</td>\n",
       "      <td>1691000</td>\n",
       "      <td>2284112</td>\n",
       "      <td>1407000</td>\n",
       "      <td>2574722</td>\n",
       "      <td>4401001</td>\n",
       "      <td>6037665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9995.51</td>\n",
       "      <td>buff01</td>\n",
       "      <td>31</td>\n",
       "      <td>20210713</td>\n",
       "      <td>19.30</td>\n",
       "      <td>40.08</td>\n",
       "      <td>30.47</td>\n",
       "      <td>32.06</td>\n",
       "      <td>6.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>770925</td>\n",
       "      <td>-770925</td>\n",
       "      <td>5614599</td>\n",
       "      <td>2339640</td>\n",
       "      <td>2185000</td>\n",
       "      <td>3960474</td>\n",
       "      <td>2255000</td>\n",
       "      <td>4179538</td>\n",
       "      <td>7290656</td>\n",
       "      <td>15262347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target        site  uid      date  b1_fpca2_0509_min  b1_fpca2_0509_max  \\\n",
       "0       0.00       nt001   71  20110523              13.78              47.00   \n",
       "1       0.00   barkely01   24  20111025               0.75               2.87   \n",
       "2       0.00   barkely04   27  20111026               2.12               6.42   \n",
       "3       0.00   barkely03   26  20111026               0.75               3.29   \n",
       "4       0.00   barkely02   25  20111026               2.12              13.78   \n",
       "..       ...         ...  ...       ...                ...                ...   \n",
       "162 20805.22  ntadac0002   79  20160506              15.54              42.39   \n",
       "163 30472.45  ntaarp0001   75  20160602              27.61              38.92   \n",
       "164 24414.13  ntaarp0002   76  20160602               9.06              22.31   \n",
       "165 17598.35  ntaarp0003   77  20160603               5.26              25.45   \n",
       "166  9995.51      buff01   31  20210713              19.30              40.08   \n",
       "\n",
       "     b1_fpca2_0509_mean  b1_fpca2_0509_med  b1_fpca2_0509_std  \\\n",
       "0                 25.85              23.87               8.48   \n",
       "1                  1.69               1.49               0.70   \n",
       "2                  4.37               4.47               1.22   \n",
       "3                  1.82               1.64               0.70   \n",
       "4                  4.97               4.73               2.43   \n",
       "..                  ...                ...                ...   \n",
       "162               29.12              29.27               6.84   \n",
       "163               34.75              34.33               3.24   \n",
       "164               14.40              14.21               3.60   \n",
       "165               13.08              12.11               5.79   \n",
       "166               30.47              32.06               6.23   \n",
       "\n",
       "     b1_h99a_01122_min  ...    NDGIm      RIm     NBRm    NDIIm    GDVIm  \\\n",
       "0                 7.27  ... -1797183  1797183   803085 -1648544  1653000   \n",
       "1                 4.38  ... -1237840  1237840   145478 -1258856  1100000   \n",
       "2                 5.93  ... -1493838  1493838   965693  -982236  1174000   \n",
       "3                 3.66  ... -1481481  1481481    -2046 -1713026  1178000   \n",
       "4                 3.13  ... -1563211  1563211   758167  -969300  1320000   \n",
       "..                 ...  ...      ...      ...      ...      ...      ...   \n",
       "162              16.05  ...  -652174   652174  4386747  1261845  1828000   \n",
       "163              17.31  ... -1485714  1485714  4017258  1123510  1746000   \n",
       "164               9.74  ... -2065698  2065698  2615783   222222  1810000   \n",
       "165               9.61  ... -1885790  1885790  2958064   395123  1691000   \n",
       "166              12.00  ...   770925  -770925  5614599  2339640  2185000   \n",
       "\n",
       "      MSAVIm     DVIm    SAVIm    NDVIm      MSRm  \n",
       "0    2108509  1334000  2374229  3891482   5080174  \n",
       "1    1063379   731000  1207466  1791228   1985065  \n",
       "2     951763   677000  1068835  1504110   1636492  \n",
       "3    1068186   738000  1210101  1779171   1970149  \n",
       "4    1195078   839000  1339257  1908119   2131007  \n",
       "..       ...      ...      ...      ...       ...  \n",
       "162  3096442  1768000  3422819  6433770  11466633  \n",
       "163  2727661  1590000  3059261  5686695   9070438  \n",
       "164  2372051  1483000  2650423  4370763   5977733  \n",
       "165  2284112  1407000  2574722  4401001   6037665  \n",
       "166  3960474  2255000  4179538  7290656  15262347  \n",
       "\n",
       "[167 rows x 174 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope:  398.39405526585966\n",
      "intersept:  -1545.2575127961409\n",
      "r2:  0.6680461286822867\n",
      "P_value:  6.104388470146404e-23\n",
      "std error:  34.54675999706051\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_y = 'target'\n",
    "value_x = 'b1_fpca2_0509_max'\n",
    "sns.regplot(x= value_x, y=value_y, data=df)\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(df[value_x], df[value_y])\n",
    "\n",
    "print(\"slope: \", slope)\n",
    "print(\"intersept: \", intercept)\n",
    "print(\"r2: \", r_value)\n",
    "print(\"P_value: \", p_value)\n",
    "print(\"std error: \", std_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 174)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml = df.copy(deep=True)\n",
    "df_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_cor = df_ml.corr().unstack().sort_values().drop_duplicates()\n",
    "df_corr_ = pd.DataFrame(df_ml_cor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_.reset_index(drop= False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_.columns =[\"feature1\", \"feature2\", \"corrolation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>corrolation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14878</th>\n",
       "      <td>target</td>\n",
       "      <td>target</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13476</th>\n",
       "      <td>target</td>\n",
       "      <td>b1_dja_0305_max</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13414</th>\n",
       "      <td>target</td>\n",
       "      <td>b1_dja_0305_mean</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13407</th>\n",
       "      <td>target</td>\n",
       "      <td>b1_fpca2_0509_mean</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13394</th>\n",
       "      <td>target</td>\n",
       "      <td>b1_dja_0305_med</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>target</td>\n",
       "      <td>b3_dbi_0608_mean</td>\n",
       "      <td>-0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>target</td>\n",
       "      <td>b3_dbi_0608_med</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>target</td>\n",
       "      <td>b6_dbi_0608_med</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>target</td>\n",
       "      <td>b5_dbi_0608_mean</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>target</td>\n",
       "      <td>b6_dbi_0608_min</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1            feature2  corrolation\n",
       "14878   target              target         1.00\n",
       "13476   target     b1_dja_0305_max         0.70\n",
       "13414   target    b1_dja_0305_mean         0.69\n",
       "13407   target  b1_fpca2_0509_mean         0.69\n",
       "13394   target     b1_dja_0305_med         0.69\n",
       "...        ...                 ...          ...\n",
       "1035    target    b3_dbi_0608_mean        -0.61\n",
       "1008    target     b3_dbi_0608_med        -0.62\n",
       "964     target     b6_dbi_0608_med        -0.62\n",
       "923     target    b5_dbi_0608_mean        -0.63\n",
       "879     target     b6_dbi_0608_min        -0.64\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_corr = df_corr_[df_corr_[\"feature1\"]==\"target\"]\n",
    "tar_corr.sort_values(by=[\"corrolation\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\target_corrolation.csv\n"
     ]
    }
   ],
   "source": [
    "out_file = os.path.join(ml_reg_dir, \"target_corrolation.csv\")\n",
    "tar_corr.to_csv(out_file, index=False)\n",
    "print(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>site</th>\n",
       "      <th>uid</th>\n",
       "      <th>date</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>nt001</td>\n",
       "      <td>71</td>\n",
       "      <td>20110523</td>\n",
       "      <td>13.78</td>\n",
       "      <td>47.00</td>\n",
       "      <td>25.85</td>\n",
       "      <td>23.87</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-1797183</td>\n",
       "      <td>1797183</td>\n",
       "      <td>803085</td>\n",
       "      <td>-1648544</td>\n",
       "      <td>1653000</td>\n",
       "      <td>2108509</td>\n",
       "      <td>1334000</td>\n",
       "      <td>2374229</td>\n",
       "      <td>3891482</td>\n",
       "      <td>5080174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely01</td>\n",
       "      <td>24</td>\n",
       "      <td>20111025</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-1237840</td>\n",
       "      <td>1237840</td>\n",
       "      <td>145478</td>\n",
       "      <td>-1258856</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1063379</td>\n",
       "      <td>731000</td>\n",
       "      <td>1207466</td>\n",
       "      <td>1791228</td>\n",
       "      <td>1985065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely04</td>\n",
       "      <td>27</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-1493838</td>\n",
       "      <td>1493838</td>\n",
       "      <td>965693</td>\n",
       "      <td>-982236</td>\n",
       "      <td>1174000</td>\n",
       "      <td>951763</td>\n",
       "      <td>677000</td>\n",
       "      <td>1068835</td>\n",
       "      <td>1504110</td>\n",
       "      <td>1636492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely03</td>\n",
       "      <td>26</td>\n",
       "      <td>20111026</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-1481481</td>\n",
       "      <td>1481481</td>\n",
       "      <td>-2046</td>\n",
       "      <td>-1713026</td>\n",
       "      <td>1178000</td>\n",
       "      <td>1068186</td>\n",
       "      <td>738000</td>\n",
       "      <td>1210101</td>\n",
       "      <td>1779171</td>\n",
       "      <td>1970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely02</td>\n",
       "      <td>25</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>13.78</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1563211</td>\n",
       "      <td>1563211</td>\n",
       "      <td>758167</td>\n",
       "      <td>-969300</td>\n",
       "      <td>1320000</td>\n",
       "      <td>1195078</td>\n",
       "      <td>839000</td>\n",
       "      <td>1339257</td>\n",
       "      <td>1908119</td>\n",
       "      <td>2131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20805.22</td>\n",
       "      <td>ntadac0002</td>\n",
       "      <td>79</td>\n",
       "      <td>20160506</td>\n",
       "      <td>15.54</td>\n",
       "      <td>42.39</td>\n",
       "      <td>29.12</td>\n",
       "      <td>29.27</td>\n",
       "      <td>6.84</td>\n",
       "      <td>16.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-652174</td>\n",
       "      <td>652174</td>\n",
       "      <td>4386747</td>\n",
       "      <td>1261845</td>\n",
       "      <td>1828000</td>\n",
       "      <td>3096442</td>\n",
       "      <td>1768000</td>\n",
       "      <td>3422819</td>\n",
       "      <td>6433770</td>\n",
       "      <td>11466633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>30472.45</td>\n",
       "      <td>ntaarp0001</td>\n",
       "      <td>75</td>\n",
       "      <td>20160602</td>\n",
       "      <td>27.61</td>\n",
       "      <td>38.92</td>\n",
       "      <td>34.75</td>\n",
       "      <td>34.33</td>\n",
       "      <td>3.24</td>\n",
       "      <td>17.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1485714</td>\n",
       "      <td>1485714</td>\n",
       "      <td>4017258</td>\n",
       "      <td>1123510</td>\n",
       "      <td>1746000</td>\n",
       "      <td>2727661</td>\n",
       "      <td>1590000</td>\n",
       "      <td>3059261</td>\n",
       "      <td>5686695</td>\n",
       "      <td>9070438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>24414.13</td>\n",
       "      <td>ntaarp0002</td>\n",
       "      <td>76</td>\n",
       "      <td>20160602</td>\n",
       "      <td>9.06</td>\n",
       "      <td>22.31</td>\n",
       "      <td>14.40</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.60</td>\n",
       "      <td>9.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-2065698</td>\n",
       "      <td>2065698</td>\n",
       "      <td>2615783</td>\n",
       "      <td>222222</td>\n",
       "      <td>1810000</td>\n",
       "      <td>2372051</td>\n",
       "      <td>1483000</td>\n",
       "      <td>2650423</td>\n",
       "      <td>4370763</td>\n",
       "      <td>5977733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>17598.35</td>\n",
       "      <td>ntaarp0003</td>\n",
       "      <td>77</td>\n",
       "      <td>20160603</td>\n",
       "      <td>5.26</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.08</td>\n",
       "      <td>12.11</td>\n",
       "      <td>5.79</td>\n",
       "      <td>9.61</td>\n",
       "      <td>...</td>\n",
       "      <td>-1885790</td>\n",
       "      <td>1885790</td>\n",
       "      <td>2958064</td>\n",
       "      <td>395123</td>\n",
       "      <td>1691000</td>\n",
       "      <td>2284112</td>\n",
       "      <td>1407000</td>\n",
       "      <td>2574722</td>\n",
       "      <td>4401001</td>\n",
       "      <td>6037665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9995.51</td>\n",
       "      <td>buff01</td>\n",
       "      <td>31</td>\n",
       "      <td>20210713</td>\n",
       "      <td>19.30</td>\n",
       "      <td>40.08</td>\n",
       "      <td>30.47</td>\n",
       "      <td>32.06</td>\n",
       "      <td>6.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>770925</td>\n",
       "      <td>-770925</td>\n",
       "      <td>5614599</td>\n",
       "      <td>2339640</td>\n",
       "      <td>2185000</td>\n",
       "      <td>3960474</td>\n",
       "      <td>2255000</td>\n",
       "      <td>4179538</td>\n",
       "      <td>7290656</td>\n",
       "      <td>15262347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target        site  uid      date  b1_fpca2_0509_min  b1_fpca2_0509_max  \\\n",
       "0       0.00       nt001   71  20110523              13.78              47.00   \n",
       "1       0.00   barkely01   24  20111025               0.75               2.87   \n",
       "2       0.00   barkely04   27  20111026               2.12               6.42   \n",
       "3       0.00   barkely03   26  20111026               0.75               3.29   \n",
       "4       0.00   barkely02   25  20111026               2.12              13.78   \n",
       "..       ...         ...  ...       ...                ...                ...   \n",
       "162 20805.22  ntadac0002   79  20160506              15.54              42.39   \n",
       "163 30472.45  ntaarp0001   75  20160602              27.61              38.92   \n",
       "164 24414.13  ntaarp0002   76  20160602               9.06              22.31   \n",
       "165 17598.35  ntaarp0003   77  20160603               5.26              25.45   \n",
       "166  9995.51      buff01   31  20210713              19.30              40.08   \n",
       "\n",
       "     b1_fpca2_0509_mean  b1_fpca2_0509_med  b1_fpca2_0509_std  \\\n",
       "0                 25.85              23.87               8.48   \n",
       "1                  1.69               1.49               0.70   \n",
       "2                  4.37               4.47               1.22   \n",
       "3                  1.82               1.64               0.70   \n",
       "4                  4.97               4.73               2.43   \n",
       "..                  ...                ...                ...   \n",
       "162               29.12              29.27               6.84   \n",
       "163               34.75              34.33               3.24   \n",
       "164               14.40              14.21               3.60   \n",
       "165               13.08              12.11               5.79   \n",
       "166               30.47              32.06               6.23   \n",
       "\n",
       "     b1_h99a_01122_min  ...    NDGIm      RIm     NBRm    NDIIm    GDVIm  \\\n",
       "0                 7.27  ... -1797183  1797183   803085 -1648544  1653000   \n",
       "1                 4.38  ... -1237840  1237840   145478 -1258856  1100000   \n",
       "2                 5.93  ... -1493838  1493838   965693  -982236  1174000   \n",
       "3                 3.66  ... -1481481  1481481    -2046 -1713026  1178000   \n",
       "4                 3.13  ... -1563211  1563211   758167  -969300  1320000   \n",
       "..                 ...  ...      ...      ...      ...      ...      ...   \n",
       "162              16.05  ...  -652174   652174  4386747  1261845  1828000   \n",
       "163              17.31  ... -1485714  1485714  4017258  1123510  1746000   \n",
       "164               9.74  ... -2065698  2065698  2615783   222222  1810000   \n",
       "165               9.61  ... -1885790  1885790  2958064   395123  1691000   \n",
       "166              12.00  ...   770925  -770925  5614599  2339640  2185000   \n",
       "\n",
       "      MSAVIm     DVIm    SAVIm    NDVIm      MSRm  \n",
       "0    2108509  1334000  2374229  3891482   5080174  \n",
       "1    1063379   731000  1207466  1791228   1985065  \n",
       "2     951763   677000  1068835  1504110   1636492  \n",
       "3    1068186   738000  1210101  1779171   1970149  \n",
       "4    1195078   839000  1339257  1908119   2131007  \n",
       "..       ...      ...      ...      ...       ...  \n",
       "162  3096442  1768000  3422819  6433770  11466633  \n",
       "163  2727661  1590000  3059261  5686695   9070438  \n",
       "164  2372051  1483000  2650423  4370763   5977733  \n",
       "165  2284112  1407000  2574722  4401001   6037665  \n",
       "166  3960474  2255000  4179538  7290656  15262347  \n",
       "\n",
       "[167 rows x 174 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which data set to run the models from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>site</th>\n",
       "      <th>uid</th>\n",
       "      <th>date</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>nt001</td>\n",
       "      <td>71</td>\n",
       "      <td>20110523</td>\n",
       "      <td>13.78</td>\n",
       "      <td>47.00</td>\n",
       "      <td>25.85</td>\n",
       "      <td>23.87</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-1797183</td>\n",
       "      <td>1797183</td>\n",
       "      <td>803085</td>\n",
       "      <td>-1648544</td>\n",
       "      <td>1653000</td>\n",
       "      <td>2108509</td>\n",
       "      <td>1334000</td>\n",
       "      <td>2374229</td>\n",
       "      <td>3891482</td>\n",
       "      <td>5080174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely01</td>\n",
       "      <td>24</td>\n",
       "      <td>20111025</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-1237840</td>\n",
       "      <td>1237840</td>\n",
       "      <td>145478</td>\n",
       "      <td>-1258856</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1063379</td>\n",
       "      <td>731000</td>\n",
       "      <td>1207466</td>\n",
       "      <td>1791228</td>\n",
       "      <td>1985065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely04</td>\n",
       "      <td>27</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-1493838</td>\n",
       "      <td>1493838</td>\n",
       "      <td>965693</td>\n",
       "      <td>-982236</td>\n",
       "      <td>1174000</td>\n",
       "      <td>951763</td>\n",
       "      <td>677000</td>\n",
       "      <td>1068835</td>\n",
       "      <td>1504110</td>\n",
       "      <td>1636492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely03</td>\n",
       "      <td>26</td>\n",
       "      <td>20111026</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-1481481</td>\n",
       "      <td>1481481</td>\n",
       "      <td>-2046</td>\n",
       "      <td>-1713026</td>\n",
       "      <td>1178000</td>\n",
       "      <td>1068186</td>\n",
       "      <td>738000</td>\n",
       "      <td>1210101</td>\n",
       "      <td>1779171</td>\n",
       "      <td>1970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>barkely02</td>\n",
       "      <td>25</td>\n",
       "      <td>20111026</td>\n",
       "      <td>2.12</td>\n",
       "      <td>13.78</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1563211</td>\n",
       "      <td>1563211</td>\n",
       "      <td>758167</td>\n",
       "      <td>-969300</td>\n",
       "      <td>1320000</td>\n",
       "      <td>1195078</td>\n",
       "      <td>839000</td>\n",
       "      <td>1339257</td>\n",
       "      <td>1908119</td>\n",
       "      <td>2131007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target       site  uid      date  b1_fpca2_0509_min  b1_fpca2_0509_max  \\\n",
       "0    0.00      nt001   71  20110523              13.78              47.00   \n",
       "1    0.00  barkely01   24  20111025               0.75               2.87   \n",
       "2    0.00  barkely04   27  20111026               2.12               6.42   \n",
       "3    0.00  barkely03   26  20111026               0.75               3.29   \n",
       "4    0.00  barkely02   25  20111026               2.12              13.78   \n",
       "\n",
       "   b1_fpca2_0509_mean  b1_fpca2_0509_med  b1_fpca2_0509_std  \\\n",
       "0               25.85              23.87               8.48   \n",
       "1                1.69               1.49               0.70   \n",
       "2                4.37               4.47               1.22   \n",
       "3                1.82               1.64               0.70   \n",
       "4                4.97               4.73               2.43   \n",
       "\n",
       "   b1_h99a_01122_min  ...    NDGIm      RIm    NBRm    NDIIm    GDVIm  \\\n",
       "0               7.27  ... -1797183  1797183  803085 -1648544  1653000   \n",
       "1               4.38  ... -1237840  1237840  145478 -1258856  1100000   \n",
       "2               5.93  ... -1493838  1493838  965693  -982236  1174000   \n",
       "3               3.66  ... -1481481  1481481   -2046 -1713026  1178000   \n",
       "4               3.13  ... -1563211  1563211  758167  -969300  1320000   \n",
       "\n",
       "    MSAVIm     DVIm    SAVIm    NDVIm     MSRm  \n",
       "0  2108509  1334000  2374229  3891482  5080174  \n",
       "1  1063379   731000  1207466  1791228  1985065  \n",
       "2   951763   677000  1068835  1504110  1636492  \n",
       "3  1068186   738000  1210101  1779171  1970149  \n",
       "4  1195078   839000  1339257  1908119  2131007  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some of the unwanted values\n",
    "df_ml.drop(['site', 'uid', 'date'], axis=1, inplace=True) # 'date',\n",
    "#df_ml.drop(['fpca2_imdate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>b1_h99a_01122_max</th>\n",
       "      <th>b1_h99a_01122_mean</th>\n",
       "      <th>b1_h99a_01122_med</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>13.78</td>\n",
       "      <td>47.00</td>\n",
       "      <td>25.85</td>\n",
       "      <td>23.87</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.27</td>\n",
       "      <td>11.78</td>\n",
       "      <td>9.24</td>\n",
       "      <td>8.86</td>\n",
       "      <td>...</td>\n",
       "      <td>-1797183</td>\n",
       "      <td>1797183</td>\n",
       "      <td>803085</td>\n",
       "      <td>-1648544</td>\n",
       "      <td>1653000</td>\n",
       "      <td>2108509</td>\n",
       "      <td>1334000</td>\n",
       "      <td>2374229</td>\n",
       "      <td>3891482</td>\n",
       "      <td>5080174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>8.97</td>\n",
       "      <td>6.83</td>\n",
       "      <td>7.18</td>\n",
       "      <td>...</td>\n",
       "      <td>-1237840</td>\n",
       "      <td>1237840</td>\n",
       "      <td>145478</td>\n",
       "      <td>-1258856</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1063379</td>\n",
       "      <td>731000</td>\n",
       "      <td>1207466</td>\n",
       "      <td>1791228</td>\n",
       "      <td>1985065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.93</td>\n",
       "      <td>8.85</td>\n",
       "      <td>7.68</td>\n",
       "      <td>7.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-1493838</td>\n",
       "      <td>1493838</td>\n",
       "      <td>965693</td>\n",
       "      <td>-982236</td>\n",
       "      <td>1174000</td>\n",
       "      <td>951763</td>\n",
       "      <td>677000</td>\n",
       "      <td>1068835</td>\n",
       "      <td>1504110</td>\n",
       "      <td>1636492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.66</td>\n",
       "      <td>7.09</td>\n",
       "      <td>4.98</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-1481481</td>\n",
       "      <td>1481481</td>\n",
       "      <td>-2046</td>\n",
       "      <td>-1713026</td>\n",
       "      <td>1178000</td>\n",
       "      <td>1068186</td>\n",
       "      <td>738000</td>\n",
       "      <td>1210101</td>\n",
       "      <td>1779171</td>\n",
       "      <td>1970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>13.78</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.13</td>\n",
       "      <td>5.82</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.43</td>\n",
       "      <td>...</td>\n",
       "      <td>-1563211</td>\n",
       "      <td>1563211</td>\n",
       "      <td>758167</td>\n",
       "      <td>-969300</td>\n",
       "      <td>1320000</td>\n",
       "      <td>1195078</td>\n",
       "      <td>839000</td>\n",
       "      <td>1339257</td>\n",
       "      <td>1908119</td>\n",
       "      <td>2131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20805.22</td>\n",
       "      <td>15.54</td>\n",
       "      <td>42.39</td>\n",
       "      <td>29.12</td>\n",
       "      <td>29.27</td>\n",
       "      <td>6.84</td>\n",
       "      <td>16.05</td>\n",
       "      <td>20.42</td>\n",
       "      <td>18.75</td>\n",
       "      <td>18.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-652174</td>\n",
       "      <td>652174</td>\n",
       "      <td>4386747</td>\n",
       "      <td>1261845</td>\n",
       "      <td>1828000</td>\n",
       "      <td>3096442</td>\n",
       "      <td>1768000</td>\n",
       "      <td>3422819</td>\n",
       "      <td>6433770</td>\n",
       "      <td>11466633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>30472.45</td>\n",
       "      <td>27.61</td>\n",
       "      <td>38.92</td>\n",
       "      <td>34.75</td>\n",
       "      <td>34.33</td>\n",
       "      <td>3.24</td>\n",
       "      <td>17.31</td>\n",
       "      <td>21.59</td>\n",
       "      <td>19.37</td>\n",
       "      <td>19.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-1485714</td>\n",
       "      <td>1485714</td>\n",
       "      <td>4017258</td>\n",
       "      <td>1123510</td>\n",
       "      <td>1746000</td>\n",
       "      <td>2727661</td>\n",
       "      <td>1590000</td>\n",
       "      <td>3059261</td>\n",
       "      <td>5686695</td>\n",
       "      <td>9070438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>24414.13</td>\n",
       "      <td>9.06</td>\n",
       "      <td>22.31</td>\n",
       "      <td>14.40</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.60</td>\n",
       "      <td>9.74</td>\n",
       "      <td>16.81</td>\n",
       "      <td>13.63</td>\n",
       "      <td>13.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-2065698</td>\n",
       "      <td>2065698</td>\n",
       "      <td>2615783</td>\n",
       "      <td>222222</td>\n",
       "      <td>1810000</td>\n",
       "      <td>2372051</td>\n",
       "      <td>1483000</td>\n",
       "      <td>2650423</td>\n",
       "      <td>4370763</td>\n",
       "      <td>5977733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>17598.35</td>\n",
       "      <td>5.26</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.08</td>\n",
       "      <td>12.11</td>\n",
       "      <td>5.79</td>\n",
       "      <td>9.61</td>\n",
       "      <td>20.05</td>\n",
       "      <td>13.89</td>\n",
       "      <td>13.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-1885790</td>\n",
       "      <td>1885790</td>\n",
       "      <td>2958064</td>\n",
       "      <td>395123</td>\n",
       "      <td>1691000</td>\n",
       "      <td>2284112</td>\n",
       "      <td>1407000</td>\n",
       "      <td>2574722</td>\n",
       "      <td>4401001</td>\n",
       "      <td>6037665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9995.51</td>\n",
       "      <td>19.30</td>\n",
       "      <td>40.08</td>\n",
       "      <td>30.47</td>\n",
       "      <td>32.06</td>\n",
       "      <td>6.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>18.40</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.50</td>\n",
       "      <td>...</td>\n",
       "      <td>770925</td>\n",
       "      <td>-770925</td>\n",
       "      <td>5614599</td>\n",
       "      <td>2339640</td>\n",
       "      <td>2185000</td>\n",
       "      <td>3960474</td>\n",
       "      <td>2255000</td>\n",
       "      <td>4179538</td>\n",
       "      <td>7290656</td>\n",
       "      <td>15262347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  b1_fpca2_0509_min  b1_fpca2_0509_max  b1_fpca2_0509_mean  \\\n",
       "0       0.00              13.78              47.00               25.85   \n",
       "1       0.00               0.75               2.87                1.69   \n",
       "2       0.00               2.12               6.42                4.37   \n",
       "3       0.00               0.75               3.29                1.82   \n",
       "4       0.00               2.12              13.78                4.97   \n",
       "..       ...                ...                ...                 ...   \n",
       "162 20805.22              15.54              42.39               29.12   \n",
       "163 30472.45              27.61              38.92               34.75   \n",
       "164 24414.13               9.06              22.31               14.40   \n",
       "165 17598.35               5.26              25.45               13.08   \n",
       "166  9995.51              19.30              40.08               30.47   \n",
       "\n",
       "     b1_fpca2_0509_med  b1_fpca2_0509_std  b1_h99a_01122_min  \\\n",
       "0                23.87               8.48               7.27   \n",
       "1                 1.49               0.70               4.38   \n",
       "2                 4.47               1.22               5.93   \n",
       "3                 1.64               0.70               3.66   \n",
       "4                 4.73               2.43               3.13   \n",
       "..                 ...                ...                ...   \n",
       "162              29.27               6.84              16.05   \n",
       "163              34.33               3.24              17.31   \n",
       "164              14.21               3.60               9.74   \n",
       "165              12.11               5.79               9.61   \n",
       "166              32.06               6.23              12.00   \n",
       "\n",
       "     b1_h99a_01122_max  b1_h99a_01122_mean  b1_h99a_01122_med  ...    NDGIm  \\\n",
       "0                11.78                9.24               8.86  ... -1797183   \n",
       "1                 8.97                6.83               7.18  ... -1237840   \n",
       "2                 8.85                7.68               7.68  ... -1493838   \n",
       "3                 7.09                4.98               4.85  ... -1481481   \n",
       "4                 5.82                4.55               4.43  ... -1563211   \n",
       "..                 ...                 ...                ...  ...      ...   \n",
       "162              20.42               18.75              18.77  ...  -652174   \n",
       "163              21.59               19.37              19.22  ... -1485714   \n",
       "164              16.81               13.63              13.79  ... -2065698   \n",
       "165              20.05               13.89              13.26  ... -1885790   \n",
       "166              18.40               16.60              17.50  ...   770925   \n",
       "\n",
       "         RIm     NBRm    NDIIm    GDVIm   MSAVIm     DVIm    SAVIm    NDVIm  \\\n",
       "0    1797183   803085 -1648544  1653000  2108509  1334000  2374229  3891482   \n",
       "1    1237840   145478 -1258856  1100000  1063379   731000  1207466  1791228   \n",
       "2    1493838   965693  -982236  1174000   951763   677000  1068835  1504110   \n",
       "3    1481481    -2046 -1713026  1178000  1068186   738000  1210101  1779171   \n",
       "4    1563211   758167  -969300  1320000  1195078   839000  1339257  1908119   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "162   652174  4386747  1261845  1828000  3096442  1768000  3422819  6433770   \n",
       "163  1485714  4017258  1123510  1746000  2727661  1590000  3059261  5686695   \n",
       "164  2065698  2615783   222222  1810000  2372051  1483000  2650423  4370763   \n",
       "165  1885790  2958064   395123  1691000  2284112  1407000  2574722  4401001   \n",
       "166  -770925  5614599  2339640  2185000  3960474  2255000  4179538  7290656   \n",
       "\n",
       "         MSRm  \n",
       "0     5080174  \n",
       "1     1985065  \n",
       "2     1636492  \n",
       "3     1970149  \n",
       "4     2131007  \n",
       "..        ...  \n",
       "162  11466633  \n",
       "163   9070438  \n",
       "164   5977733  \n",
       "165   6037665  \n",
       "166  15262347  \n",
       "\n",
       "[167 rows x 171 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qty of 0 values dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pathway  D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\no0_values\n",
      "D:\\cdu\\data\\zonal_stats\\output\\20230225\\ml_reg_dir\\no0_values\n",
      "(110, 171)\n"
     ]
    }
   ],
   "source": [
    "model_data_ = df_ml\n",
    "model_data_name = \"all_values\"\n",
    "\n",
    "\n",
    "model_data_ = df_ml[df_ml['target']>0.0]\n",
    "model_data_name = \"no0_values\"\n",
    "\n",
    "\n",
    "# ## Select a randon number of 0 values\n",
    "# n = 3\n",
    "# agb_0 = df_ml[df_ml['target']==0.0].sample(n)\n",
    "# model_data = pd.concat([df_ml[df_ml['target']>0.0], agb_0])\n",
    "# model_data_name = f\"s{n}_0_values\"\n",
    "\n",
    "\n",
    "model_outputs = os.path.join(ml_reg_dir, f\"{model_data_name}\")\n",
    "mk_dir_fn(model_outputs)\n",
    "\n",
    "print(model_outputs)\n",
    "print(model_data_.shape)\n",
    "\n",
    "# define model output name for all outputs\n",
    "#str_model = f\"{mdl}_{model_data_name}_n_est_{n_est}_m_feat_{m_feat}_m_depth_{maxd}_{date_time_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_data_ = df_ml\n",
    "# model_data_name = \"all_values\"\n",
    "\n",
    "# # ## Filter out all taregt == 0 values\n",
    "# model_data = df_ml[df_ml['target']>0]\n",
    "# model_data_name = \"no0_values\"\n",
    "\n",
    "# # # ## Filter out all taregt == 0 values\n",
    "# # model_data = df_ml[(df_ml['target']>0) & (df_ml['target']!=9)]\n",
    "# # model_data_name = \"no0or9_values\"\n",
    "\n",
    "\n",
    "# # ## Select a randon number of 0 values\n",
    "# # n = 3\n",
    "# # agb_0 = df_ml[df_ml['target']==0.0].sample(n)\n",
    "# # model_data = pd.concat([df_ml[df_ml['target']>0.0], agb_0])\n",
    "# # model_data_name = f\"s{n}_0_values\"\n",
    "\n",
    "\n",
    "# model_outputs = os.path.join(ml_reg_dir, f\"{model_data_name}\")\n",
    "# mk_dir_fn(model_outputs)\n",
    "\n",
    "# print(model_outputs)\n",
    "# print(model_data_.shape)\n",
    "\n",
    "# # print(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 171)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\biomass_zonal\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model_data_.dropna(inplace=True)\n",
    "print(model_data_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the dataset to run the models from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define if you are using all variabes or selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>b1_h99a_01122_max</th>\n",
       "      <th>b1_h99a_01122_mean</th>\n",
       "      <th>b1_h99a_01122_med</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1894.20</td>\n",
       "      <td>1.79</td>\n",
       "      <td>5.83</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.80</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-1452928</td>\n",
       "      <td>1452928</td>\n",
       "      <td>400174</td>\n",
       "      <td>-1264158</td>\n",
       "      <td>1238000</td>\n",
       "      <td>1250407</td>\n",
       "      <td>846000</td>\n",
       "      <td>1420098</td>\n",
       "      <td>2149390</td>\n",
       "      <td>2440148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1536.15</td>\n",
       "      <td>6.80</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.44</td>\n",
       "      <td>12.23</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-2148760</td>\n",
       "      <td>2148760</td>\n",
       "      <td>-267789</td>\n",
       "      <td>-1724138</td>\n",
       "      <td>1148000</td>\n",
       "      <td>1156438</td>\n",
       "      <td>732000</td>\n",
       "      <td>1358238</td>\n",
       "      <td>2373541</td>\n",
       "      <td>2737539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1164.24</td>\n",
       "      <td>6.80</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.44</td>\n",
       "      <td>12.23</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-2327297</td>\n",
       "      <td>2327297</td>\n",
       "      <td>176162</td>\n",
       "      <td>-1318945</td>\n",
       "      <td>1571000</td>\n",
       "      <td>1235708</td>\n",
       "      <td>877000</td>\n",
       "      <td>1377054</td>\n",
       "      <td>1926203</td>\n",
       "      <td>2153802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4736.01</td>\n",
       "      <td>6.80</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.44</td>\n",
       "      <td>12.23</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-2063673</td>\n",
       "      <td>2063673</td>\n",
       "      <td>799162</td>\n",
       "      <td>-701031</td>\n",
       "      <td>1106000</td>\n",
       "      <td>1197360</td>\n",
       "      <td>743000</td>\n",
       "      <td>1417038</td>\n",
       "      <td>2593368</td>\n",
       "      <td>3039489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1549.09</td>\n",
       "      <td>6.80</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.44</td>\n",
       "      <td>12.23</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-1639267</td>\n",
       "      <td>1639267</td>\n",
       "      <td>1618314</td>\n",
       "      <td>-88940</td>\n",
       "      <td>1673000</td>\n",
       "      <td>2020530</td>\n",
       "      <td>1324000</td>\n",
       "      <td>2256305</td>\n",
       "      <td>3482378</td>\n",
       "      <td>4382641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>16375.18</td>\n",
       "      <td>19.30</td>\n",
       "      <td>27.61</td>\n",
       "      <td>23.58</td>\n",
       "      <td>23.34</td>\n",
       "      <td>2.53</td>\n",
       "      <td>15.64</td>\n",
       "      <td>19.64</td>\n",
       "      <td>18.24</td>\n",
       "      <td>18.35</td>\n",
       "      <td>...</td>\n",
       "      <td>-1222031</td>\n",
       "      <td>1222031</td>\n",
       "      <td>3647779</td>\n",
       "      <td>881563</td>\n",
       "      <td>1718000</td>\n",
       "      <td>2675640</td>\n",
       "      <td>1576000</td>\n",
       "      <td>3000000</td>\n",
       "      <td>5472222</td>\n",
       "      <td>8485610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20805.22</td>\n",
       "      <td>15.54</td>\n",
       "      <td>42.39</td>\n",
       "      <td>29.12</td>\n",
       "      <td>29.27</td>\n",
       "      <td>6.84</td>\n",
       "      <td>16.05</td>\n",
       "      <td>20.42</td>\n",
       "      <td>18.75</td>\n",
       "      <td>18.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-652174</td>\n",
       "      <td>652174</td>\n",
       "      <td>4386747</td>\n",
       "      <td>1261845</td>\n",
       "      <td>1828000</td>\n",
       "      <td>3096442</td>\n",
       "      <td>1768000</td>\n",
       "      <td>3422819</td>\n",
       "      <td>6433770</td>\n",
       "      <td>11466633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>30472.45</td>\n",
       "      <td>27.61</td>\n",
       "      <td>38.92</td>\n",
       "      <td>34.75</td>\n",
       "      <td>34.33</td>\n",
       "      <td>3.24</td>\n",
       "      <td>17.31</td>\n",
       "      <td>21.59</td>\n",
       "      <td>19.37</td>\n",
       "      <td>19.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-1485714</td>\n",
       "      <td>1485714</td>\n",
       "      <td>4017258</td>\n",
       "      <td>1123510</td>\n",
       "      <td>1746000</td>\n",
       "      <td>2727661</td>\n",
       "      <td>1590000</td>\n",
       "      <td>3059261</td>\n",
       "      <td>5686695</td>\n",
       "      <td>9070438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>17598.35</td>\n",
       "      <td>5.26</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.08</td>\n",
       "      <td>12.11</td>\n",
       "      <td>5.79</td>\n",
       "      <td>9.61</td>\n",
       "      <td>20.05</td>\n",
       "      <td>13.89</td>\n",
       "      <td>13.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-1885790</td>\n",
       "      <td>1885790</td>\n",
       "      <td>2958064</td>\n",
       "      <td>395123</td>\n",
       "      <td>1691000</td>\n",
       "      <td>2284112</td>\n",
       "      <td>1407000</td>\n",
       "      <td>2574722</td>\n",
       "      <td>4401001</td>\n",
       "      <td>6037665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9995.51</td>\n",
       "      <td>19.30</td>\n",
       "      <td>40.08</td>\n",
       "      <td>30.47</td>\n",
       "      <td>32.06</td>\n",
       "      <td>6.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>18.40</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.50</td>\n",
       "      <td>...</td>\n",
       "      <td>770925</td>\n",
       "      <td>-770925</td>\n",
       "      <td>5614599</td>\n",
       "      <td>2339640</td>\n",
       "      <td>2185000</td>\n",
       "      <td>3960474</td>\n",
       "      <td>2255000</td>\n",
       "      <td>4179538</td>\n",
       "      <td>7290656</td>\n",
       "      <td>15262347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  b1_fpca2_0509_min  b1_fpca2_0509_max  b1_fpca2_0509_mean  \\\n",
       "5    1894.20               1.79               5.83                2.86   \n",
       "8    1536.15               6.80              17.20               11.25   \n",
       "9    1164.24               6.80              17.20               11.25   \n",
       "10   4736.01               6.80              17.20               11.25   \n",
       "12   1549.09               6.80              17.20               11.25   \n",
       "..       ...                ...                ...                 ...   \n",
       "161 16375.18              19.30              27.61               23.58   \n",
       "162 20805.22              15.54              42.39               29.12   \n",
       "163 30472.45              27.61              38.92               34.75   \n",
       "165 17598.35               5.26              25.45               13.08   \n",
       "166  9995.51              19.30              40.08               30.47   \n",
       "\n",
       "     b1_fpca2_0509_med  b1_fpca2_0509_std  b1_h99a_01122_min  \\\n",
       "5                 2.48               1.22               3.80   \n",
       "8                11.00               2.80               7.44   \n",
       "9                11.00               2.80               7.44   \n",
       "10               11.00               2.80               7.44   \n",
       "12               11.00               2.80               7.44   \n",
       "..                 ...                ...                ...   \n",
       "161              23.34               2.53              15.64   \n",
       "162              29.27               6.84              16.05   \n",
       "163              34.33               3.24              17.31   \n",
       "165              12.11               5.79               9.61   \n",
       "166              32.06               6.23              12.00   \n",
       "\n",
       "     b1_h99a_01122_max  b1_h99a_01122_mean  b1_h99a_01122_med  ...    NDGIm  \\\n",
       "5                 5.20                4.29               4.27  ... -1452928   \n",
       "8                12.23                9.75               9.73  ... -2148760   \n",
       "9                12.23                9.75               9.73  ... -2327297   \n",
       "10               12.23                9.75               9.73  ... -2063673   \n",
       "12               12.23                9.75               9.73  ... -1639267   \n",
       "..                 ...                 ...                ...  ...      ...   \n",
       "161              19.64               18.24              18.35  ... -1222031   \n",
       "162              20.42               18.75              18.77  ...  -652174   \n",
       "163              21.59               19.37              19.22  ... -1485714   \n",
       "165              20.05               13.89              13.26  ... -1885790   \n",
       "166              18.40               16.60              17.50  ...   770925   \n",
       "\n",
       "         RIm     NBRm    NDIIm    GDVIm   MSAVIm     DVIm    SAVIm    NDVIm  \\\n",
       "5    1452928   400174 -1264158  1238000  1250407   846000  1420098  2149390   \n",
       "8    2148760  -267789 -1724138  1148000  1156438   732000  1358238  2373541   \n",
       "9    2327297   176162 -1318945  1571000  1235708   877000  1377054  1926203   \n",
       "10   2063673   799162  -701031  1106000  1197360   743000  1417038  2593368   \n",
       "12   1639267  1618314   -88940  1673000  2020530  1324000  2256305  3482378   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "161  1222031  3647779   881563  1718000  2675640  1576000  3000000  5472222   \n",
       "162   652174  4386747  1261845  1828000  3096442  1768000  3422819  6433770   \n",
       "163  1485714  4017258  1123510  1746000  2727661  1590000  3059261  5686695   \n",
       "165  1885790  2958064   395123  1691000  2284112  1407000  2574722  4401001   \n",
       "166  -770925  5614599  2339640  2185000  3960474  2255000  4179538  7290656   \n",
       "\n",
       "         MSRm  \n",
       "5     2440148  \n",
       "8     2737539  \n",
       "9     2153802  \n",
       "10    3039489  \n",
       "12    4382641  \n",
       "..        ...  \n",
       "161   8485610  \n",
       "162  11466633  \n",
       "163   9070438  \n",
       "165   6037665  \n",
       "166  15262347  \n",
       "\n",
       "[88 rows x 171 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split off unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with 20%\n",
    "# values of original dataframe\n",
    "unseen_data = model_data_.sample(frac = 0.2)\n",
    " \n",
    "# Creating dataframe with\n",
    "# rest of the 80% values\n",
    "model_data = model_data_.drop(unseen_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>b1_fpca2_0509_min</th>\n",
       "      <th>b1_fpca2_0509_max</th>\n",
       "      <th>b1_fpca2_0509_mean</th>\n",
       "      <th>b1_fpca2_0509_med</th>\n",
       "      <th>b1_fpca2_0509_std</th>\n",
       "      <th>b1_h99a_01122_min</th>\n",
       "      <th>b1_h99a_01122_max</th>\n",
       "      <th>b1_h99a_01122_mean</th>\n",
       "      <th>b1_h99a_01122_med</th>\n",
       "      <th>...</th>\n",
       "      <th>NDGIm</th>\n",
       "      <th>RIm</th>\n",
       "      <th>NBRm</th>\n",
       "      <th>NDIIm</th>\n",
       "      <th>GDVIm</th>\n",
       "      <th>MSAVIm</th>\n",
       "      <th>DVIm</th>\n",
       "      <th>SAVIm</th>\n",
       "      <th>NDVIm</th>\n",
       "      <th>MSRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>6922.04</td>\n",
       "      <td>2.12</td>\n",
       "      <td>22.31</td>\n",
       "      <td>10.57</td>\n",
       "      <td>7.68</td>\n",
       "      <td>5.32</td>\n",
       "      <td>6.20</td>\n",
       "      <td>18.01</td>\n",
       "      <td>11.41</td>\n",
       "      <td>10.78</td>\n",
       "      <td>...</td>\n",
       "      <td>-2008766</td>\n",
       "      <td>2008766</td>\n",
       "      <td>2254215</td>\n",
       "      <td>-131606</td>\n",
       "      <td>1815000</td>\n",
       "      <td>2524744</td>\n",
       "      <td>1540000</td>\n",
       "      <td>2822581</td>\n",
       "      <td>4836683</td>\n",
       "      <td>6951340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7203.20</td>\n",
       "      <td>14.65</td>\n",
       "      <td>20.29</td>\n",
       "      <td>16.37</td>\n",
       "      <td>15.54</td>\n",
       "      <td>1.69</td>\n",
       "      <td>9.25</td>\n",
       "      <td>14.04</td>\n",
       "      <td>11.53</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1346154</td>\n",
       "      <td>1346154</td>\n",
       "      <td>2222586</td>\n",
       "      <td>-611013</td>\n",
       "      <td>1417000</td>\n",
       "      <td>2217732</td>\n",
       "      <td>1277000</td>\n",
       "      <td>2568727</td>\n",
       "      <td>5197395</td>\n",
       "      <td>7788780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2439.36</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.29</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1962121</td>\n",
       "      <td>1962121</td>\n",
       "      <td>1941265</td>\n",
       "      <td>-478650</td>\n",
       "      <td>1704000</td>\n",
       "      <td>1717255</td>\n",
       "      <td>1186000</td>\n",
       "      <td>1903896</td>\n",
       "      <td>2730203</td>\n",
       "      <td>3232945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5778.08</td>\n",
       "      <td>6.80</td>\n",
       "      <td>17.20</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.44</td>\n",
       "      <td>12.23</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-2144638</td>\n",
       "      <td>2144638</td>\n",
       "      <td>817031</td>\n",
       "      <td>-1128728</td>\n",
       "      <td>1405000</td>\n",
       "      <td>1329826</td>\n",
       "      <td>889000</td>\n",
       "      <td>1513449</td>\n",
       "      <td>2332721</td>\n",
       "      <td>2682615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>129.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.63</td>\n",
       "      <td>3.28</td>\n",
       "      <td>5.57</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1991361</td>\n",
       "      <td>1991361</td>\n",
       "      <td>1832298</td>\n",
       "      <td>-596462</td>\n",
       "      <td>1359000</td>\n",
       "      <td>1359298</td>\n",
       "      <td>898000</td>\n",
       "      <td>1552917</td>\n",
       "      <td>2444203</td>\n",
       "      <td>2833449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3603.60</td>\n",
       "      <td>0.97</td>\n",
       "      <td>5.26</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.31</td>\n",
       "      <td>6.66</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1241722</td>\n",
       "      <td>1241722</td>\n",
       "      <td>793720</td>\n",
       "      <td>-1136974</td>\n",
       "      <td>1417000</td>\n",
       "      <td>1684005</td>\n",
       "      <td>1117000</td>\n",
       "      <td>1896864</td>\n",
       "      <td>2914166</td>\n",
       "      <td>3500123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7658.54</td>\n",
       "      <td>2.48</td>\n",
       "      <td>27.61</td>\n",
       "      <td>10.33</td>\n",
       "      <td>9.06</td>\n",
       "      <td>6.57</td>\n",
       "      <td>12.56</td>\n",
       "      <td>16.83</td>\n",
       "      <td>14.28</td>\n",
       "      <td>14.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1754917</td>\n",
       "      <td>1754917</td>\n",
       "      <td>445050</td>\n",
       "      <td>-767996</td>\n",
       "      <td>1180000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>948000</td>\n",
       "      <td>1895495</td>\n",
       "      <td>3788969</td>\n",
       "      <td>4899924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>295.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.19</td>\n",
       "      <td>5.17</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-2123987</td>\n",
       "      <td>2123987</td>\n",
       "      <td>1792230</td>\n",
       "      <td>-606941</td>\n",
       "      <td>1629000</td>\n",
       "      <td>1462415</td>\n",
       "      <td>1026000</td>\n",
       "      <td>1625475</td>\n",
       "      <td>2296329</td>\n",
       "      <td>2633942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8031.45</td>\n",
       "      <td>12.93</td>\n",
       "      <td>28.71</td>\n",
       "      <td>19.42</td>\n",
       "      <td>19.79</td>\n",
       "      <td>4.33</td>\n",
       "      <td>10.19</td>\n",
       "      <td>16.27</td>\n",
       "      <td>14.23</td>\n",
       "      <td>14.49</td>\n",
       "      <td>...</td>\n",
       "      <td>-1986971</td>\n",
       "      <td>1986971</td>\n",
       "      <td>1175911</td>\n",
       "      <td>-637281</td>\n",
       "      <td>1271000</td>\n",
       "      <td>1743222</td>\n",
       "      <td>1027000</td>\n",
       "      <td>2054274</td>\n",
       "      <td>4109644</td>\n",
       "      <td>5477017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>4327.62</td>\n",
       "      <td>6.42</td>\n",
       "      <td>14.65</td>\n",
       "      <td>9.83</td>\n",
       "      <td>9.42</td>\n",
       "      <td>1.99</td>\n",
       "      <td>8.46</td>\n",
       "      <td>12.20</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.69</td>\n",
       "      <td>...</td>\n",
       "      <td>-1803279</td>\n",
       "      <td>1803279</td>\n",
       "      <td>2207727</td>\n",
       "      <td>-611615</td>\n",
       "      <td>1708000</td>\n",
       "      <td>2191866</td>\n",
       "      <td>1389000</td>\n",
       "      <td>2457827</td>\n",
       "      <td>3994823</td>\n",
       "      <td>5265843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>166.32</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.01</td>\n",
       "      <td>3.88</td>\n",
       "      <td>...</td>\n",
       "      <td>-1632440</td>\n",
       "      <td>1632440</td>\n",
       "      <td>1110588</td>\n",
       "      <td>-666140</td>\n",
       "      <td>1159000</td>\n",
       "      <td>1006134</td>\n",
       "      <td>690000</td>\n",
       "      <td>1145926</td>\n",
       "      <td>1711310</td>\n",
       "      <td>1886658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>22881.07</td>\n",
       "      <td>8.36</td>\n",
       "      <td>21.29</td>\n",
       "      <td>14.25</td>\n",
       "      <td>13.78</td>\n",
       "      <td>3.24</td>\n",
       "      <td>12.57</td>\n",
       "      <td>17.09</td>\n",
       "      <td>14.67</td>\n",
       "      <td>14.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-1156337</td>\n",
       "      <td>1156337</td>\n",
       "      <td>3815343</td>\n",
       "      <td>441252</td>\n",
       "      <td>1557000</td>\n",
       "      <td>2468686</td>\n",
       "      <td>1432000</td>\n",
       "      <td>2812255</td>\n",
       "      <td>5428355</td>\n",
       "      <td>8370609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>8376.22</td>\n",
       "      <td>16.45</td>\n",
       "      <td>33.19</td>\n",
       "      <td>23.64</td>\n",
       "      <td>24.39</td>\n",
       "      <td>3.81</td>\n",
       "      <td>7.49</td>\n",
       "      <td>17.52</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-1400000</td>\n",
       "      <td>1400000</td>\n",
       "      <td>3706745</td>\n",
       "      <td>541272</td>\n",
       "      <td>1821000</td>\n",
       "      <td>2779416</td>\n",
       "      <td>1653000</td>\n",
       "      <td>3091260</td>\n",
       "      <td>5471698</td>\n",
       "      <td>8484228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4717.19</td>\n",
       "      <td>11.31</td>\n",
       "      <td>22.31</td>\n",
       "      <td>17.25</td>\n",
       "      <td>17.86</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.89</td>\n",
       "      <td>11.98</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1868898</td>\n",
       "      <td>1868898</td>\n",
       "      <td>2758906</td>\n",
       "      <td>-174394</td>\n",
       "      <td>1727000</td>\n",
       "      <td>2384952</td>\n",
       "      <td>1459000</td>\n",
       "      <td>2681657</td>\n",
       "      <td>4615628</td>\n",
       "      <td>6475599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>10544.60</td>\n",
       "      <td>4.73</td>\n",
       "      <td>36.62</td>\n",
       "      <td>25.05</td>\n",
       "      <td>25.99</td>\n",
       "      <td>10.21</td>\n",
       "      <td>17.54</td>\n",
       "      <td>20.11</td>\n",
       "      <td>18.86</td>\n",
       "      <td>18.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-1099099</td>\n",
       "      <td>1099099</td>\n",
       "      <td>3627293</td>\n",
       "      <td>259268</td>\n",
       "      <td>1623000</td>\n",
       "      <td>2574788</td>\n",
       "      <td>1501000</td>\n",
       "      <td>2911548</td>\n",
       "      <td>5492133</td>\n",
       "      <td>8538307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7432.81</td>\n",
       "      <td>5.83</td>\n",
       "      <td>15.54</td>\n",
       "      <td>10.64</td>\n",
       "      <td>11.31</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.94</td>\n",
       "      <td>13.44</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-2108230</td>\n",
       "      <td>2108230</td>\n",
       "      <td>1250308</td>\n",
       "      <td>-1369656</td>\n",
       "      <td>1581000</td>\n",
       "      <td>1907643</td>\n",
       "      <td>1207000</td>\n",
       "      <td>2166966</td>\n",
       "      <td>3597615</td>\n",
       "      <td>4573387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>35199.71</td>\n",
       "      <td>30.93</td>\n",
       "      <td>52.67</td>\n",
       "      <td>41.46</td>\n",
       "      <td>40.65</td>\n",
       "      <td>5.37</td>\n",
       "      <td>12.14</td>\n",
       "      <td>23.43</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-193705</td>\n",
       "      <td>193705</td>\n",
       "      <td>5005329</td>\n",
       "      <td>1645988</td>\n",
       "      <td>1707000</td>\n",
       "      <td>3018036</td>\n",
       "      <td>1691000</td>\n",
       "      <td>3367184</td>\n",
       "      <td>6675878</td>\n",
       "      <td>12397828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1293.60</td>\n",
       "      <td>3.74</td>\n",
       "      <td>8.36</td>\n",
       "      <td>5.03</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.31</td>\n",
       "      <td>6.60</td>\n",
       "      <td>12.27</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-1971154</td>\n",
       "      <td>1971154</td>\n",
       "      <td>1576238</td>\n",
       "      <td>-158614</td>\n",
       "      <td>1697000</td>\n",
       "      <td>1768182</td>\n",
       "      <td>1205000</td>\n",
       "      <td>1966170</td>\n",
       "      <td>2873837</td>\n",
       "      <td>3440832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10763.32</td>\n",
       "      <td>17.38</td>\n",
       "      <td>30.93</td>\n",
       "      <td>24.45</td>\n",
       "      <td>24.40</td>\n",
       "      <td>3.18</td>\n",
       "      <td>10.89</td>\n",
       "      <td>13.91</td>\n",
       "      <td>12.09</td>\n",
       "      <td>11.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-1500743</td>\n",
       "      <td>1500743</td>\n",
       "      <td>1497280</td>\n",
       "      <td>-742278</td>\n",
       "      <td>1436000</td>\n",
       "      <td>2065120</td>\n",
       "      <td>1234000</td>\n",
       "      <td>2378566</td>\n",
       "      <td>4435658</td>\n",
       "      <td>6106878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>406.56</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.17</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1988017</td>\n",
       "      <td>1988017</td>\n",
       "      <td>2984615</td>\n",
       "      <td>199396</td>\n",
       "      <td>1905000</td>\n",
       "      <td>1545380</td>\n",
       "      <td>1175000</td>\n",
       "      <td>1666352</td>\n",
       "      <td>2106867</td>\n",
       "      <td>2384863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>7478.59</td>\n",
       "      <td>1.21</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>8.34</td>\n",
       "      <td>12.98</td>\n",
       "      <td>10.63</td>\n",
       "      <td>10.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-1979434</td>\n",
       "      <td>1979434</td>\n",
       "      <td>615672</td>\n",
       "      <td>-1104742</td>\n",
       "      <td>1083000</td>\n",
       "      <td>1277097</td>\n",
       "      <td>775000</td>\n",
       "      <td>1521796</td>\n",
       "      <td>2936718</td>\n",
       "      <td>3533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9995.51</td>\n",
       "      <td>19.30</td>\n",
       "      <td>40.08</td>\n",
       "      <td>30.47</td>\n",
       "      <td>32.06</td>\n",
       "      <td>6.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>18.40</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.50</td>\n",
       "      <td>...</td>\n",
       "      <td>770925</td>\n",
       "      <td>-770925</td>\n",
       "      <td>5614599</td>\n",
       "      <td>2339640</td>\n",
       "      <td>2185000</td>\n",
       "      <td>3960474</td>\n",
       "      <td>2255000</td>\n",
       "      <td>4179538</td>\n",
       "      <td>7290656</td>\n",
       "      <td>15262347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  b1_fpca2_0509_min  b1_fpca2_0509_max  b1_fpca2_0509_mean  \\\n",
       "159  6922.04               2.12              22.31               10.57   \n",
       "49   7203.20              14.65              20.29               16.37   \n",
       "129  2439.36               0.97               2.48                1.66   \n",
       "145  5778.08               6.80              17.20               11.25   \n",
       "100   129.36               0.57               2.87                1.17   \n",
       "128  3603.60               0.97               5.26                3.23   \n",
       "40   7658.54               2.48              27.61               10.33   \n",
       "97    295.68               0.75               3.74                1.90   \n",
       "39   8031.45              12.93              28.71               19.42   \n",
       "154  4327.62               6.42              14.65                9.83   \n",
       "96    166.32               0.75               2.48                1.61   \n",
       "135 22881.07               8.36              21.29               14.25   \n",
       "55   8376.22              16.45              33.19               23.64   \n",
       "118  4717.19              11.31              22.31               17.25   \n",
       "149 10544.60               4.73              36.62               25.05   \n",
       "82   7432.81               5.83              15.54               10.64   \n",
       "44  35199.71              30.93              52.67               41.46   \n",
       "18   1293.60               3.74               8.36                5.03   \n",
       "142 10763.32              17.38              30.93               24.45   \n",
       "99    406.56               0.41               1.49                0.80   \n",
       "141  7478.59               1.21               4.22                3.26   \n",
       "166  9995.51              19.30              40.08               30.47   \n",
       "\n",
       "     b1_fpca2_0509_med  b1_fpca2_0509_std  b1_h99a_01122_min  \\\n",
       "159               7.68               5.32               6.20   \n",
       "49               15.54               1.69               9.25   \n",
       "129               1.49               0.45               0.95   \n",
       "145              11.00               2.80               7.44   \n",
       "100               0.97               0.63               3.28   \n",
       "128               3.52               1.25               5.31   \n",
       "40                9.06               6.57              12.56   \n",
       "97                1.79               0.93               3.19   \n",
       "39               19.79               4.33              10.19   \n",
       "154               9.42               1.99               8.46   \n",
       "96                1.49               0.55               2.98   \n",
       "135              13.78               3.24              12.57   \n",
       "55               24.39               3.81               7.49   \n",
       "118              17.86               2.96               3.89   \n",
       "149              25.99              10.21              17.54   \n",
       "82               11.31               2.70               3.94   \n",
       "44               40.65               5.37              12.14   \n",
       "18                4.73               1.31               6.60   \n",
       "142              24.40               3.18              10.89   \n",
       "99                0.75               0.32               2.17   \n",
       "141               3.29               0.89               8.34   \n",
       "166              32.06               6.23              12.00   \n",
       "\n",
       "     b1_h99a_01122_max  b1_h99a_01122_mean  b1_h99a_01122_med  ...    NDGIm  \\\n",
       "159              18.01               11.41              10.78  ... -2008766   \n",
       "49               14.04               11.53              12.00  ... -1346154   \n",
       "129               7.29                4.02               4.13  ... -1962121   \n",
       "145              12.23                9.75               9.73  ... -2144638   \n",
       "100               5.57                4.18               4.06  ... -1991361   \n",
       "128               6.66                6.00               6.01  ... -1241722   \n",
       "40               16.83               14.28              14.09  ... -1754917   \n",
       "97                5.17                4.19               4.15  ... -2123987   \n",
       "39               16.27               14.23              14.49  ... -1986971   \n",
       "154              12.20               10.52              10.69  ... -1803279   \n",
       "96                5.20                4.01               3.88  ... -1632440   \n",
       "135              17.09               14.67              14.26  ... -1156337   \n",
       "55               17.52               13.01              13.04  ... -1400000   \n",
       "118              11.98                9.04               9.31  ... -1868898   \n",
       "149              20.11               18.86              18.83  ... -1099099   \n",
       "82               13.44                8.73               8.76  ... -2108230   \n",
       "44               23.43               17.00              17.02  ...  -193705   \n",
       "18               12.27                8.60               8.32  ... -1971154   \n",
       "142              13.91               12.09              11.77  ... -1500743   \n",
       "99                4.46                3.25               3.17  ... -1988017   \n",
       "141              12.98               10.63              10.56  ... -1979434   \n",
       "166              18.40               16.60              17.50  ...   770925   \n",
       "\n",
       "         RIm     NBRm    NDIIm    GDVIm   MSAVIm     DVIm    SAVIm    NDVIm  \\\n",
       "159  2008766  2254215  -131606  1815000  2524744  1540000  2822581  4836683   \n",
       "49   1346154  2222586  -611013  1417000  2217732  1277000  2568727  5197395   \n",
       "129  1962121  1941265  -478650  1704000  1717255  1186000  1903896  2730203   \n",
       "145  2144638   817031 -1128728  1405000  1329826   889000  1513449  2332721   \n",
       "100  1991361  1832298  -596462  1359000  1359298   898000  1552917  2444203   \n",
       "128  1241722   793720 -1136974  1417000  1684005  1117000  1896864  2914166   \n",
       "40   1754917   445050  -767996  1180000  1600000   948000  1895495  3788969   \n",
       "97   2123987  1792230  -606941  1629000  1462415  1026000  1625475  2296329   \n",
       "39   1986971  1175911  -637281  1271000  1743222  1027000  2054274  4109644   \n",
       "154  1803279  2207727  -611615  1708000  2191866  1389000  2457827  3994823   \n",
       "96   1632440  1110588  -666140  1159000  1006134   690000  1145926  1711310   \n",
       "135  1156337  3815343   441252  1557000  2468686  1432000  2812255  5428355   \n",
       "55   1400000  3706745   541272  1821000  2779416  1653000  3091260  5471698   \n",
       "118  1868898  2758906  -174394  1727000  2384952  1459000  2681657  4615628   \n",
       "149  1099099  3627293   259268  1623000  2574788  1501000  2911548  5492133   \n",
       "82   2108230  1250308 -1369656  1581000  1907643  1207000  2166966  3597615   \n",
       "44    193705  5005329  1645988  1707000  3018036  1691000  3367184  6675878   \n",
       "18   1971154  1576238  -158614  1697000  1768182  1205000  1966170  2873837   \n",
       "142  1500743  1497280  -742278  1436000  2065120  1234000  2378566  4435658   \n",
       "99   1988017  2984615   199396  1905000  1545380  1175000  1666352  2106867   \n",
       "141  1979434   615672 -1104742  1083000  1277097   775000  1521796  2936718   \n",
       "166  -770925  5614599  2339640  2185000  3960474  2255000  4179538  7290656   \n",
       "\n",
       "         MSRm  \n",
       "159   6951340  \n",
       "49    7788780  \n",
       "129   3232945  \n",
       "145   2682615  \n",
       "100   2833449  \n",
       "128   3500123  \n",
       "40    4899924  \n",
       "97    2633942  \n",
       "39    5477017  \n",
       "154   5265843  \n",
       "96    1886658  \n",
       "135   8370609  \n",
       "55    8484228  \n",
       "118   6475599  \n",
       "149   8538307  \n",
       "82    4573387  \n",
       "44   12397828  \n",
       "18    3440832  \n",
       "142   6106878  \n",
       "99    2384863  \n",
       "141   3533459  \n",
       "166  15262347  \n",
       "\n",
       "[22 rows x 171 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklean.model\n",
    "# randomly split data into train and test datasets, the user needs to define the variables \n",
    "xdata1 = model_data.iloc[:, 1:].astype('float32')\n",
    "ydata1 = model_data[[\"target\"]].astype('int')\n",
    "ydata2 = ydata1.values\n",
    "ydata = ydata2.ravel()\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdata1, ydata, train_size=0.70) #, stratify=ydata)  \n",
    "         \n",
    "#y_test.value_counts()\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot up Histograms of Lidar meanCHM for train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(y_train)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(y_test)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_plot(X_train, y_train, model, str_model, x_limit, y_limit, x_off, y_off):\n",
    "    \n",
    "#     print(\"=\"*50)\n",
    "#     print(str_model)\n",
    "#     print(\"Train model\")\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "#     # fit model with training data\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     train_ev, train_me, train_mae, train_mse, train_rmse, train_msle, train_rmsle, train_mape, train_medae, train_r2, \\\n",
    "#     train_bias, train_n = metrics(y_1, train_predict)\n",
    "\n",
    "#     r2 = round(model.score(X_train, y_train), 2)\n",
    "#     mse = round(np.mean((y_train - model.predict(X_train))**2), 2)\n",
    "\n",
    "   \n",
    "#     print(f\"R2: {r2}\")\n",
    "#     print(f\"MSE: {mse}\")\n",
    "#     #print('intercept:', model.intercept_)\n",
    "#     #print('slope:', model.coef_) \n",
    "\n",
    "\n",
    "#     # plotting the training datat\n",
    "#     plt.title(f\"Model: {str_model.replace('_', ' ')} - Data: training\")\n",
    "#     plt.scatter(model.predict(X_train), y_train)  \n",
    "#     x = [x_limit,y_limit]\n",
    "#     y = [x_limit, y_limit]\n",
    "\n",
    "#     #sets the limits of the axis\n",
    "#     plt.xlim(x_limit, y_limit)\n",
    "#     plt.ylim(x_limit, y_limit)\n",
    "\n",
    "#     plt.ylabel('Observed target')\n",
    "#     plt.xlabel('Predicted target')\n",
    "    \n",
    "\n",
    "\n",
    "#     # 1 for 1 line\n",
    "#     #adding text inside the plot\n",
    "#     plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "# #     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {120}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$n = {len(y_train)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "#     plt.plot(x, y, color = 'r')\n",
    "\n",
    "#     plot_out = os.path.join(model_outputs, f'{str_model}_train_plot.jpg')\n",
    "#     plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "#     print(f\"plot saved to: {plot_out}\")\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "#     # ------------------------------------ Predict Test ----------------------------------\n",
    "#     print(\"=\"*50)\n",
    "#     print(str_model)\n",
    "#     print(\"Test model\")\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "    \n",
    "#     # call the model\n",
    "#     y_test_predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "#     r2 = model.score(X_test, y_test)\n",
    "#     mse = np.mean((y_test - model.predict(X_test))** 2)\n",
    "#     rmse = np.sqrt(np.mean((y_test_predict - y_test) ** 2))\n",
    "#     bias = np.mean(y_test) - np.mean(y_test_predict)\n",
    "#     print('Predicted data r2 =', r2)\n",
    "#     print('MSE =', mse)\n",
    "# #     print(f\"Intercept: {model.intercept_}\")\n",
    "#     #print(f\"Slope: {model.coef_}\")\n",
    "# #     print('RMSE =', format(rmse, '.3f'))\n",
    "# #     print('bias =' , format(bias, '.3f'))\n",
    "# #     print('n =' , len(y_test))\n",
    "\n",
    "\n",
    "#     # r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "#     # mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "#     # rmse = round(np.sqrt(np.mean((y2_predict - y_2) ** 2)), 2)\n",
    "#     # bias = round(np.mean(y_2) - np.mean(y2_predict), 2)\n",
    "\n",
    "#     # plotting the training datat\n",
    "#     plt.title(f\"Model: {str_model.replace('_', ' ')} - Data: testing\")\n",
    "    \n",
    "#     plt.scatter(y_test_predict, y_test) #, s=10, c='b', marker='o')\n",
    "#     # data for the 1 for 1 line\n",
    "#     x = [x_limit,y_limit]\n",
    "#     y = [x_limit, y_limit]\n",
    "\n",
    "#     #sets the limits of the axis\n",
    "#     plt.xlim(x_limit, y_limit)\n",
    "#     plt.ylim(x_limit, y_limit)\n",
    "#     plt.ylabel('Observed')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     # 1 for 1 line\n",
    "\n",
    "#     #adding text inside the plot\n",
    "#     plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$RMSE = {round(rmse, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*4)), f'$Bias = {round(bias, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*5)), f'$n = {len(y_test)}$', fontsize = 12)\n",
    "\n",
    "#     plt.plot(x, y, color = 'r')\n",
    "#     plot_out = os.path.join(model_outputs, f'{str_model}_test_plot.jpg')\n",
    "#     plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "#     print(f\"plot saved to: {plot_out}\")\n",
    "#     plt.show()    \n",
    "    \n",
    "    \n",
    "#     return str_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_plot(X_train, y_train, model, str_model, x_limit, y_limit, x_off, y_off):\n",
    "    \n",
    "#     feature_importance = model.feature_importances_\n",
    "#     fi = enumerate(model.feature_importances_)\n",
    "#     cols = xdata1.columns\n",
    "#     fiResult = [(value,cols[i]) for (i,value) in fi]\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Train model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # fit model with training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_predict = model.predict(X_train)\n",
    "    \n",
    "    \n",
    "    train_ev, train_me, train_mae, train_mse, train_rmse, train_msle, train_rmsle, train_mape, train_medae, train_r2, \\\n",
    "    train_bias, train_n = metrics(y_train, train_predict)\n",
    "\n",
    "#     r2 = round(model.score(X_train, y_train), 2)\n",
    "#     mse = round(np.mean((y_train - model.predict(X_train))**2), 2)\n",
    "#     from sklearn import metrics\n",
    "#     mse_ = metrics.mean_absolute_error(y_train, train_predict)\n",
    "\n",
    "   \n",
    "#     print(f\"R2: {r2}\")\n",
    "#     print(f\"MSE: {mse}\")\n",
    "#     #print('intercept:', model.intercept_)\n",
    "#     #print('slope:', model.coef_) \n",
    "\n",
    "\n",
    "    # plotting the training datat\n",
    "    plt.title(f\"TRAINING - {str_model.replace('_', ' ')} - Observed vrs. Pridicted AGB\")\n",
    "    plt.scatter(model.predict(X_train), y_train)  \n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "\n",
    "    plt.ylabel('Observed AGB')\n",
    "    plt.xlabel('Predicted AGB')\n",
    "    \n",
    "#     plt.text(300, 37000, f'$R^2 = {round(train_r2, 2)}$', fontsize = 12)\n",
    "#     plt.text(300, 35000, f'$MSE = {round(train_mse, 2)}$', fontsize = 12)\n",
    "#     plt.text(300, 33000, f'$RMSE = {round(train_rmse, 2)}$', fontsize = 12)\n",
    "#     plt.text(300, 31000, f'$Bias = {round(train_bias, 2)}$', fontsize = 12)\n",
    "#     plt.text(300, 29000, f'$n = {train_n}$', fontsize = 12)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # 1 for 1 line\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(train_r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(train_mse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$RMSE = {round(train_rmse, 2)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*4)), f'$Bias = {round(train_bias, 2)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*5)), f'$n = {train_n}$', fontsize = 12)\n",
    "\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_training_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ Predict Test ----------------------------------\n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Test model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    \n",
    "    # call the model\n",
    "    test_predict = model.predict(X_test)\n",
    "\n",
    "    test_ev, test_me, test_mae, test_mse, test_rmse, test_msle, test_rmsle, test_mape, \\\n",
    "    test_medae, test_r2, test_bias, test_n = metrics(y_test, test_predict)\n",
    "#     r2 = model.score(X_test, y_test)\n",
    "#     mse = np.mean((y_test - model.predict(X_test))** 2)\n",
    "#     rmse = np.sqrt(np.mean((y_test_predict - y_test) ** 2))\n",
    "#     bias = np.mean(y_test) - np.mean(y_test_predict)\n",
    "#     print('Predicted data r2 =', r2)\n",
    "#     print('MSE =', mse)\n",
    "# #     print(f\"Intercept: {model.intercept_}\")\n",
    "#     #print(f\"Slope: {model.coef_}\")\n",
    "# #     print('RMSE =', format(rmse, '.3f'))\n",
    "# #     print('bias =' , format(bias, '.3f'))\n",
    "# #     print('n =' , len(y_test))\n",
    "\n",
    "\n",
    "#     # r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "#     # mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "#     # rmse = round(np.sqrt(np.mean((y2_predict - y_2) ** 2)), 2)\n",
    "#     # bias = round(np.mean(y_2) - np.mean(y2_predict), 2)\n",
    "\n",
    "    # plotting the training datat\n",
    "    plt.title(f\"TESTING {str_model.replace('_', ' ')} - Observed vrs. Pridicted AGB\")\n",
    "    \n",
    "    plt.scatter(test_predict, y_test) #, s=10, c='b', marker='o')\n",
    "    # data for the 1 for 1 line\n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "    plt.ylabel('Observed')\n",
    "    plt.xlabel('Predicted')\n",
    "    # 1 for 1 line\n",
    "\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(test_r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(test_mse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$RMSE = {round(test_rmse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*4)), f'$Bias = {round(test_bias, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*5)), f'$n = {test_n}$', fontsize = 12)\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_test_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    return str_model, model, fiResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-40133459e065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m str_model, model, fiResult = train_model_plot(X_train, y_train, LinearRegression(),\n\u001b[1;32m----> 2\u001b[1;33m                                     \"Linear_regression\", 0, 40000, 1000, 3000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-117-cd92548825da>\u001b[0m in \u001b[0;36mtrain_model_plot\u001b[1;34m(X_train, y_train, model, str_model, x_limit, y_limit, x_off, y_off)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_model_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_off\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_off\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "str_model, model, fiResult = train_model_plot(X_train, y_train, LinearRegression(),\n",
    "                                    \"Linear_regression\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model, model = train_model_plot(X_train, y_train, KernelRidge(),\n",
    "                                    \"kernel_ridge\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str_model, model = train_model_plot(X_train, y_train, ElasticNet(),\n",
    "                                    \"elastic_net\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str_model, model = train_model_plot(X_train, y_train, BayesianRidge(),\n",
    "                                    \"bayesian_ridge\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model, model = train_model_plot(X_train, y_train, GradientBoostingRegressor(),\n",
    "                                    \"gradient_boosting_regressor\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model, model = train_model_plot(X_train, y_train, SVR(),\n",
    "                                    \"svr\", 0, 40000, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly_model_plot(X_train, y_train, model, str_model, x_limit, y_limit, x_off, y_off):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Train model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    \n",
    "    x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_train)\n",
    "    x__ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_test)\n",
    "    \n",
    "    model.fit(x_, y_train)\n",
    "\n",
    "    r2 = round(model.score(x_, y_train), 2)\n",
    "    mse = round(np.mean((y_train - model.predict(x_))**2), 2)\n",
    "    \n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "#     print(f\"Intercept: {model.intercept_}\")\n",
    "    #print(f\"Slope: {model.coef_}\")\n",
    "\n",
    "\n",
    "    plt.scatter(model.predict(x_), y_train)  \n",
    "    # data for the 1 for 1 line\n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "\n",
    "    plt.ylabel('Observed target')\n",
    "\n",
    "    plt.xlabel('Predicted target')\n",
    "\n",
    "    # 1 for 1 line\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {120}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$n = {len(y_train)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_train_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ Predict Test ----------------------------------\n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Test model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    \n",
    "    # call the model\n",
    "    y_test_predict = model.predict(x__)\n",
    "\n",
    "\n",
    "    r2 = model.score(x__, y_test)\n",
    "    mse = np.mean((y_test - model.predict(x__))** 2)\n",
    "    rmse = np.sqrt(np.mean((y_test_predict - y_test) ** 2))\n",
    "    bias = np.mean(y_test) - np.mean(y_test_predict)\n",
    "    print('Predicted data r2 =', r2)\n",
    "    print('MSE =', mse)\n",
    "    print(f\"Intercept: {model.intercept_}\")\n",
    "#     print(f\"Slope: {model.coef_}\")\n",
    "    print('RMSE =', format(rmse, '.3f'))\n",
    "    print('bias =' , format(bias, '.3f'))\n",
    "    print('n =' , len(y_test))\n",
    "\n",
    "\n",
    "    # r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "    # mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "    # rmse = round(np.sqrt(np.mean((y2_predict - y_2) ** 2)), 2)\n",
    "    # bias = round(np.mean(y_2) - np.mean(y2_predict), 2)\n",
    "\n",
    "    plt.scatter(y_test_predict, y_test) #, s=10, c='b', marker='o')\n",
    "    # data for the 1 for 1 line\n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "    plt.ylabel('Observed')\n",
    "    plt.xlabel('Predicted')\n",
    "    # 1 for 1 line\n",
    "\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$RMSE = {round(rmse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*4)), f'$Bias = {round(bias, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*5)), f'$n = {len(y_test)}$', fontsize = 12)\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_test_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    return str_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model, model = train_poly_model_plot(X_train, y_train, LinearRegression(),\n",
    "                                    \"polynomia_features\", 0, 40000, 1000, 3000)\n",
    "# PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to load through different env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/multivariate-adaptive-regression-splines-mars-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyearth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xdata1, ydata, train_size=0.70) #, stratify=ydata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data into 4 bins\n",
    "df_cut, bins = pd.cut(X_train, 4, retbins=True, right=True)\n",
    "df_cut.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_splines_model_plot(X_train, y_train, model, str_model, x_limit, y_limit, x_off, y_off):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Train model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    \n",
    "    x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_train)\n",
    "    x__ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_test)\n",
    "    \n",
    "    model.fit(x_, y_train)\n",
    "\n",
    "    r2 = round(model.score(x_, y_train), 2)\n",
    "    mse = round(np.mean((y_train - model.predict(x_))**2), 2)\n",
    "    \n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "#     print(f\"Intercept: {model.intercept_}\")\n",
    "    #print(f\"Slope: {model.coef_}\")\n",
    "\n",
    "\n",
    "    plt.scatter(model.predict(x_), y_train)  \n",
    "    # data for the 1 for 1 line\n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "\n",
    "    plt.ylabel('Observed target')\n",
    "\n",
    "    plt.xlabel('Predicted target')\n",
    "\n",
    "    # 1 for 1 line\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {120}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$n = {len(y_train)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_train_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ Predict Test ----------------------------------\n",
    "    print(\"=\"*50)\n",
    "    print(str_model)\n",
    "    print(\"Test model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    \n",
    "    # call the model\n",
    "    y_test_predict = model.predict(x__)\n",
    "\n",
    "\n",
    "    r2 = model.score(x__, y_test)\n",
    "    mse = np.mean((y_test - model.predict(x__))** 2)\n",
    "    rmse = np.sqrt(np.mean((y_test_predict - y_test) ** 2))\n",
    "    bias = np.mean(y_test) - np.mean(y_test_predict)\n",
    "    print('Predicted data r2 =', r2)\n",
    "    print('MSE =', mse)\n",
    "    print(f\"Intercept: {model.intercept_}\")\n",
    "#     print(f\"Slope: {model.coef_}\")\n",
    "    print('RMSE =', format(rmse, '.3f'))\n",
    "    print('bias =' , format(bias, '.3f'))\n",
    "    print('n =' , len(y_test))\n",
    "\n",
    "\n",
    "    # r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "    # mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "    # rmse = round(np.sqrt(np.mean((y2_predict - y_2) ** 2)), 2)\n",
    "    # bias = round(np.mean(y_2) - np.mean(y2_predict), 2)\n",
    "\n",
    "    plt.scatter(y_test_predict, y_test) #, s=10, c='b', marker='o')\n",
    "    # data for the 1 for 1 line\n",
    "    x = [x_limit,y_limit]\n",
    "    y = [x_limit, y_limit]\n",
    "\n",
    "    #sets the limits of the axis\n",
    "    plt.xlim(x_limit, y_limit)\n",
    "    plt.ylim(x_limit, y_limit)\n",
    "    plt.ylabel('Observed')\n",
    "    plt.xlabel('Predicted')\n",
    "    # 1 for 1 line\n",
    "\n",
    "    #adding text inside the plot\n",
    "    plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$RMSE = {round(rmse, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*4)), f'$Bias = {round(bias, 4)}$', fontsize = 12)\n",
    "    plt.text((x_limit + x_off), (y_limit - (y_off*5)), f'$n = {len(y_test)}$', fontsize = 12)\n",
    "\n",
    "    plt.plot(x, y, color = 'r')\n",
    "    plot_out = os.path.join(model_outputs, f'{str_model}_test_plot.jpg')\n",
    "    plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "    print(f\"plot saved to: {plot_out}\")\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    return str_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(str_model)\n",
    "print(\"Train model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "model.fit(x_, y_train)\n",
    "\n",
    "r2 = round(model.score(x_, y_train), 2)\n",
    "mse = round(np.mean((y_train - model.predict(x_))**2), 2)\n",
    "\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "#     print(f\"Intercept: {model.intercept_}\")\n",
    "#print(f\"Slope: {model.coef_}\")\n",
    "\n",
    "\n",
    "plt.scatter(model.predict(x_), y_train)  \n",
    "# data for the 1 for 1 line\n",
    "x = [x_limit,y_limit]\n",
    "y = [x_limit, y_limit]\n",
    "\n",
    "#sets the limits of the axis\n",
    "plt.xlim(x_limit, y_limit)\n",
    "plt.ylim(x_limit, y_limit)\n",
    "\n",
    "plt.ylabel('Observed target')\n",
    "\n",
    "plt.xlabel('Predicted target')\n",
    "\n",
    "# 1 for 1 line\n",
    "#adding text inside the plot\n",
    "plt.text((x_limit + x_off), (y_limit - y_off), f'$R^2 = {round(r2, 4)}$', fontsize = 12)\n",
    "plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {round(mse, 4)}$', fontsize = 12)\n",
    "#     plt.text((x_limit + x_off), (y_limit - (y_off*2)), f'$MSE = {120}$', fontsize = 12)\n",
    "plt.text((x_limit + x_off), (y_limit - (y_off*3)), f'$n = {len(y_train)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "plt.plot(x, y, color = 'r')\n",
    "\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_train_plot.jpg')\n",
    "plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "print(f\"plot saved to: {plot_out}\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(class_weight='balanced', max_depth=10)\n",
    "model .fit(X_train_res, y_train_res.target.ravel())\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(class_weight='balanced', random_state=1,max_depth=5, \\\n",
    "                              n_estimators =10, max_features=1)\n",
    "model .fit(X_train_res, y_train_res.target.ravel())\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(3)\n",
    "model .fit(X_train_res, y_train_res.target.ravel())\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(learning_rate=0.01)\n",
    "model .fit(X_train_res, y_train_res.target.ravel())\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_dim=26, activation='relu'),\n",
    "    keras.layers.Dense(16, activation = \"relu\"),\n",
    "    keras.layers.Dropout(0,3),\n",
    "    keras.layers.Dense(8, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = pd.get_dummies(y_train_res)\n",
    "y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "enc_df = pd.DataFrame(enc.fit_transform(y_train_res[['target']]).toarray())\n",
    "# merge with main df bridge_df on key values\n",
    "y_train_res = y_train_res.join(enc_df)\n",
    "y_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.DataFrame(enc.fit_transform(y_test_res[['target']]).toarray())\n",
    "# merge with main df bridge_df on key values\n",
    "y_test_res = y_test_res.join(enc_df)\n",
    "y_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res.drop(\"target\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_res, y_train_enc, epochs=500, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educba.com/keras-sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=(3,))\n",
    "x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
    "outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, the first layer can receive an `input_shape` argument:\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
    "# Afterwards, we do automatic shape inference:\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "\n",
    "# This is identical to the following:\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(16,)))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "\n",
    "# Note that you can also omit the `input_shape` argument.\n",
    "# In that case the model doesn't have any weights until the first call\n",
    "# to a training/evaluation method (since it isn't yet built):\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "# model.weights not created yet\n",
    "\n",
    "# Whereas if you specify the input shape, the model gets built\n",
    "# continuously as you are adding layers:\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "len(model.weights)\n",
    "# Returns \"4\"\n",
    "\n",
    "# When using the delayed-build pattern (no input shape specified), you can\n",
    "# choose to manually build your model by calling\n",
    "# `build(batch_input_shape)`:\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "model.build((None, 16))\n",
    "len(model.weights)\n",
    "# Returns \"4\"\n",
    "\n",
    "# Note that when using the delayed-build pattern (no input shape specified),\n",
    "# the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
    "# or the first time you call the model on some input data.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "# This builds the model for the first time:\n",
    "model.fit(x, y, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_res, y_train_enc))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((X_test_res, y_test_enc))\n",
    "\n",
    "history = model.fit(X_train_res, y_train_enc, epochs=500, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainMixed, trainVocals,epochs=10, validation_data=(testMixed, testVocals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((trainMixed, trainVocals))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((testMixed, testVocals))\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model_svc,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = iris.target_names\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel=\"linear\", C=0.01).fit(X_train, y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = iris.target_names\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel=\"linear\", C=0.01).fit(X_train, y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "#         display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterise the Random Forest Regressor alogorthim\n",
    "\n",
    "for details see: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 300\n",
    "rs = 1\n",
    "maxd = 4\n",
    "\n",
    "rng = np.random.RandomState(rs)\n",
    "rfrModel_1 = abr(dtr(max_depth=maxd), n_estimators=n_est, random_state=rng)\n",
    "print(rfrModel_1)\n",
    "mdl = \"abr\"\n",
    "str_model = f\"rf_{abr}_{model_data_name}_n_est_{n_est}_rs_{rs}_maxd_{maxd}_{date_time_str}\"\n",
    "print(str_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 100\n",
    "lr=0.1\n",
    "rs = 1\n",
    "maxd = 4\n",
    "loss = 'squared_error'\n",
    "\n",
    "rfrModel_1 = gbr(n_estimators=n_est, learning_rate=lr, max_depth=maxd, random_state=1, loss=loss)\n",
    "print(rfrModel_1)\n",
    "mdl = \"gbr\"\n",
    "str_model = f\"rf_{mdl}_{model_data_name}_n_est_{n_est}_lr{lr}_{loss}_rs_{rs}_maxd_{maxd}_{date_time_str}\"\n",
    "print(str_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfrModel_1 = etr(n_estimators=100, bootstrap=True, oob_score=True,  max_features='log2', min_samples_split=1,n_jobs=-1) \n",
    "# rfrModel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rfrModel_1 = rfr(n_estimators=100, oob_score=True,  max_depth=None, max_features='log2', min_samples_split=1.0,n_jobs=-1) \n",
    "# rfrModel_1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfrModel_1 = rfr(n_estimators=100, oob_score=True) #,  max_depth=None, max_features='log2', min_samples_split=1.0,n_jobs=-1) \n",
    "# rfrModel_1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan_to_num(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit random forest regressor model and compute variable importance score \n",
    "\n",
    "may need to restrict the number of variables for the bar graph to be legible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrLCHM = rfrModel_1.fit(X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rfrModel_1.feature_importances_\n",
    "\n",
    "### TRY THIS\n",
    "# use \"feature importance\" scores to see what the top 10 important features are\n",
    "fi = enumerate(rfrModel_1.feature_importances_)\n",
    "cols = xdata1.columns\n",
    "fiResult = [(value,cols[i]) for (i,value) in fi]\n",
    "#fiResult = [(value,cols[i]) for (i,value) in fi if value > 0.001]\n",
    "## Change the value 0.04 which we picked empirically to give us 10 variables\n",
    "## try running this code after changing the value up and down so you get more or less variables\n",
    "## do you see how this might be useful in refining the model?\n",
    "## Here is the code in case you mess up the line above\n",
    "## [(value,cols[i]) for (i,value) in fi if value > 0.04]\n",
    "#print fiResult\n",
    "r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "# print(r2)\n",
    "# print('Fitted model r2 =' ,  format(rfrLCHM.score(X_1, y_1), '.2f'))\n",
    "# print('Fitted model mse =', format(np.mean((y_1 - rfrLCHM.predict(X_1))**2), '.2f'))\n",
    "# print('n =', len(y_1))\n",
    "plt.scatter(rfrLCHM.predict(X_1), y_1,)  \n",
    "# data for the 1 for 1 line\n",
    "x = [0,35000]\n",
    "y = [0,35000]\n",
    "\n",
    "#sets the limits of the axis\n",
    "#plt.xlim(-1,35)\n",
    "#plt.ylim(-1,35)\n",
    "\n",
    "plt.ylabel('Observed target')\n",
    "\n",
    "plt.xlabel('Predicted target')\n",
    "\n",
    "# 1 for 1 line\n",
    "#adding text inside the plot\n",
    "plt.text(100, 33000, f'$R^2 = {r2}$', fontsize = 12)\n",
    "plt.text(100, 30000, f'$MSE = {mse}$', fontsize = 12)\n",
    "plt.text(100, 27000, f'$n = {len(y_1)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "plt.plot(x, y, color = 'r')\n",
    "\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_plot.jpg')\n",
    "plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "print(f\"plot saved to: {plot_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiResult = np.array(fiResult)\n",
    "score = (fiResult[:,0])\n",
    "band = fiResult[:,1]\n",
    "a = fiResult[np.argsort(fiResult[:, 1])]\n",
    "\n",
    "df_band = pd.DataFrame(dict(band=band,n=score))\n",
    "df_band['n'].astype('float')\n",
    "dfsort = df_band.sort_values(['n'], ascending=[False])\n",
    "print(dfsort)\n",
    " \n",
    "## my complicated way to get the bar plot to sort in ascending order and display the assocated band names in the y axis\n",
    "dfsort2 = df_band.sort_values(['n'], ascending=[True])\n",
    "b = dfsort2[['band']]\n",
    "c = b.values.tolist()\n",
    "# convert the list of band names in the correct order to a string\n",
    "e = str(c)\n",
    "# strips all the rubbish from the string\n",
    "f = e.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",' ')\n",
    "# convert the cleaned up string back into a list to plot the band names in the bar graph\n",
    "g = f.split()\n",
    " \n",
    "ind = np.arange(len(df_band))\n",
    "width = 0.4\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(ind, dfsort2.n, width, color='blue')\n",
    "ax.set(yticks=ind + width, yticklabels= g, ylim=[2*width - 1, len(df_band)])\n",
    "ax.set_xlabel('Performance')\n",
    "ax.set_ylabel('Ranked variables')\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_title('Variable Importance Rank')\n",
    "\n",
    "plt.show()\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_feature_importance_plot.jpg')\n",
    "fig.savefig(plot_out,dpi=600)# save out your figure to a pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsort['n'].astype('float')\n",
    "dfsort.info()\n",
    "dfsort['n'] = dfsort['n'].astype('float')\n",
    "dfsort.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bf_selection = 0.01\n",
    "df_var = dfsort[dfsort['n'] > num_bf_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_var = df_var.band.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_var.insert(0, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model_data = df_ml[column_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = select_model_data.corr()\n",
    "df_corr.to_csv(os.path.join(model_outputs, f'{str_model}_feature_imp_n_{num_bf_selection}_.csv'), index=False)\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_imp_list = dfsort.band.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_imp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run same model and same peramiters with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split data into train and test datasets, the user needs to define the variables \n",
    "xdata1 = select_model_data.iloc[:, 1:].astype('float32')\n",
    "ydata1 = select_model_data[[value_x]].astype('float32')\n",
    "ydata2 = ydata1.values\n",
    "ydata = ydata2.ravel()\n",
    "\n",
    "X_1, X_2, y_1, y_2 = train_test_split(xdata1, ydata, train_size=0.70)  \n",
    "         \n",
    "print(X_1.shape, y_1.shape)\n",
    "print(X_2.shape, y_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mdl == \"abr\":\n",
    "    \n",
    "    rng = np.random.RandomState(rs)\n",
    "    rfrModel_1 = abr(dtr(max_depth=maxd), n_estimators=n_est, random_state=rng)\n",
    "    str_model = f\"rf_abr_{model_data_name}_slc_feat_n_est_{n_est}_rs_{rs}_maxd_{maxd}_{date_time_str}\"\n",
    "    print(str_model)\n",
    "\n",
    "\n",
    "elif mdl== gbr:\n",
    "    rfrModel_1 = gbr(n_estimators=n_est, learning_rate=lr, max_depth=maxd, random_state=1, loss=loss)\n",
    "    str_model = f\"rf_gbr_{model_data_name}_slc_feat_n_est_{n_est}_lr{lr}_{loss}_rs_{rs}_maxd_{maxd}_{date_time_str}\"\n",
    "    print(str_model)\n",
    "else:\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan_to_num(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit random forest regressor model and compute variable importance score \n",
    "\n",
    "may need to restrict the number of variables for the bar graph to be legible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrLCHM = rfrModel_1.fit(X_1, y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### good info on the feature importance score - http://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rfrModel_1.feature_importances_\n",
    "\n",
    "### TRY THIS\n",
    "# use \"feature importance\" scores to see what the top 10 important features are\n",
    "fi = enumerate(rfrModel_1.feature_importances_)\n",
    "cols = xdata1.columns\n",
    "fiResult = [(value,cols[i]) for (i,value) in fi]\n",
    "#fiResult = [(value,cols[i]) for (i,value) in fi if value > 0.001]\n",
    "## Change the value 0.04 which we picked empirically to give us 10 variables\n",
    "## try running this code after changing the value up and down so you get more or less variables\n",
    "## do you see how this might be useful in refining the model?\n",
    "## Here is the code in case you mess up the line above\n",
    "## [(value,cols[i]) for (i,value) in fi if value > 0.04]\n",
    "#print fiResult\n",
    "r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "# print(r2)\n",
    "# print('Fitted model r2 =' ,  format(rfrLCHM.score(X_1, y_1), '.2f'))\n",
    "# print('Fitted model mse =', format(np.mean((y_1 - rfrLCHM.predict(X_1))**2), '.2f'))\n",
    "# print('n =', len(y_1))\n",
    "plt.scatter(rfrLCHM.predict(X_1), y_1,)  \n",
    "# data for the 1 for 1 line\n",
    "x = [0,35000]\n",
    "y = [0,35000]\n",
    "\n",
    "#sets the limits of the axis\n",
    "#plt.xlim(-1,35)\n",
    "#plt.ylim(-1,35)\n",
    "\n",
    "plt.ylabel('Observed target')\n",
    "\n",
    "plt.xlabel('Predicted target')\n",
    "\n",
    "# 1 for 1 line\n",
    "#adding text inside the plot\n",
    "plt.text(100, 33000, f'$R^2 = {r2}$', fontsize = 12)\n",
    "plt.text(100, 30000, f'$MSE = {mse}$', fontsize = 12)\n",
    "plt.text(100, 27000, f'$n = {len(y_1)}$', fontsize = 12)\n",
    "\n",
    "\n",
    "plt.plot(x, y, color = 'r')\n",
    "\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_select_feat_plot.jpg')\n",
    "plt.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "print(f\"plot saved to: {plot_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiResult = np.array(fiResult)\n",
    "score = (fiResult[:,0])\n",
    "band = fiResult[:,1]\n",
    "a = fiResult[np.argsort(fiResult[:, 1])]\n",
    "\n",
    "df_band = pd.DataFrame(dict(band=band,n=score))\n",
    "df_band['n'].astype('float')\n",
    "dfsort = df_band.sort_values(['n'], ascending=[False])\n",
    "print(dfsort)\n",
    " \n",
    "## my complicated way to get the bar plot to sort in ascending order and display the assocated band names in the y axis\n",
    "dfsort2 = df_band.sort_values(['n'], ascending=[True])\n",
    "b = dfsort2[['band']]\n",
    "c = b.values.tolist()\n",
    "# convert the list of band names in the correct order to a string\n",
    "e = str(c)\n",
    "# strips all the rubbish from the string\n",
    "f = e.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",' ')\n",
    "# convert the cleaned up string back into a list to plot the band names in the bar graph\n",
    "g = f.split()\n",
    " \n",
    "ind = np.arange(len(df_band))\n",
    "width = 0.4\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(ind, dfsort2.n, width, color='blue')\n",
    "ax.set(yticks=ind + width, yticklabels= g, ylim=[2*width - 1, len(df_band)])\n",
    "ax.set_xlabel('Performance')\n",
    "ax.set_ylabel('Ranked variables')\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.set_title('Variable Importance Rank')\n",
    "\n",
    "plt.show()\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_select_features_feature_importance_plot.jpg')\n",
    "fig.savefig(plot_out,dpi=600)# save out your figure to a pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the selected model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(y2_predict, y_2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_predict = rfrLCHM.predict(X_2)\n",
    "\n",
    "print('Predicted data r2 =', rfrLCHM.score(X_2, y_2))\n",
    "print('MSE =', format(np.mean((y_2 - rfrLCHM.predict(X_2))** 2), '.3f'))\n",
    "print('RMSE =', format(np.sqrt(np.mean((y2_predict - y_2) ** 2)), '.3f'))\n",
    "#print 'explained_var =',format(explained_variance_score(y_2, y2_predict),  '.3f') \n",
    "print('bias =' , format(np.mean(y_2) - np.mean(y2_predict), '.3f'))\n",
    "print('n =' , len(y_2))\n",
    "\n",
    "\n",
    "r2 = round(rfrLCHM.score(X_1, y_1), 2)\n",
    "mse = round(np.mean((y_1 - rfrLCHM.predict(X_1))**2), 2)\n",
    "rmse = round(np.sqrt(np.mean((y2_predict - y_2) ** 2)), 2)\n",
    "bias = round(np.mean(y_2) - np.mean(y2_predict), 2)\n",
    "\n",
    "plt.scatter(y2_predict, y_2 ,s=10, c='b', marker='o')\n",
    "# data for the 1 for 1 line\n",
    "x = [-1,40000]\n",
    "y = [-1,40000]\n",
    "\n",
    "#sets the limits of the axis\n",
    "plt.xlim(-1,40000)\n",
    "plt.ylim(-1, 40000)\n",
    "plt.ylabel('Observed mean AGB')\n",
    "plt.xlabel('Predicted mean AGB')\n",
    "# 1 for 1 line\n",
    "\n",
    "#adding text inside the plot\n",
    "plt.text(300, 37000, f'$R^2 = {r2}$', fontsize = 12)\n",
    "plt.text(300, 35000, f'$MSE = {mse}$', fontsize = 12)\n",
    "plt.text(300, 33000, f'$RMSE = {mse}$', fontsize = 12)\n",
    "plt.text(300, 31000, f'$Bias = {bias}$', fontsize = 12)\n",
    "plt.text(300, 29000, f'$n = {len(y_1)}$', fontsize = 12)\n",
    "\n",
    "plt.plot(x, y, color = 'r')\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_test_data.jpg')\n",
    "fig.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# data to plot\n",
    "x = y2_predict\n",
    "y = y_2\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density, so that the densest points are plotted last\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(x, y, c=z, s=0.8, edgecolor='')\n",
    "\n",
    "# # data for the 1 for 1 line\n",
    "# a = [-1,25]\n",
    "# b = [-1,25]\n",
    "\n",
    "# #sets the limits of the axis\n",
    "# plt.xlim(-1,25)\n",
    "# plt.ylim(-1,25)\n",
    "# plt.ylabel('Observed mean CHM')\n",
    "# plt.xlabel('Predicted mean CHM')\n",
    "# # 1 for 1 line\n",
    "# ax.plot(a, b, color = 'black')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP - do you realy want to save this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remember to change the cPickle file name !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save current fitted model and apply to unseen validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#rfrL8CHM = rfr()\n",
    "#rfrL8CHM.fit(X_1, y_1)\n",
    "\n",
    "pkl_out = os.path.join(model_outputs, f'rf_model_{str_model}')\n",
    "\n",
    "\n",
    "with open(pkl_out, 'wb') as f:\n",
    "    pickle.dump(rfrLCHM, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in unseen data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your validation dataset which has never been seen by rfr model - NOTE in this example I am just reading the same data used to train the model\n",
    "new_data = select_model_data\n",
    "# df = pd.read_csv(csv_file, header=0)\n",
    "# df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = new_data.columns.tolist()\n",
    "c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df[(df['comp'] == 'l57')]\n",
    "df1 = new_data[(new_data['target'] > 0.01)]\n",
    "df1.dropna(inplace=True)\n",
    "print (df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[column_var].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata2 = df1[column_var].astype('float32')\n",
    "xdata2.drop(\"target\", axis=1, inplace=True)\n",
    "\n",
    "ydata1 = df1[['target']].astype('float32')\n",
    "\n",
    "ydata2 = ydata1.values\n",
    "\n",
    "ydata = ydata2.ravel()\n",
    "\n",
    "print(len(ydata1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_out, 'rb') as f:\n",
    "        rf = pickle.load(f)\n",
    "\n",
    "        predicted = rf.predict(xdata2)\n",
    "\n",
    "#print 'r2 =' ,  rf.score(predicted, y_2)\n",
    "#print 'rmse =', np.sqrt(np.mean((y_2 - predicted)**2))\n",
    "#print 'n =' , len(y_2)\n",
    "\n",
    "print('Predicted data r2 =', rf.score(xdata2, ydata))\n",
    "print('MSE =', format(np.mean((ydata - rf.predict(xdata2))** 2), '.3f'))\n",
    "print('RMSE =', format(np.sqrt(np.mean((predicted - ydata) ** 2)), '.3f'))\n",
    "print('explained_var =',format(explained_variance_score(ydata, predicted),  '.3f'))\n",
    "print('bias =' , format(np.mean(ydata) - np.mean(predicted), '.3f'))\n",
    "print('n =' , len(ydata))\n",
    "\n",
    "r2 = round(rf.score(xdata2, ydata), 2)\n",
    "mse = round(np.mean((ydata - rf.predict(xdata2))** 2), 2)\n",
    "rmse = round(np.sqrt(np.mean((predicted - ydata) ** 2)), 2)\n",
    "exp_var = round(explained_variance_score(ydata, predicted), 2)\n",
    "bias = round(np.mean(ydata) - np.mean(predicted), 2)\n",
    "\n",
    "# plot up predicted and observed data \n",
    "plt.scatter(predicted, ydata,s=8, c='b', marker='o')\n",
    "\n",
    "# data for the 1 for 1 line\n",
    "x = [-1,40000]\n",
    "y = [-1,40000]\n",
    "\n",
    "#sets the limits of the axis\n",
    "plt.xlim(-1, 40000)\n",
    "plt.ylim(-1, 40000)\n",
    "\n",
    "#adding text inside the plot\n",
    "plt.text(300, 37000, f'$R^2 = {r2}$', fontsize = 12)\n",
    "plt.text(300, 35000, f'$MSE = {mse}$', fontsize = 12)\n",
    "plt.text(300, 33000, f'$RMSE = {mse}$', fontsize = 12)\n",
    "plt.text(300, 31000, f'$Bias = {bias}$', fontsize = 12)\n",
    "plt.text(300, 29000, f'$Var = {exp_var}$', fontsize = 12)\n",
    "plt.text(300, 27000, f'$n = {len(y_1)}$', fontsize = 12)\n",
    "\n",
    "# 1 for 1 line\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.plot(x, y, color = 'black')\n",
    "\n",
    "plot_out = os.path.join(model_outputs, f'{str_model}_unseen_data.jpg')\n",
    "\n",
    "fig.savefig(plot_out,dpi=600)# save out your figure to a pdf \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
